{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a Lots of Chorales from \"Schmucke dich, o liebe Seele\" by JS Bach BWV 180 \n",
    "<p>The goals of this round:\n",
    "    \n",
    "- Create many 16 voice chorales and save them to files\n",
    "    \n",
    " </p>\n",
    "<p>Please note that the Csound instance in this notebook require sample files of a Bosendorfer piano which are licensed and cannot be included in the repository. The calls to Csound won't work without significant installation and configuration work.</P>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gTZ5xE7jaVy0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mido\n",
    "import time\n",
    "from midi2audio import FluidSynth\n",
    "from IPython.display import Audio, display\n",
    "import os\n",
    "import muspy\n",
    "import piano \n",
    "import subprocess\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng(42) # random seed in parens.\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "softmax = torch.nn.functional.softmax\n",
    "\n",
    "base_dir = ''\n",
    "CSD_FILE = 'goldberg_aria1.csd'\n",
    "NOTES_FILE = \"goldberg_aria1.mac.csv\"\n",
    "LOGNAME = 'goldberg5.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rng.standard_normal())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the csd file in prep for sending notes to Csound\n",
    "This function creates a csound instance by loading a csd file, \n",
    "returning the ctcsound.CsoundPerformanceThread object and the ctcsound.Csound object\n",
    "the caller can then send score events to the pt (CsoundPerformanceThread) object, and \n",
    "when finished, close the ctcsout.Csound object.\n",
    "Caller needs to assemble a python list that looks like this:\n",
    "\n",
    "<code>\n",
    "#                  Inst Start  Dur   Vel   Ton  Oct  Voic Ste Envl Glis Upsm Renv 2-gl 3r-gl  Mult chan\n",
    "# pfields contains [1, 0.3125, 1.0, 68.0, 90.0, 5.0, 1.0, 7.0, 0.0, 0.0, 0.0, 1.0, 0.0, 4.0, 10.0, 2.0]\n",
    "\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csound_piano():\n",
    "    piano.start_logger(fname=LOGNAME)\n",
    "    piano.logging.info(f'Logging messages to: {LOGNAME}')\n",
    "    csd_content, lines = piano.load_csd(CSD_FILE)\n",
    "    piano.logging.info(f'Loaded the csd file {CSD_FILE}. There are {lines} lines read containg {len(csd_content)} bytes')\n",
    "    cs, pt = piano.load_csound(csd_content)\n",
    "    return (pt,cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sGQCWjlCbGAu"
   },
   "outputs": [],
   "source": [
    "# set global variables\n",
    "\n",
    "I = 4 # number of voices\n",
    "T = 32 # length of samples (32 = two 4/4 measures in 1/16th note increments)\n",
    "P = (86-30) +1 # number of different pitches\n",
    "print(f'I voices: {I}, T sample length: {T}, P number of distinct pitches in the input chorales: {P}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ogxsHxCgak0U"
   },
   "outputs": [],
   "source": [
    "# function for converting arrays of shape (T, 4) into midi files\n",
    "# the input array has entries that are np.nan (representing a rest)\n",
    "# or an integer between 0 and 127 inclusive\n",
    "#\n",
    "# Altered to accept pieces of arbitrary number of voices. \n",
    "# Mine are all 264 notes by 16 voices per chorale\n",
    "# what comes into this is (16,264)\n",
    "def piano_roll_to_midi(piece):\n",
    "    \"\"\"\n",
    "    piece is a an array of shape (T, 4) for some T.\n",
    "    The (i,j)th entry of the array is the midi pitch of the jth voice at time i. It's an integer in range(128).\n",
    "    outputs a mido object mid that you can convert to a midi file by called its .save() method\n",
    "    \"\"\"\n",
    "    # piece = np.concatenate([piece, [[np.nan, np.nan, np.nan, np.nan]]], axis=0)\n",
    "\n",
    "    bpm = 50\n",
    "    microseconds_per_beat = 60 * 1000000 / bpm\n",
    "\n",
    "    mid = mido.MidiFile()\n",
    "    \n",
    "    # modified to make the number of voices dependent on what is passed into the function\n",
    "    v = 0\n",
    "    tracks = {}\n",
    "    past_pitches = {}\n",
    "    delta_time = {}\n",
    "    for voice in play_chorale:\n",
    "        tracks['piano' + str(v)] = mido.MidiTrack()\n",
    "        past_pitches['piano' + str(v)] = np.nan\n",
    "        delta_time['piano' + str(v)] = 0\n",
    "        v += 1\n",
    "    \n",
    "    # create a track containing tempo data\n",
    "    metatrack = mido.MidiTrack()\n",
    "    metatrack.append(mido.MetaMessage('set_tempo',\n",
    "                                      tempo=int(microseconds_per_beat), time=0))\n",
    "    mid.tracks.append(metatrack)\n",
    "\n",
    "    # create the N voice tracks (was 4)\n",
    "    for voice in tracks:\n",
    "        mid.tracks.append(tracks[voice])\n",
    "        tracks[voice].append(mido.Message(\n",
    "            'program_change', program=0, time=0)) # choir aahs=52, piano = 0\n",
    "\n",
    "    # add notes to the N voice tracks\n",
    "    # this function expects an array in this form: chorale type: <class 'numpy.ndarray'>\n",
    "    # piece.shape: (33, 4) \n",
    "    # mine are (16,264)\n",
    "    \n",
    "    pitches = {}\n",
    "    for i in range(piece[1].shape[0]): # 0 - 263 in my case\n",
    "        v = 0\n",
    "        for voice in piece: # 0-15 in my case\n",
    "            pitches['piano'+str(v)] = piece[v,i] # i is from 0 to 263, v is 0 to 15\n",
    "            v += 1\n",
    "        for voice in tracks:\n",
    "            if np.isnan(past_pitches[voice]):\n",
    "                past_pitches[voice] = None\n",
    "            if np.isnan(pitches[voice]):\n",
    "                pitches[voice] = None\n",
    "            if pitches[voice] != past_pitches[voice]:\n",
    "                if past_pitches[voice]:\n",
    "                    tracks[voice].append(mido.Message('note_off', note=int(past_pitches[voice]),\n",
    "                                                      velocity=64, time=delta_time[voice]))\n",
    "                    delta_time[voice] = 0\n",
    "                if pitches[voice]:\n",
    "                    tracks[voice].append(mido.Message('note_on', note=int(pitches[voice]),\n",
    "                                                      velocity=64, time=delta_time[voice]))\n",
    "                    delta_time[voice] = 0\n",
    "            past_pitches[voice] = pitches[voice]\n",
    "            # 480 ticks per beat and each line of the array is a 16th note\n",
    "            delta_time[voice] += 120\n",
    "\n",
    "    return mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ogxsHxCgak0U"
   },
   "outputs": [],
   "source": [
    "class Chorale:\n",
    "    \"\"\"\n",
    "    A class to store and manipulate an array self.arr that stores a chorale.\n",
    "    \"\"\"\n",
    "    def __init__(self, arr, subtract_30=False):\n",
    "        # arr is an array of shape (4, 32) with values in range(0, 57)\n",
    "        self.arr = arr.copy()\n",
    "        if subtract_30:\n",
    "            self.arr -= 30\n",
    "            \n",
    "        # the one_hot representation of the array\n",
    "        reshaped = self.arr.reshape(-1)\n",
    "        self.one_hot = np.zeros((I*T, P))\n",
    "        r = np.arange(I*T)\n",
    "        self.one_hot[r, reshaped] = 1\n",
    "        self.one_hot = self.one_hot.reshape(I, T, P)\n",
    "        \n",
    "\n",
    "    def to_image(self):\n",
    "        # visualize the four tracks as a images\n",
    "        soprano = self.one_hot[0].transpose()\n",
    "        alto = self.one_hot[1].transpose()\n",
    "        tenor = self.one_hot[2].transpose()\n",
    "        bass = self.one_hot[3].transpose()\n",
    "        \n",
    "        fig, axs = plt.subplots(1, 4)\n",
    "        axs[0].imshow(np.flip(soprano, axis=0), cmap='hot', interpolation='nearest')\n",
    "        axs[0].set_title('soprano')\n",
    "        axs[1].imshow(np.flip(alto, axis=0), cmap='hot', interpolation='nearest')\n",
    "        axs[1].set_title('alto')\n",
    "        axs[2].imshow(np.flip(tenor, axis=0), cmap='hot', interpolation='nearest')\n",
    "        axs[2].set_title('tenor')\n",
    "        axs[3].imshow(np.flip(bass, axis=0), cmap='hot', interpolation='nearest')\n",
    "        axs[3].set_title('bass')\n",
    "        fig.set_figheight(5)\n",
    "        fig.set_figwidth(15)\n",
    "        return fig, axs\n",
    "    \n",
    "    def play(self, filename='midi_track.mid'):\n",
    "        # display an in-notebook widget for playing audio\n",
    "        # saves the midi file as a file named name in base_dir/midi_files\n",
    "        \n",
    "        midi_arr = self.arr.transpose().copy()\n",
    "        midi_arr += 30\n",
    "        midi = piano_roll_to_midi(midi_arr)\n",
    "        midi.save(base_dir + 'midi_files/' + filename)\n",
    "        play_midi('midi_files/' + filename,10)\n",
    "        \n",
    "    def elaborate_on_voices(self, voices, model):\n",
    "        # voice is a set consisting of 0, 1, 2, or 3\n",
    "        # create a mask consisting of the given voices\n",
    "        # generate a chorale with the same voices as in voices\n",
    "        mask = np.zeros((I, T))\n",
    "        y = np.random.randint(P, size=(I, T))\n",
    "        for i in voices:\n",
    "            mask[i] = 1\n",
    "            y[i] = self.arr[i].copy()\n",
    "        return harmonize(y, mask, model)\n",
    "    \n",
    "    # I think we could improve this scoring method. It's pretty lame.\n",
    "    def score(self):\n",
    "        consonance_dict = {0: 1, 1: 0, 2: 0, 3: 1, 4: 1, 5: 1, 6: 0, \n",
    "                           7: 1, 8: 1, 9: 1, 10: 0, 11: 0}\n",
    "        consonance_score = 0\n",
    "        for k in range(32):\n",
    "            for i in range(4):\n",
    "                for j in range(i):\n",
    "                    consonance_score += consonance_dict[((self.arr[i, k] - self.arr[j, k]) % 12)]\n",
    "        \n",
    "        note_score = 0\n",
    "        for i in range(4):\n",
    "            for j in range(1, 32):\n",
    "                if self.arr[i, j] != self.arr[i, j-1]:\n",
    "                    note_score += 1\n",
    "        return consonance_score, note_score\n",
    "        \n",
    "# harmonize a melody\n",
    "def harmonize(y, C, model):\n",
    "    \"\"\"\n",
    "    Generate an artificial Bach Chorale starting with y, and keeping the pitches\n",
    "    where C==1.\n",
    "    Here C is an array of shape (4, 32) whose entries are 0 and 1.\n",
    "    The pitches outside of C are repeatedly resampled to generate new values.\n",
    "    For example, to harmonize the soprano line, let y be random except y[0] \n",
    "    contains the soprano line, let C[1:] be 0 and C[0] be 1.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = y\n",
    "        C2 = C.copy()\n",
    "        num_steps = int(2*I*T)\n",
    "        alpha_max = .999\n",
    "        alpha_min = .001\n",
    "        eta = 3/4\n",
    "        for i in range(num_steps):\n",
    "            p = np.maximum(alpha_min, alpha_max - i*(alpha_max-alpha_min)/(eta*num_steps))\n",
    "            sampled_binaries = np.random.choice(2, size = C.shape, p=[p, 1-p])\n",
    "            C2 += sampled_binaries\n",
    "            C2[C==1] = 1\n",
    "            x_cache = x\n",
    "            x = model.pred(x, C2)\n",
    "            x[C2==1] = x_cache[C2==1]\n",
    "            C2 = C.copy()\n",
    "        return x\n",
    "    \n",
    "def generate_random_chorale(model): # \n",
    "    \"\"\"\n",
    "    Calls harmonize with random initialization and C=0, masking none \n",
    "    and so generates a new sample that sounds like Bach.\n",
    "    \"\"\"\n",
    "    y = np.random.randint(P, size=(I, T)).astype(int)\n",
    "    C = np.zeros((I, T)).astype(int)\n",
    "    x = harmonize(y, C, model)\n",
    "    return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "24hPqElFbKhk"
   },
   "outputs": [],
   "source": [
    "hidden_size = 32\n",
    "\n",
    "class Unit(nn.Module):\n",
    "    \"\"\"\n",
    "    Two convolution layers each followed by batchnorm and relu, \n",
    "    plus a residual connection.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Unit, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(hidden_size, hidden_size, 3, padding=1)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(hidden_size, hidden_size, 3, padding=1)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        y = self.conv1(y)\n",
    "        y = self.batchnorm1(y)\n",
    "        y = self.relu1(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.batchnorm2(y)\n",
    "        y = y + x\n",
    "        y = self.relu2(y)\n",
    "        return y\n",
    "    \n",
    "    \n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    A CNN that where you input a starter chorale and a mask and it outputs a prediction for the values\n",
    "    in the starter chorale away from the mask that are most like the training data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.initial_conv = nn.Conv2d(2*I, hidden_size, 3, padding=1)\n",
    "        self.initial_batchnorm = nn.BatchNorm2d(hidden_size)\n",
    "        self.initial_relu = nn.ReLU()\n",
    "        self.unit1 = Unit()\n",
    "        self.unit2 = Unit()\n",
    "        self.unit3 = Unit()\n",
    "        self.unit4 = Unit()\n",
    "        self.unit5 = Unit()\n",
    "        self.unit6 = Unit()\n",
    "        self.unit7 = Unit()\n",
    "        self.unit8 = Unit()\n",
    "        self.unit9 = Unit()\n",
    "        self.unit10 = Unit()\n",
    "        self.unit11 = Unit()\n",
    "        self.unit12 = Unit()\n",
    "        self.unit13 = Unit()\n",
    "        self.unit14 = Unit()\n",
    "        self.unit15 = Unit()\n",
    "        self.unit16 = Unit()\n",
    "        self.affine = nn.Linear(hidden_size*T*P, I*T*P)\n",
    "        \n",
    "    def forward(self, x, C):\n",
    "        # x is a tensor of shape (N, I, T, P)\n",
    "        # C is a tensor of 0s and 1s of shape (N, I, T)\n",
    "        # returns a tensor of shape (N, I, T, P)\n",
    "        \n",
    "        # get the number of batches\n",
    "        N = x.shape[0]\n",
    "        \n",
    "        # tile the array C out of a tensor of shape (N, I, T, P)\n",
    "        tiled_C = C.view(N, I, T, 1)\n",
    "        tiled_C = tiled_C.repeat(1, 1, 1, P)\n",
    "        \n",
    "        # mask x and combine it with the mask to produce a tensor of shape (N, 2*I, T, P)\n",
    "        y = torch.cat((tiled_C*x, tiled_C), dim=1)\n",
    "        \n",
    "        # apply the convolution and relu layers\n",
    "        y = self.initial_conv(y)\n",
    "        y = self.initial_batchnorm(y)\n",
    "        y = self.initial_relu(y)\n",
    "        y = self.unit1(y)\n",
    "        y = self.unit2(y)\n",
    "        y = self.unit3(y)\n",
    "        y = self.unit4(y)\n",
    "        y = self.unit5(y)\n",
    "        y = self.unit6(y)\n",
    "        y = self.unit7(y)\n",
    "        y = self.unit8(y)\n",
    "        y = self.unit9(y)\n",
    "        y = self.unit10(y)\n",
    "        y = self.unit11(y)\n",
    "        y = self.unit12(y)\n",
    "        y = self.unit13(y)\n",
    "        y = self.unit14(y)\n",
    "        y = self.unit15(y)\n",
    "        y = self.unit16(y)\n",
    "            \n",
    "        # reshape before applying the fully connected layer\n",
    "        y = y.view(N, hidden_size*T*P)\n",
    "        y = self.affine(y)\n",
    "        \n",
    "        # reshape to (N, I, T, P)\n",
    "        y = y.view(N, I, T, P)\n",
    "                \n",
    "        return y\n",
    "    \n",
    "    def pred(self, y, C):\n",
    "        # y is an array of shape (I, T) with integer entries in [0, P)\n",
    "        # C is an array of shape (I, T) consisting of 0s and 1s\n",
    "        # the entries of y away from the support of C should be considered 'unknown'\n",
    "        \n",
    "        # x is shape (I, T, P) one-hot representation of y\n",
    "        compressed = y.reshape(-1)\n",
    "        x = np.zeros((I*T, P))\n",
    "        r = np.arange(I*T)\n",
    "        x[r, compressed] = 1\n",
    "        x = x.reshape(I, T, P)\n",
    "        \n",
    "        # prep x and C for the plugging into the model\n",
    "        x = torch.tensor(x).type(torch.FloatTensor).to(device)\n",
    "        x = x.view(1, I, T, P)\n",
    "        C2 = torch.tensor(C).type(torch.FloatTensor).view(1, I, T).to(device)\n",
    "        \n",
    "        # plug x and C2 into the model\n",
    "        with torch.no_grad():\n",
    "            out = self.forward(x, C2).view(I, T, P).cpu().numpy()\n",
    "            out = out.transpose(2, 0, 1) # shape (P, I, T)\n",
    "            probs = np.exp(out) / np.exp(out).sum(axis=0) # shape (P, I, T)\n",
    "            cum_probs = np.cumsum(probs, axis=0) # shape (P, I, T)\n",
    "            u = np.random.rand(I, T) # shape (I, T)\n",
    "            return np.argmax(cum_probs > u, axis=0)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xm0YK6yGbZg1"
   },
   "outputs": [],
   "source": [
    "model = Net().to(device) # need this in order to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bucYvJ5u7fyl",
    "outputId": "3544bbfd-62af-42b2-cf60-5a0750028bd6"
   },
   "outputs": [],
   "source": [
    "# uncomment to load the previously trained model\n",
    "model.load_state_dict(torch.load('model1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_number(n):\n",
    "    \"\"\"\n",
    "    prepare numbers for better file storage\n",
    "    \"\"\"\n",
    "    if n == 0:\n",
    "        return '00000'\n",
    "    else:\n",
    "        digits = int(np.ceil(np.log10(n)))\n",
    "        pad_zeros = 5 - digits\n",
    "        return '0'* pad_zeros + str(n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompression of model output back to the 2 1/2 measure segment.\n",
    "<p>This section turns the output of the model into a 40 slot segment from the output of the model. We compress the segment going into the model, so we decompress it coming out of the model. Decompress does several things. \n",
    "    \n",
    "- expands the end of segment for 0,1,2,3 and convert it to 4x40 array\n",
    "- fixes the end of the 4,5,6th by adding padding to convert to a 4x40 array\n",
    "    \n",
    "This will be called just after emerging from the model and before the midi file is written    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompress_end_of_segment(this_segment):\n",
    "    # this function expects a single segment of shape (4,32)\n",
    "    # this function will de-compress the last 8 1/16th notes into the space of 16 16/th notes\n",
    "    # it moves elements in the four voices, one at a time the new, larger array\n",
    "\n",
    "    expanded = np.zeros((4,40),dtype=int) # this is the original shape of the array before compression\n",
    "    # print(f'this_segment.shape: {this_segment.shape}')\n",
    "    \n",
    "    for voice in range(4): # for all voices in the segment, copy the first 32 slots into a new larger numpy array\n",
    "        # print(f'voice: {voice}')\n",
    "        for source_index in range(24): # copy the first 24 slots with no change\n",
    "            # print(f'source_index: {source_index}  ')\n",
    "            # print(f'this_segment[{voice}][{source_index}]:  {this_segment[voice][source_index]}')                  \n",
    "            expanded[voice][source_index] = this_segment[voice][source_index]\n",
    "    \n",
    "    \n",
    "    for voice in range(4): # then for each voice in the segment, spread the last 8 slots over 16 slots in the expanded array.\n",
    "        target_index = 24\n",
    "        for source_index in range(24,32):\n",
    "            # print(f'voice: {voice}, source_index: {source_index}')\n",
    "            expanded[voice,target_index] = this_segment[voice,source_index]\n",
    "            target_index += 1\n",
    "            # print(f'source_index: {source_index}, target_index starts at 24 and is now: {target_index}')\n",
    "            expanded[voice,target_index] = this_segment[voice,source_index]\n",
    "            target_index += 1\n",
    "\n",
    "    return(expanded) # return all four voices all notes in each voice. Return all 40 slots\n",
    "\n",
    "def decompress(arr):\n",
    "    s = 0\n",
    "    my_expanded_segment = np.zeros((7,4,40),dtype=int)\n",
    "    # for segments 0,1,2,3 passed into this function (that is the first ten measures of the chorale, which comprise four phrases, each 2 1/2 measures long)\n",
    "    for seg in arr: \n",
    "        if s > 3: break # process the decompression on segments 0,1,2,3. If you reach 4, stop processing\n",
    "        # print(f'arr.shape: {arr.shape}')\n",
    "        # print(f'arr[s].shape: {arr.shape[s]}')\n",
    "        my_expanded_segment[s] = decompress_end_of_segment(arr[s]) \n",
    "        s += 1\n",
    "\n",
    "    pad8 = np.zeros((4,8))  # pad the end of the segment with zeros\n",
    "    for i in range(4,7): # segments 4,5,6\n",
    "        my_expanded_segment[i] = np.concatenate((arr[i],pad8),axis=1)\n",
    "    return(my_expanded_segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpose from the key of C to the original key\n",
    "This is done to restore what the input midi file key was. I found that model inputs in the key of C are harmonized much better than those that are in other keys. I thought they took care of this in the model by transposing to different keys, but my experience suggests otherwize.\n",
    "Add the value of root (F is 5) to each note in the array, with the exception of the 0's, which have to remain the same 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_up_segment(my_segment,root):\n",
    "    new_segment = np.copy(my_segment) # just make a copy, you will change the non zero elements \n",
    "    v = 0\n",
    "    for voice in new_segment:\n",
    "        n = 0\n",
    "        for note in voice:\n",
    "            if note > 0:\n",
    "                new_segment[v,n] = note + root\n",
    "            n += 1\n",
    "        v += 1\n",
    "        \n",
    "    return(new_segment)\n",
    "\n",
    "def transpose_up(segments,root): # read in \n",
    "    s = 0\n",
    "    new_segment = np.copy(segments)\n",
    "    for seg in segments:\n",
    "        new_segment[s] = transpose_up_segment(seg,root)\n",
    "    return(new_segment)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIDI helper functions \n",
    "1. Save an array to a midi file\n",
    "2. Generate a random chorale\n",
    "3. Harmonize one particular line in a chorale and let the model figure out the other notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save a array as a midi file. It can be any length\n",
    "def save_midi_chorale(prediction, id_number):\n",
    "    \"\"\"\n",
    "    Save an existing chorale in a midi file named {id_number}midi.mid\n",
    "    \"\"\"  \n",
    "    prediction = prediction.transpose().tolist() \n",
    "    # prediction = np.array(prediction)\n",
    "    midi_output = piano_roll_to_midi(prediction)\n",
    "    save_name = str(pad_number(id_number)) + 'midi.mid'\n",
    "    # print(f'in save_midi_chorale. About to save file to: {save_name}')\n",
    "    midi_output.save(save_name)    \n",
    "    \n",
    "\n",
    "# generate a totally random chorale out of whole cloth\n",
    "def save_midi_random(id_number):\n",
    "    \"\"\"\n",
    "    Generate an artificial chorale from a random seed \n",
    "    \"\"\"\n",
    "    prediction = generate_random_chorale(model) + 30 # 30 back on before passing to piano_roll_to_midi\n",
    "    save_midi_chorale(prediction, id_number)\n",
    "    \n",
    "# keep one voice, and choose the other voices as you might if you were Bach on deadline.\n",
    "def save_midi_harm(base_chorale, keep, id_number):\n",
    "    \"\"\"\n",
    "    Keep one voice and harmonize around it with the other three. \n",
    "    Before passing to the model, the assumption is that you need to subtract 30 from the midi note numbers \n",
    "    This is so that the model never sees a number greater than 56\n",
    "    \"\"\"\n",
    "    chorale_type = Chorale(base_chorale) \n",
    "    chorale = chorale_type.elaborate_on_voices([keep], model)\n",
    "    expanded_chorale = decompress(chorale)\n",
    "    transposed_chorale = transpose_up(expanded_chorale,root)\n",
    "    save_midi_chorale(transposed_chorale + 30, id_number)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "def start_logger(fname = 'coconet.log'):\n",
    "      logger = logging.getLogger()\n",
    "      fhandler = logging.FileHandler(filename=fname, mode='w')\n",
    "      formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "      fhandler.setFormatter(formatter)\n",
    "      logger.addHandler(fhandler)\n",
    "      logger.setLevel(logging.DEBUG)\n",
    "start_logger()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messing with BWV 180 Schmucke dich, o liebe Seele\n",
    "This next section is dividing the above named chorale by Bach into segments that are two measures long to match the model input dimensions. Some have to be compressed, some lengthened, some are already two measures long. The requirement is for 32 1/6th note segments.\n",
    "<img src='BWV 180 Schmücke dich, o liebe Seele.png' width=\"800\" height=\"400\">\n",
    "<p>Here is a table of segments that I will create</p>\n",
    "\n",
    "| segment | note# | measures | start & | end keys |\n",
    "| :-: | :-: | :-: | :-: | :-: |\n",
    "| 0 | 1-9 | 2 1/2 | F | F |\n",
    "| 1 | 10-18 | 2 1/2 | C | F | \n",
    "| 2 | 19-27 | 2 1/2 | F | C |\n",
    "| 3 | 28-36 | 2 1/2 | F | C |\n",
    "| 4 | 37-44 | 2 | C | C |\n",
    "| 5 | 44-52 | 2 | A min | C |\n",
    "| 6 | 53 | 1 | C | C |\n",
    "\n",
    "<p>Segments 0 through 3 need to have the final 16 1/16th notes reduced to 8 1/16th notes, and then get the output of the model stretched back out again. The segments 4 and 5 don't need adjusting. Segment 6 needs to be doubled in length, by repeating the last measure a second time. When played, the chord will just be held a bit longer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lth '/home/prent/Downloads/chorales_018007b_(c)greentree.mid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the BWV 180 Schmucke dich, o liebe Seele Chorale - nice variety of phrase lengths.\n",
    "# load a midi file into a list called sample - load the entire file, all tracks, all notes in all tracks\n",
    "# if the midi file has a key signature, it will print what it is. \n",
    "# the notes will be transposed by the loader to the key of C, by subtracting the root from each note. F = 5\n",
    "file_name = '/home/prent/Downloads/chorales_018007b_(c)greentree.mid'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a midi file into a numpy array\n",
    "Set certain values:\n",
    "\n",
    "- the numpy array of the whole piece is stored in variable \"sample'\n",
    "- store the root key and mode (F major, for example)\n",
    "- print the values of the time signature (must be 4/4 of you will need to do some extra work), quarter note clicks, clicks per 1/16th notes\n",
    "- any transpositions that must be performed to restore the original key\n",
    "- print the first 5 notes in each voice\n",
    "- print the shape of the variable \"sample\" containing the whole midi file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in a midi file, check the key, load into piano roll, set up np.array containing Nx4 sample.\n",
    "# calling program should slice the returned array as needed to create two measure segments for sending into the prediction model.\n",
    "\n",
    "def midi_to_input(midi_file):\n",
    "    music = muspy.read(midi_file)\n",
    "    if music.key_signatures != []: # check if the midi file includes a key signature - some don't\n",
    "        root = music.key_signatures[0].root \n",
    "        mode = music.key_signatures[0].mode # major or minor\n",
    "    else: \n",
    "        print('Warning: no key signature found. Assuming C major')\n",
    "        mode = \"major\"\n",
    "        root = 0    \n",
    "    if music.time_signatures != []: # check if the midi file includes a time signature - some don't\n",
    "        numerator = music.time_signatures[0].numerator\n",
    "        denominator = music.time_signatures[0].denominator \n",
    "    else: \n",
    "        print('Warning: no time signature found. Assuming 4/4')\n",
    "        numerator = 4\n",
    "        denominator = 4\n",
    "    # turn it into a piano roll\n",
    "    piano_roll = muspy.to_pianoroll_representation(music,encode_velocity=False) # boolean piano roll if False, default True\n",
    "    # print(piano_roll.shape) # should be one time step for every click in the midi file\n",
    "    q = music.resolution # quarter note value in this midi file. \n",
    "    q16 = q // 4 # my desired resolution is by 1/16th notes\n",
    "    print(f'time signatures: {numerator}/{denominator}')\n",
    "    time_steps = piano_roll.shape[0] // q16\n",
    "    print(f'music.resolution is q: {q}. q16: {q16} time_steps: {time_steps} 1/16th notes')\n",
    "    sample= np.zeros(shape=(time_steps,4)).astype(int) # default is float unless .astype(int)\n",
    "    # This loop is able to load an array of shape N,4 with the notes that are being played in each time step\n",
    "    for click in range(0,piano_roll.shape[0],q16): # q16 is skip 240 steps for 1/16th note resolution\n",
    "        voice = 3 # start with the low voices and decrement for the higher voices as notes get higher\n",
    "        for i in range(piano_roll.shape[1]): # check if any notes are non-zero\n",
    "            time_interval = (click) // q16 \n",
    "            if (piano_roll[click][i]): # if velocity anything but zero - unless you set encode_velocity = False\n",
    "                # if time_interval % 16 == 0:\n",
    "                #     print(f'time step: {click} at index {i}, time_interval: {time_interval}, voice: {voice}')\n",
    "                # i is the midi note number. I want to transpose it into C\n",
    "                sample[time_interval][voice] = i - root # index to the piano roll with a note - transposed by the key if not C which is 0\n",
    "                voice -= 1 # next instrument will get the higher note\n",
    "    return (sample,root,mode)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the midi file into an instance of the music class from muspy.\n",
    "sample, root, mode = midi_to_input(file_name) # sample is time interval, voice\n",
    "keys = ['C ','C#','D ','D#','E ','F ','F#','G ','G#','A ','A#','B ']\n",
    "print(f'{file_name}, \\n{keys[root]} {mode} transposed into C and then used to create the segments')\n",
    "i = 0\n",
    "for t in sample: # for each time interval\n",
    "    i += 1\n",
    "    for v in t: # for each voice\n",
    "        print(v,' ' , end='')\n",
    "    print('')\n",
    "    if i > 4: break\n",
    "\n",
    "print(f'sample.shape: {sample.shape}. dtype(sample): {type(sample[0,0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the sample into segments based on phrase length\n",
    "In this case, the 1st four segments are 2 1/2 measures long. That Bach guy was full of surprises. The next two are repeats and can be discarded for now. The 4th and 5th are 2 measures long, which is what the model expects. The final one is the closing chord. At the end of this cell, you have a variable called \"segment\" which contains an array of 0 through 6 segments of the piece, each with 40 time slots for each of 4 voices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample is a piano roll of pitches in 1/16th note intervals of dimension (320 time intervals, 4 voices, 1 pitch per time interval and voice)\n",
    "\n",
    "seg_num = 0 # index into the segment array\n",
    "segment = np.zeros((7,4,40),dtype=int)  # seg_num, voices, 1/16th note values\n",
    "print(f'seg_num\\tlength\\tstart\\tend')\n",
    "pad8 = np.zeros((8,4)) # 8 zeros in each of four voices for segments 4 & 5\n",
    "\n",
    "phrase_len = int(4 * 4 * 2.5) # the first segmenst have phrases of 2 1/2 measures in length 4*4*2.5 = 40 12/16th notes\n",
    "for i in range(6): # sample 0 though 5, seg_num 0,1,2,3\n",
    "    start = i * phrase_len \n",
    "    end = (i + 1) * phrase_len\n",
    "    if i in (2,3): # note that the first two segments are repeated, so we can discard segments 2 & 3    \n",
    "        pass\n",
    "        # print(f'Ignore segments 2 & 3 they are repeats. seg_num: {seg_num}')\n",
    "    else:\n",
    "        print(f'{seg_num}\\t{phrase_len}\\t{start}\\t{end-1}')\n",
    "        transfer = sample[start:end]\n",
    "        segment[seg_num] = transfer.transpose()\n",
    "        seg_num += 1\n",
    "    \n",
    "phrase_len = int(4 * 4 * 2) # 32 1/16th notes   \n",
    "for i in range(6, 8): # seg_num: 4 & 5\n",
    "    start = end \n",
    "    end = (start + phrase_len)\n",
    "    print(f'{seg_num}\\t{phrase_len}\\t{start}\\t{end-1}')\n",
    "    transfer = np.concatenate((sample[start:end],pad8),axis=0) # load the segment with the first 8 1/16th notes from the next segment. We will ignore these later.\n",
    "    segment[seg_num] = transfer.transpose()\n",
    "    seg_num += 1\n",
    "\n",
    "phrase_len = int(4 * 2) # 8 1/16th notes in a whole note\n",
    "for i in range(8,9): # seg_num 6\n",
    "    start = end \n",
    "    end = (start + phrase_len)\n",
    "    print(f'{seg_num}\\t{phrase_len}\\t{start}\\t{end-1}')\n",
    "    transfer = sample[start:end], # load the segment with the first 8 1/16th notes from the next segment. We will ignore these later.\n",
    "    transfer = np.concatenate(transfer*5) # put 5 copies of the 8 1/16th notes one after the other fill out to 40 slots. Ignore the later slots.\n",
    "    segment[seg_num] = transfer.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress the 40 slot segments down to 32 slots\n",
    "This is done to match the model requirements. We create a helper function that compresses the last 16 slots down to 8 by skipping every other note in the 16. Not as crude at the clipping that was done in the mode, but it looses some information that cannot be retrieved upon decompressions. At the end of this process, we have a 7,4,32 array with 7 segments that are all 32 1/16th notes in length in a variable called \"sub_segment\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will take a 4,40 array and return a 4,32 array. It compresses the last 16 slots into 8 slots by skipping every other slot in the array.\n",
    "def compress_end_of_segment(input_array):\n",
    "    # let numpy do the slicing. It's better than a python list\n",
    "    # np_input = np.array(input_array) # don't need it because it's already a np.array\n",
    "    # this function will compress the last 16 1/16th notes into the space of 8 16/th notes\n",
    "    # it looks at the four voices, one at a time and moves the \n",
    "    for v in range(4):\n",
    "        n = 24 # start at this slot for each voice\n",
    "        for i in range(n,40,2): # start at 24, increment until just before 40 by 2 each time\n",
    "            input_array[v][n] = input_array[v][i]\n",
    "            n += 1\n",
    "    return(input_array[:,:-8]) # return all four voices all notes in each voice. Return only the first 32 slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compress segments 0,1,2,3 from 40 slots to 32 slots for all four voices\n",
    "# It leaves segments 4 & 5 alone, and expands the held note on segment 6 to 32 time slices.\n",
    "# print(segment)\n",
    "print(segment.shape)\n",
    "pad8 = np.reshape(pad8,(4,8))\n",
    "for seg_num in range(4): # we need to take the 40 slot arrays and reduce them to 32 slots.\n",
    "    print(f'seg_num: {seg_num} before compression') \n",
    "    print(f'segment[{seg_num}]: {segment[seg_num][0]}')\n",
    "    my_segment = compress_end_of_segment(segment[seg_num])\n",
    "    print('after compression')\n",
    "    print(f'my_segment: {my_segment[0]}')\n",
    "    segment[seg_num] = np.concatenate((my_segment,pad8),axis=1)\n",
    "sub_segment = segment[:,:,:32] # chop off the 33-40'th 1/16th note in the piano roll leaving 32 slots    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_segment.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harmonize to the bass part\n",
    "This next cell does everything the previous one, except it adds harmonized versions of the segments.\n",
    "\n",
    "- Takes the segments of the original midi file, which have been compressed down to 7 segments of 32 1/16th notes.\n",
    "- passes that through the harmonization function of the chorale class, preserving the bass line, letting the model figure out new soprano, alto, and tenor lines.\n",
    "- decompresses the prediction output by taking the last 8 slots and turning them into 16 slots for a total of 40 in the segment\n",
    "- transposes the segments back to the original key. \n",
    "- concatenates all the segments into one long array. Make sure to only include 32 slots for segments 4 and 5. \n",
    "<code>trans_segment[4,:,:32],trans_segment[5,:,:32]</code>\n",
    "- passes that to generate a midi file\n",
    "- plays the midi file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a 16xN array stacking 4 predictions on top of each other\n",
    "What comes in is a 7x4x40 array of notes. 7 segments, each with 4 voices, and each voice with 40 notes in each segment. Segments 4&5 are padded with 8 additional zero notes, which have to be stripped away. I create a set of 4 predictions using only 32 of the notes in each segment. The preditions are based on harmonization of one part, the one specified by keep = 3, which is the bass part. I stack those 4 predictions on top of each other to create a 16 voice array of 16 by 264 notes. This cell takes about 10 minutes to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def stack_em(sub_segment,keep):\n",
    "    trans_segment = np.zeros((4,4,40),dtype=int) # save all four predictions after being de-compressed and transposed, then combined into fourple_segment\n",
    "    fourple_segment = np.zeros((7,16,40),dtype=int) # combine four trans_segments into one - don't need to predefine these\n",
    "    pad8 = np.zeros((4,8))  # pad the end of the segment with zeros\n",
    "    for examples in range(1):\n",
    "        s = 0\n",
    "        for seg in sub_segment: # for each segment of the chorale\n",
    "            print(f'\\nbeginning prediction for segment {s}')\n",
    "            for pred in range(4): # make four predictions and save all of them into trans_segment\n",
    "                # the next line takes about 31 seconds of wall clock time per prediction\n",
    "                prediction = Chorale(seg - 30).elaborate_on_voices([keep], model) + 30 # subtract 30 to go through the model, then add 30 back after.\n",
    "                print(f'prediction {pred} complete. last 8 notes of the soprano part {pred}: {prediction[0,24:]}')\n",
    "                # print(f'prediction.shape: {prediction.shape}')\n",
    "                if s in (0,1,2,3): # need to expand these\n",
    "                    expanded_segment = decompress_end_of_segment(prediction) #  just decompress one segment\n",
    "                    # print(f'expanded_segment last 16 notes of the soprano part: {expanded_segment[0,24:]}')\n",
    "                    trans_segment[pred] = transpose_up_segment(expanded_segment,root) # you need to preserve all four transposed predictions\n",
    "                else:\n",
    "                    expanded_segment = prediction[:,:32] \n",
    "                    # print(f'expanded_segment last 8 notes of the soprano part: {expanded_segment[0,24:]}')\n",
    "                    # ValueError: could not broadcast input array from shape (4,32) into shape (4,40)\n",
    "                    # the transpose_up_segment returns (4,32), and trans_segment[pred] expects (4,40) - need to pad 4,32 before sending to 4,40\n",
    "                    trans_segment[pred] = np.concatenate((transpose_up_segment(expanded_segment,root),pad8),axis=1) # you need to preserve all four transposed predictions\n",
    "                # print(f'trans_segment[{pred}] from slot 24 to the end: {trans_segment[pred,0,24:]}') \n",
    "\n",
    "            # print(f'trans_segment.shape: {trans_segment.shape}')\n",
    "            fourple_segment[s] = np.reshape(trans_segment,(16,40))\n",
    "            # print(f'fourple_segment[{s}].shape: {fourple_segment[s].shape}')\n",
    "            s += 1\n",
    "            # if s > 1: break\n",
    "        # print(f'shape of fourple_segment: {fourple_segment.shape}')\n",
    "        # you have to concatenate the fourple_segments, not reshape, because some are 40 and some are 32 slots long.\n",
    "        concat_chorale = np.concatenate((fourple_segment[0],\n",
    "                        fourple_segment[1],\n",
    "                        fourple_segment[2],\n",
    "                        fourple_segment[3],\n",
    "                        fourple_segment[4,:,:32],\n",
    "                        fourple_segment[5,:,:32],\n",
    "                        fourple_segment[6]),axis=1)\n",
    "        return(concat_chorale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in range(1):\n",
    "    for i in range(8):        \n",
    "        concat_chorale = stack_em(sub_segment,v)\n",
    "        outfile = 'saved_chorale' + str(v) + str(i) + '.npy' #<-- if you don't end it with .npy then it appends .npy to the name automatically\n",
    "        np.save(outfile, concat_chorale)\n",
    "!ls -lth *.npy\n",
    "    # saved_chorale = np.load(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Play a piano roll of any dimension\n",
    "This function takes in an array of (voices,time_steps) of indeterminant shape. It sends notes to csound for playing to an audio wave file.\n",
    "<p>There are several adjustments that can be made. Velocity is the proxy for how hard the keys are pressed. If it's less than 62, then the csound instance chooses extremely soft samples, as if the player is barely touching the keys. Here is a complete breakdown of velocity and sample sets:</p>\n",
    "    \n",
    "| velocity | sample set | max volume |\n",
    "| :-: | :-: | :-: |\n",
    "| 60 | 25 | 24 | \n",
    "| 62 | 31 | 16 |    \n",
    "| 64 | 39 | 15 |\n",
    "| 66 | 47 | 13 |    \n",
    "| 68 | 63 | 10 |\n",
    "| 70 | 78 | 10 |\n",
    "| 72 | 85 | 10 |\n",
    " \n",
    "<p>Notice that the range of the velocity is very small. Anything 60 or less uses the 25 sample set, which is very soft and quiet, while anything 72 or more uses the 85 sample set. There are three additional sample sets at 99, 113, and 127, which I left out of the csound csd file to reduce storage. I think it might be better to eliminate some of the higher notes, every other sample, and include the louder ones, but that's a job for another day.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ogxsHxCgak0U"
   },
   "outputs": [],
   "source": [
    "def piano_roll_to_csound(piece,velocity,volume,tpq):\n",
    "    if os.path.exists('goldberg5.log'):\n",
    "        os.remove(\"goldberg5.log\") # make sure the log starts over with a fresh log file. Next line starts the logger.\n",
    "    pt,cs = load_csound_piano() # load the csd file and return a performance thread and a Csound instance, start the logger.\n",
    "    piano.logging.info('ins star dur vel ton oc voi stero env glis upsa rEnv 2nd 3rd vol chan')\n",
    "    tp16th = tpq / 4 # time per 1/16th note\n",
    "    hold = 0.2 # how long to hold to make more of a legato\n",
    "    pfields = []\n",
    "       \n",
    "    v = 0\n",
    "    for voice in piece: # once for each voice\n",
    "        prev_note = 0\n",
    "        duration = 0\n",
    "        start_time = 0\n",
    "        first = True\n",
    "        # print(f'\\tvoice\\tstart\\tnote\\tduration')\n",
    "        for note in voice: # one note for each time step in this voice [69 69 69 69 67 67 67 67 65 65 65 65 67 67 67 67]\n",
    "            if first:\n",
    "                prev_note = note\n",
    "                first = False\n",
    "            if note == prev_note:\n",
    "                duration += tp16th # add another 1/16th note duration to the current duration\n",
    "            else: # send the note to csound\n",
    "                octave = prev_note // 12\n",
    "                if octave > 0:\n",
    "                    tone = prev_note - 12 * octave\n",
    "                    octave -= 1\n",
    "                    #        inst start       duration  velocity  tone  octave voice ster env glis upsa rEnv 2nd 3rd gl mult chan\n",
    "                    random_velocity = velocity + np.random.randint(-3,2) # chose different sample sets based on greater or lesser velocity\n",
    "                    random_start = start_time + round(rng.standard_normal()/75,5)\n",
    "                    if random_start < 0: random_start = 0\n",
    "                    stereo = v // 4 * 4 + 2 # based on the voice returns: 2 2 2 2 6 6 6 6 10 10 10 10 14 14 14 14 to space out the pianos\n",
    "                    pfields.append([1, random_start, duration + hold, random_velocity, tone, octave, 1.0, stereo, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, volume, 1])\n",
    "                start_time += duration\n",
    "                duration = tp16th\n",
    "                prev_note = note \n",
    "        v += 1\n",
    "    \n",
    "    pfields.sort() # This is done automatically by csound when called from the command line, but not when note events are sent\n",
    "    \n",
    "    print(f'list of notes is {len(pfields)} long')\n",
    "    for i in range(len(pfields)):\n",
    "        # print(f'index: {i}\\t{pfields[i]}')\n",
    "        pt.scoreEvent(0, 'i', pfields[i]) # here is where the notes are sent to ctcsound\n",
    "        piano.logging.info(pfields[i]) \n",
    "\n",
    "    piano.printMessages(cs)\n",
    "    delay_time =  max(10,len(pfields) // 35) # need enough time to prevent csound being told to stop processing prematurely\n",
    "    print(f'about to delay to allow ctcsound to process the notes. delay_time: {delay_time}')\n",
    "    print(f'last start time was at {start_time}. Set f0 to {start_time+1}')\n",
    "    time.sleep(delay_time) # once you hit the next line csound stops\n",
    "    pt.stop() # this is important I think. It closes the output file.\n",
    "    pt.join()   \n",
    "    piano.printMessages(cs)    \n",
    "    cs.reset()\n",
    "    subprocess.run(['grep', 'invalid\\|range\\|error\\|replacing\\|overall', 'goldberg5.log']) # look in the log for important messages\n",
    "    audio = Audio('/home/prent/Music/sflib/goldberg_aria1.wav')\n",
    "    display(audio)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove any dissonant notes from each time step\n",
    "This is a hack to remove some of the notes that aren't in the key of F major. I'm not happy with it. Some issues:\n",
    "\n",
    "- Who am I to doubt coconet's wisdom? Well, I'm stacking four chorales on top of each other, and each one may be coherent to itself, but it has no knowledge of any of the other three chorales. So I've got that going for me.\n",
    "- I replace the note that is \"bad\" with the key of the piece, F major. I tried replacing it with the most common note, but what if that common note was not actually in the scale? I ended up with a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_scale(root: int, mode: str):\n",
    "    \"\"\"Return the scale mask for a specific root.\"\"\"\n",
    "    if mode == \"major\":\n",
    "        c_scale = np.array([1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1], bool)\n",
    "    elif mode == \"minor\":\n",
    "        c_scale = np.array([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0], bool)\n",
    "    else:\n",
    "        raise ValueError(\"`mode` must be either 'major' or 'minor'.\")\n",
    "    return np.roll(c_scale, root)\n",
    "\n",
    "# this function checks the consonance of a single time step of 16 notes. \n",
    "# I need to find out which notes are causing the trouble, exclude them, then try again, until I get a better score.\n",
    "def quick_score(time_step,root):\n",
    "    scale = _get_scale(root, mode.lower())\n",
    "    note_count = 0\n",
    "    in_scale_count = 0\n",
    "    bad_note = 0\n",
    "    for note in time_step:\n",
    "        if scale[note % 12]:\n",
    "            in_scale_count += 1\n",
    "        else: \n",
    "            bad_note = note_count\n",
    "        note_count += 1\n",
    "    if note_count < 1:\n",
    "        return math.nan\n",
    "    return (in_scale_count / note_count), bad_note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_dissonant(chorale):\n",
    "    chorale_t = chorale.transpose() # turn a (16,264) array into (264,16)\n",
    "    \n",
    "    t = 0\n",
    "    root = 5\n",
    "    mode = 'major'\n",
    "    \n",
    "    for time_step in chorale_t:\n",
    "        loop_check = 0\n",
    "        most_common_note = np.bincount(time_step).argmax() # what if the most common note is not in the scale? Loop-de-loop!\n",
    "        replacement_note = root + 60\n",
    "        consonant = 0\n",
    "        while consonant < 1:\n",
    "            consonant,bad_note = quick_score(time_step,root)\n",
    "            if consonant < 1.0: \n",
    "                # print(f'time_step: {t}, bad_note@ {bad_note},\\\n",
    "                # time_step[{bad_note}]: {time_step[bad_note]},\\\n",
    "                # most_common_note: {most_common_note}, replacement_note: {replacement_note}')\n",
    "                time_step[bad_note] = replacement_note\n",
    "                # print(f'time_step[{bad_note}]: {time_step[bad_note]}')\n",
    "            loop_check += 1\n",
    "            if loop_check > 15: \n",
    "                print('entered an endless loop. Halt & catch fire')\n",
    "                break\n",
    "        t += 1\n",
    "    return chorale_t.transpose() # turn it back the way it was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(chorale[:4,:40])\n",
    "def arpeggiate(chorale,mask):\n",
    "    #              +-- start\n",
    "    #              |  +-- stop\n",
    "    #              |  |                                +-- step size 2 or 3\n",
    "    for i in range(0, chorale.shape[1]// mask.shape[1],2): # don't arpegiate the final 16 notes, stop at the 29th arpegiation, skip every third one.\n",
    "        start = i * 8\n",
    "        end = (i+1) * 8\n",
    "        chorale[:,start:end] = mask * chorale[:,start:end]\n",
    "    v = 0\n",
    "    for voice in chorale:\n",
    "        n = 0\n",
    "        for note in voice:\n",
    "            if v == 0 : # soprano - need to randomly add an octave or two to the soprano voices\n",
    "                note += np.random.choice([0,0,0,12,24])\n",
    "            elif v == 3: # bass reduce the octave\n",
    "                note += np.random.choice([-36,-24,-12,0,0,0,0,])\n",
    "            n += 1\n",
    "        \n",
    "\n",
    "    return(chorale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, you can't just multiply by other than 1. If you multiply by 2, you take a midi number like 65 and make it 130, which makes an invalid midi number\n",
    "mask = np.zeros((16,8))\n",
    "\n",
    "# 1st part\n",
    "mask[0,] =  [0,0,0,1,1,0,1,1]\n",
    "mask[1,] =  [0,0,1,1,0,1,1,1]\n",
    "mask[2,] =  [0,1,1,0,1,1,1,0]\n",
    "mask[3,] =  [1,1,1,1,1,1,0,1]\n",
    "# 2nd part\n",
    "mask[4,] =  [0,1,1,1,0,1,1,1]\n",
    "mask[5,] =  [0,0,1,1,0,0,1,1]\n",
    "mask[6,] =  [0,0,0,1,0,0,0,1]\n",
    "mask[7,] =  [1,1,1,1,1,0,1,0]\n",
    "# 3rd part\n",
    "mask[8,] =  [0,0,1,1,0,1,1,1]\n",
    "mask[9,] =  [0,1,1,1,0,0,0,1]\n",
    "mask[10,] = [0,0,0,1,0,0,1,1]\n",
    "mask[11,] = [1,1,1,0,1,0,1,0]\n",
    "# 4th part\n",
    "mask[12,] = [0,0,0,1,1,0,1,1]\n",
    "mask[13,] = [0,0,1,1,0,1,1,1]\n",
    "mask[14,] = [0,1,1,0,1,1,1,0]\n",
    "mask[15,] = [1,1,1,0,1,1,0,1]\n",
    "\n",
    "numpy_file = os.path.join('numpy_chorales','saved_chorale399.npy')\n",
    "chorale = strip_dissonant(np.load(numpy_file))\n",
    "new_chorale = arpeggiate(chorale,mask)\n",
    "np.save(os.path.join('numpy_chorales','arpeggio399.npy'),new_chorale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play some of the randomly generated chorales stored earlier\n",
    "In the cells above, we created and saved some chorales based on preserved voices of the Schmucke chorale. They are named with this convention:\n",
    "<code>\n",
    "    +-- name collection\n",
    "    |            +-- voice that was preserved. 0: soprano, 1: alto, 2:tenor, 3:bass\n",
    "    |            |+-- which one of several made, this is the sixth in the series\n",
    "    saved_chorale06.npy\n",
    "</code>   \n",
    "<p>Save them to a wav file and also a midi file.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ogxsHxCgak0U"
   },
   "outputs": [],
   "source": [
    "play_chorale = np.load(os.path.join('numpy_chorales','arpeggio398.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ogxsHxCgak0U"
   },
   "outputs": [],
   "source": [
    "# play_chorale = transpose_up_segment(play_chorale,5) # chorales stored in the key of C by mistake.\n",
    "# print(play_chorale[:16,240:]) # print the final 24 notes\n",
    "print(f'shape sent to piano_roll_to_csound: {play_chorale.shape}')    \n",
    "\n",
    "midi_output = piano_roll_to_midi(play_chorale) # convert to mido object\n",
    "music = muspy.from_mido(midi_output) # convert mido to muspy music\n",
    "muspy.write_midi('apreggio398.mid',music)\n",
    "tpq = 1.8 # time per quarter note\n",
    "#                    +-- name of the array containing the notes 16x264 or other shape\n",
    "#                    |            +-- velocity used to chose the sample set\n",
    "#                    |            |  +-- overall volume\n",
    "#                    |            |  |  +-- time per quarter note\n",
    "piano_roll_to_csound(play_chorale,69,13,tpq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the various artificial chorales using Muspy metrics\n",
    "\n",
    "- read in the numpy arrays in a folder, one at a time\n",
    "- convert them to midi\n",
    "- load them into a music class\n",
    "- evaluate them by some metrics\n",
    "- sort the results by one metric\n",
    "- print them all\n",
    "\n",
    "Docs on muspy:\n",
    "https://salu133445.github.io/muspy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_key(elem):\n",
    "    return (-elem)\n",
    "\n",
    "dirname = 'numpy_chorales'\n",
    "metrics = []\n",
    "print(f'\\tfile\\t\\tpitch\\tpitch\\tpitch\\tpitch\\tpoly-\\tpoly\\tscale\\tclass\\tconson\\tnote')\n",
    "print(f'\\tname\\t\\trange\\tuse\\tclas\\tentr\\tphony\\trate\\tconsist\\tentropy\\tscore\\tscore')\n",
    "for filename in os.listdir(dirname):\n",
    "    path = os.path.join(dirname,filename)\n",
    "    chorale = np.load(path) # bring in a premade chorale stored as a numpy array (16,264)\n",
    "    # consonant, note_score = quick_score(chorale,root) # I screwed this up.\n",
    "    consonant = 0\n",
    "    note_score = 0 \n",
    "    # print(f'chorale.shape: {chorale.shape}')\n",
    "    midi_output = piano_roll_to_midi(chorale) # convert to mido object\n",
    "    music = muspy.from_mido(midi_output) # convert mido to muspy music\n",
    "    metric = [os.path.basename(filename), # make a list of metrics\n",
    "        muspy.pitch_range(music),\n",
    "        muspy.n_pitches_used(music),\n",
    "        muspy.n_pitch_classes_used(music), \n",
    "        round(muspy.pitch_entropy(music),3),\n",
    "        round(muspy.polyphony(music),3), \n",
    "        round(muspy.polyphony_rate(music),3),\n",
    "        round(muspy.scale_consistency(music),3), \n",
    "        round(muspy.pitch_class_entropy(music),3),\n",
    "        consonant, note_score]\n",
    "    metrics.append(metric)\n",
    "metrics.sort(key=sort_key())\n",
    "for metric in metrics:\n",
    "    print('{:15}'.format(metric[0]),'\\t',metric[1],'\\t',metric[2],\\\n",
    "          '\\t',metric[3],'\\t',metric[4],'\\t',metric[5],'\\t',metric[6],\\\n",
    "          '\\t',metric[7],'\\t',metric[8],\n",
    "         '\\t',metric[9],'\\t',metric[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Convolve with the impulse response from Teatro Alcorcon in Madrid from Angelo Farina Collection\n",
    "<p>The next few cells require a great deal of installation work to accomplish. You need to install the following:\n",
    "    \n",
    "- Csound - available in most Linux repos with the operating system's standard program installer\n",
    "- sox \n",
    "- ffmpeg\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!csound goldberg_aria1c.csd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lth /home/prent/Music/sflib/goldberg_aria1a-c.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sox /home/prent/Music/sflib/goldberg_aria1a-c.wav save1.wav reverse\n",
    "!sox save1.wav save2.wav silence 1 0.01 0.01\n",
    "!sox save2.wav save1.wav reverse\n",
    "!sox save1.wav /home/prent/Music/sflib/goldberg_aria1-t9.wav silence 1 0.01 0.01\n",
    "!rm save1.wav\n",
    "!rm save2.wav\n",
    "!ls -lth /home/prent/Music/sflib/goldberg_aria1-t9.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -y -i /home/prent/Music/sflib/goldberg_aria1-t9.wav -b:a 320k /home/prent/Music/sflib/goldberg_aria1-t9.mp3\n",
    "audio = Audio('/home/prent/Music/sflib/goldberg_aria1-t9.mp3')\n",
    "display(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "coconet_reorganized.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
