{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coconet Synthetic Chrorales with Variations\n",
    "<p>This notebook is derived from the pytorch implementation of the original coconet paper (https://arxiv.org/pdf/1903.07227.pdf) created by Kevin Donoghue: (https://github.com/kevindonoghue/coconet-pytorch). I was able to get his code to run with a few modifications. This notebook uses some of his code to load the model trained over 30,000 iterations. </P>\n",
    "<p>Here is what I hope to do:</p>\n",
    "\n",
    "-  Load the trained model\n",
    "-  provide a prompt that the model can start with, and produce a four part chorale\n",
    "-  take that chorale and perform it using Victorian Rational Well Temperament, a tuning that sounds better than twelve tone equal temperament for Bach.\n",
    "-  use a Bosendorfer Piano sample library instead of the chorus of vocals used in the Donoghue notebook\n",
    "-  Add variations in they style of Bach's Goldberg Variations (I'm pretty vague about that part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gvs_VMUkuWmB"
   },
   "source": [
    "This is a notebook that implements the paper https://arxiv.org/pdf/1903.07227.pdf in PyTorch. The goal is to generate samples of music, in the form of midi files, that sound like Bach chorales. Each Bach chorale is a piece of music for four voices. These chorales can be encoded in arrays of shape (4, N) where N is the number of 16th notes on the chorale and a value of 60 (say) at i, j indicates that voice is singing the pitch 60 at the jth 16th note. These encodings are in the file Jsb16thSeparated.npz.\n",
    "\n",
    "I split these encodings into two measure chunks, so arrays of shape (4, 32). After one-hot encoding the entries, they become arrays of shape (4, 32, P) where P is the number of possible pitches.\n",
    "\n",
    "To train a neural net to generate samples like the training samples, you generate samples which consist of random entries from a chorale plus the location of those entries. The neural net is then trained to predict the rest of the entries. For example, the net might be given the entries of one voice in the chorale and then its job is to predict the rest of the entries. In practice, this works by randomly generating arrays of shape (4, 32) whose entries are 0 and 1. A chorale is multiplied by this array to erase part of its data. Then the partially erased array and the masking array of 0s and 1s are fed through the neural net, which outputs a predicted array of shape (4, 32, P). This output array is compared with the full (4, 32, P) array of the inputted chorale via cross entropy loss, and gradient descent is applied with respect to this loss function. This encourages the network to learn the pitches in the Bach chorale that were erased in the input.\n",
    "\n",
    "To generate good samples for listening, it helps to repeatedly resample. You generate a completely unmasked chorale, then slowly freeze notes (as if the composer has decided finally that this note is good) and resample with the frozen notes masked. As you resample, you freeze more and more notes, until you're masking all the notes. At this point the sample has been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "nVc5sjL6PMPM",
    "outputId": "8567e9b4-6571-4912-9652-2852a0553efe"
   },
   "outputs": [],
   "source": [
    "# installations needed for in-colab midi playback\n",
    "# !apt install fluidsynth\n",
    "# !cp /usr/share/sounds/sf2/FluidR3_GM.sf2 ./font.sf2\n",
    "#!cp /usr/share/sounds/sf2/'Realistic Piano_1_2.sf2' ./font.sf2\n",
    "# !pip install midi2audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gTZ5xE7jaVy0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mido\n",
    "import time\n",
    "from midi2audio import FluidSynth\n",
    "from IPython.display import Audio, display\n",
    "import os\n",
    "import muspy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "softmax = torch.nn.functional.softmax\n",
    "\n",
    "base_dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "colab_type": "code",
    "id": "oDwOLhiCOw4C",
    "outputId": "e3d3bb86-94d7-4c0c-ad7d-a8f478a1ef9e"
   },
   "outputs": [],
   "source": [
    "def play_midi(path):\n",
    "    \"\"\"\n",
    "    A script for playing the midi files in the notebook. path is the path to the midi file to be played, relative to base_dir.\n",
    "    \"\"\"\n",
    "    if os.path.exists('test.wav'):\n",
    "        os.remove('test.wav')\n",
    "    FluidSynth('font.sf2').midi_to_audio(base_dir + path, 'test.wav')\n",
    "    audio = Audio('test.wav')\n",
    "    display(audio)\n",
    "    \n",
    "path = 'test.mid'\n",
    "play_midi(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L_fxHt7VaaZU"
   },
   "outputs": [],
   "source": [
    "# load training data\n",
    "data = np.load('Jsb16thSeparated.npz', encoding='bytes', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "n-BehQbialC1",
    "outputId": "25b7cbf8-b122-492d-8c6e-19fe77fb2741"
   },
   "outputs": [],
   "source": [
    "# transpose chorales to different keys, so there's more variation in training data\n",
    "all_tracks = []\n",
    "for x in data.files:\n",
    "    for y in data[x]:\n",
    "        for i in range(-6, 6):\n",
    "            all_tracks.append(y + i)\n",
    "\n",
    "print(len(all_tracks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Axm9wOqobD9G",
    "outputId": "21a97d41-956b-48e4-9f12-d49145a18351"
   },
   "outputs": [],
   "source": [
    "# determine highest and lowest pitches\n",
    "\n",
    "max_midi_pitch = -np.inf\n",
    "min_midi_pitch = np.inf\n",
    "for x in all_tracks:\n",
    "    if x.max() > max_midi_pitch:\n",
    "        max_midi_pitch = int(x.max())\n",
    "    if x.min() < min_midi_pitch:\n",
    "        min_midi_pitch = int(x.min())\n",
    "        \n",
    "print(max_midi_pitch, min_midi_pitch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sGQCWjlCbGAu"
   },
   "outputs": [],
   "source": [
    "# set global variables\n",
    "\n",
    "I = 4 # number of voices\n",
    "T = 32 # length of samples (32 = two 4/4 measures)\n",
    "P = max_midi_pitch - min_midi_pitch +1 # number of different pitches\n",
    "batch_size=24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IRiSKKXhbHI6"
   },
   "outputs": [],
   "source": [
    "# prepare the training dataset by cutting chorales in 2 measure pieces\n",
    "\n",
    "train_tracks = []\n",
    "\n",
    "for track in all_tracks:\n",
    "    track = track.transpose()\n",
    "    cut = 0\n",
    "    while cut < track.shape[1]-T:\n",
    "        if (track[:, cut:cut+T] > 0).all():\n",
    "            train_tracks.append(track[:, cut:cut+T] - min_midi_pitch)\n",
    "        cut += T\n",
    "        \n",
    "\n",
    "train_tracks = np.array(train_tracks).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jGGVthWObJI7",
    "outputId": "7a4cac32-5247-407f-9436-565d45758ae3"
   },
   "outputs": [],
   "source": [
    "print(train_tracks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ogxsHxCgak0U"
   },
   "outputs": [],
   "source": [
    "# function for converting arrays of shape (T, 4) into midi files\n",
    "# the input array has entries that are np.nan (representing a rest)\n",
    "# of an integer between 0 and 127 inclusive\n",
    "\n",
    "def piano_roll_to_midi(piece):\n",
    "    \"\"\"\n",
    "    piece is a an array of shape (T, 4) for some T.\n",
    "    The (i,j)th entry of the array is the midi pitch of the jth voice at time i. It's an integer in range(128).\n",
    "    outputs a mido object mid that you can convert to a midi file by called its .save() method\n",
    "    \"\"\"\n",
    "    piece = np.concatenate([piece, [[np.nan, np.nan, np.nan, np.nan]]], axis=0)\n",
    "\n",
    "    bpm = 50\n",
    "    microseconds_per_beat = 60 * 1000000 / bpm\n",
    "\n",
    "    mid = mido.MidiFile()\n",
    "    \n",
    "    tracks = {'soprano': mido.MidiTrack(), 'alto': mido.MidiTrack(),\n",
    "              'tenor': mido.MidiTrack(), 'bass': mido.MidiTrack()}\n",
    "    past_pitches = {'soprano': np.nan, 'alto': np.nan,\n",
    "                    'tenor': np.nan, 'bass': np.nan}\n",
    "    delta_time = {'soprano': 0, 'alto': 0, 'tenor': 0, 'bass': 0}\n",
    "\n",
    "\n",
    "    # create a track containing tempo data\n",
    "    metatrack = mido.MidiTrack()\n",
    "    metatrack.append(mido.MetaMessage('set_tempo',\n",
    "                                      tempo=int(microseconds_per_beat), time=0))\n",
    "    mid.tracks.append(metatrack)\n",
    "\n",
    "    # create the four voice tracks\n",
    "    for voice in tracks:\n",
    "        mid.tracks.append(tracks[voice])\n",
    "        tracks[voice].append(mido.Message(\n",
    "            'program_change', program=0, time=0)) # choir aahs=52, piano = 0\n",
    "\n",
    "    # add notes to the four voice tracks\n",
    "    for i in range(len(piece)):\n",
    "        pitches = {'soprano': piece[i, 0], 'alto': piece[i, 1],\n",
    "                   'tenor': piece[i, 2], 'bass': piece[i, 3]}\n",
    "        for voice in tracks:\n",
    "            if np.isnan(past_pitches[voice]):\n",
    "                past_pitches[voice] = None\n",
    "            if np.isnan(pitches[voice]):\n",
    "                pitches[voice] = None\n",
    "            if pitches[voice] != past_pitches[voice]:\n",
    "                if past_pitches[voice]:\n",
    "                    tracks[voice].append(mido.Message('note_off', note=int(past_pitches[voice]),\n",
    "                                                      velocity=64, time=delta_time[voice]))\n",
    "                    delta_time[voice] = 0\n",
    "                if pitches[voice]:\n",
    "                    tracks[voice].append(mido.Message('note_on', note=int(pitches[voice]),\n",
    "                                                      velocity=64, time=delta_time[voice]))\n",
    "                    delta_time[voice] = 0\n",
    "            past_pitches[voice] = pitches[voice]\n",
    "            # 480 ticks per beat and each line of the array is a 16th note\n",
    "            delta_time[voice] += 120\n",
    "\n",
    "    return mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ogxsHxCgak0U"
   },
   "outputs": [],
   "source": [
    "class Chorale:\n",
    "    \"\"\"\n",
    "    A class to store and manipulate an array self.arr that stores a chorale.\n",
    "    \"\"\"\n",
    "    def __init__(self, arr, subtract_30=False):\n",
    "        # arr is an array of shape (4, 32) with values in range(0, 57)\n",
    "        self.arr = arr.copy()\n",
    "        if subtract_30:\n",
    "            self.arr -= 30\n",
    "            \n",
    "        # the one_hot representation of the array\n",
    "        reshaped = self.arr.reshape(-1)\n",
    "        self.one_hot = np.zeros((I*T, P))\n",
    "        r = np.arange(I*T)\n",
    "        self.one_hot[r, reshaped] = 1\n",
    "        self.one_hot = self.one_hot.reshape(I, T, P)\n",
    "        \n",
    "\n",
    "    def to_image(self):\n",
    "        # visualize the four tracks as a images\n",
    "        soprano = self.one_hot[0].transpose()\n",
    "        alto = self.one_hot[1].transpose()\n",
    "        tenor = self.one_hot[2].transpose()\n",
    "        bass = self.one_hot[3].transpose()\n",
    "        \n",
    "        fig, axs = plt.subplots(1, 4)\n",
    "        axs[0].imshow(np.flip(soprano, axis=0), cmap='hot', interpolation='nearest')\n",
    "        axs[0].set_title('soprano')\n",
    "        axs[1].imshow(np.flip(alto, axis=0), cmap='hot', interpolation='nearest')\n",
    "        axs[1].set_title('alto')\n",
    "        axs[2].imshow(np.flip(tenor, axis=0), cmap='hot', interpolation='nearest')\n",
    "        axs[2].set_title('tenor')\n",
    "        axs[3].imshow(np.flip(bass, axis=0), cmap='hot', interpolation='nearest')\n",
    "        axs[3].set_title('bass')\n",
    "        fig.set_figheight(5)\n",
    "        fig.set_figwidth(15)\n",
    "        return fig, axs\n",
    "    \n",
    "    def play(self, filename='midi_track.mid'):\n",
    "        # display an in-notebook widget for playing audio\n",
    "        # saves the midi file as a file named name in base_dir/midi_files\n",
    "        \n",
    "        midi_arr = self.arr.transpose().copy()\n",
    "        midi_arr += 30\n",
    "        midi = piano_roll_to_midi(midi_arr)\n",
    "        midi.save(base_dir + 'midi_files/' + filename)\n",
    "        play_midi('midi_files/' + filename)\n",
    "        \n",
    "    def elaborate_on_voices(self, voices, model):\n",
    "        # voice is a set consisting of 0, 1, 2, or 3\n",
    "        # create a mask consisting of the given voices\n",
    "        # generate a chorale with the same voices as in voices\n",
    "        mask = np.zeros((I, T))\n",
    "        y = np.random.randint(P, size=(I, T))\n",
    "        for i in voices:\n",
    "            mask[i] = 1\n",
    "            y[i] = self.arr[i].copy()\n",
    "        return harmonize(y, mask, model)\n",
    "# I think we could improve this scoring method. It's never actually used for evaluations.\n",
    "    def score(self):\n",
    "        consonance_dict = {0: 1, 1: 0, 2: 0, 3: 1, 4: 1, 5: 1, 6: 0, \n",
    "                           7: 1, 8: 1, 9: 1, 10: 0, 11: 0}\n",
    "        consonance_score = 0\n",
    "        for k in range(32):\n",
    "            for i in range(4):\n",
    "                for j in range(i):\n",
    "                    consonance_score += consonance_dict[((self.arr[i, k] - self.arr[j, k]) % 12)]\n",
    "        \n",
    "        note_score = 0\n",
    "        for i in range(4):\n",
    "            for j in range(1, 32):\n",
    "                if self.arr[i, j] != self.arr[i, j-1]:\n",
    "                    note_score += 1\n",
    "        return consonance_score, note_score\n",
    "        \n",
    "# harmonize a melody\n",
    "def harmonize(y, C, model):\n",
    "    \"\"\"\n",
    "    Generate an artificial Bach Chorale starting with y, and keeping the pitches\n",
    "    where C==1.\n",
    "    Here C is an array of shape (4, 32) whose entries are 0 and 1.\n",
    "    The pitches outside of C are repeatedly resampled to generate new values.\n",
    "    For example, to harmonize the soprano line, let y be random except y[0] \n",
    "    contains the soprano line, let C[1:] be 0 and C[0] be 1.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = y\n",
    "        C2 = C.copy()\n",
    "        num_steps = int(2*I*T)\n",
    "        alpha_max = .999\n",
    "        alpha_min = .001\n",
    "        eta = 3/4\n",
    "        for i in range(num_steps):\n",
    "            p = np.maximum(alpha_min, alpha_max - i*(alpha_max-alpha_min)/(eta*num_steps))\n",
    "            sampled_binaries = np.random.choice(2, size = C.shape, p=[p, 1-p])\n",
    "            C2 += sampled_binaries\n",
    "            C2[C==1] = 1\n",
    "            x_cache = x\n",
    "            x = model.pred(x, C2)\n",
    "            x[C2==1] = x_cache[C2==1]\n",
    "            C2 = C.copy()\n",
    "        return x\n",
    "    \n",
    "def generate_random_chorale(model): # \n",
    "    \"\"\"\n",
    "    Calls harmonize with random initialization and C=0, masking none \n",
    "    and so generates a new sample that sounds like Bach.\n",
    "    \"\"\"\n",
    "    y = np.random.randint(P, size=(I, T)).astype(int)\n",
    "    C = np.zeros((I, T)).astype(int)\n",
    "    x = harmonize(y, C, model)\n",
    "    return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "24hPqElFbKhk"
   },
   "outputs": [],
   "source": [
    "hidden_size = 32\n",
    "\n",
    "class Unit(nn.Module):\n",
    "    \"\"\"\n",
    "    Two convolution layers each followed by batchnorm and relu, \n",
    "    plus a residual connection.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Unit, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(hidden_size, hidden_size, 3, padding=1)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(hidden_size, hidden_size, 3, padding=1)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        y = self.conv1(y)\n",
    "        y = self.batchnorm1(y)\n",
    "        y = self.relu1(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.batchnorm2(y)\n",
    "        y = y + x\n",
    "        y = self.relu2(y)\n",
    "        return y\n",
    "    \n",
    "    \n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    A CNN that where you input a starter chorale and a mask and it outputs a prediction for the values\n",
    "    in the starter chorale away from the mask that are most like the training data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.initial_conv = nn.Conv2d(2*I, hidden_size, 3, padding=1)\n",
    "        self.initial_batchnorm = nn.BatchNorm2d(hidden_size)\n",
    "        self.initial_relu = nn.ReLU()\n",
    "        self.unit1 = Unit()\n",
    "        self.unit2 = Unit()\n",
    "        self.unit3 = Unit()\n",
    "        self.unit4 = Unit()\n",
    "        self.unit5 = Unit()\n",
    "        self.unit6 = Unit()\n",
    "        self.unit7 = Unit()\n",
    "        self.unit8 = Unit()\n",
    "        self.unit9 = Unit()\n",
    "        self.unit10 = Unit()\n",
    "        self.unit11 = Unit()\n",
    "        self.unit12 = Unit()\n",
    "        self.unit13 = Unit()\n",
    "        self.unit14 = Unit()\n",
    "        self.unit15 = Unit()\n",
    "        self.unit16 = Unit()\n",
    "        self.affine = nn.Linear(hidden_size*T*P, I*T*P)\n",
    "        \n",
    "    def forward(self, x, C):\n",
    "        # x is a tensor of shape (N, I, T, P)\n",
    "        # C is a tensor of 0s and 1s of shape (N, I, T)\n",
    "        # returns a tensor of shape (N, I, T, P)\n",
    "        \n",
    "        # get the number of batches\n",
    "        N = x.shape[0]\n",
    "        \n",
    "        # tile the array C out of a tensor of shape (N, I, T, P)\n",
    "        tiled_C = C.view(N, I, T, 1)\n",
    "        tiled_C = tiled_C.repeat(1, 1, 1, P)\n",
    "        \n",
    "        # mask x and combine it with the mask to produce a tensor of shape (N, 2*I, T, P)\n",
    "        y = torch.cat((tiled_C*x, tiled_C), dim=1)\n",
    "        \n",
    "        # apply the convolution and relu layers\n",
    "        y = self.initial_conv(y)\n",
    "        y = self.initial_batchnorm(y)\n",
    "        y = self.initial_relu(y)\n",
    "        y = self.unit1(y)\n",
    "        y = self.unit2(y)\n",
    "        y = self.unit3(y)\n",
    "        y = self.unit4(y)\n",
    "        y = self.unit5(y)\n",
    "        y = self.unit6(y)\n",
    "        y = self.unit7(y)\n",
    "        y = self.unit8(y)\n",
    "        y = self.unit9(y)\n",
    "        y = self.unit10(y)\n",
    "        y = self.unit11(y)\n",
    "        y = self.unit12(y)\n",
    "        y = self.unit13(y)\n",
    "        y = self.unit14(y)\n",
    "        y = self.unit15(y)\n",
    "        y = self.unit16(y)\n",
    "            \n",
    "        # reshape before applying the fully connected layer\n",
    "        y = y.view(N, hidden_size*T*P)\n",
    "        y = self.affine(y)\n",
    "        \n",
    "        # reshape to (N, I, T, P)\n",
    "        y = y.view(N, I, T, P)\n",
    "                \n",
    "        return y\n",
    "    \n",
    "    def pred(self, y, C):\n",
    "        # y is an array of shape (I, T) with integer entries in [0, P)\n",
    "        # C is an array of shape (I, T) consisting of 0s and 1s\n",
    "        # the entries of y away from the support of C should be considered 'unknown'\n",
    "        \n",
    "        # x is shape (I, T, P) one-hot representation of y\n",
    "        compressed = y.reshape(-1)\n",
    "        x = np.zeros((I*T, P))\n",
    "        r = np.arange(I*T)\n",
    "        x[r, compressed] = 1\n",
    "        x = x.reshape(I, T, P)\n",
    "        \n",
    "        # prep x and C for the plugging into the model\n",
    "        x = torch.tensor(x).type(torch.FloatTensor).to(device)\n",
    "        x = x.view(1, I, T, P)\n",
    "        C2 = torch.tensor(C).type(torch.FloatTensor).view(1, I, T).to(device)\n",
    "        \n",
    "        # plug x and C2 into the model\n",
    "        with torch.no_grad():\n",
    "            out = self.forward(x, C2).view(I, T, P).cpu().numpy()\n",
    "            out = out.transpose(2, 0, 1) # shape (P, I, T)\n",
    "            probs = np.exp(out) / np.exp(out).sum(axis=0) # shape (P, I, T)\n",
    "            cum_probs = np.cumsum(probs, axis=0) # shape (P, I, T)\n",
    "            u = np.random.rand(I, T) # shape (I, T)\n",
    "            return np.argmax(cum_probs > u, axis=0)         \n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xm0YK6yGbZg1"
   },
   "outputs": [],
   "source": [
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bucYvJ5u7fyl",
    "outputId": "3544bbfd-62af-42b2-cf60-5a0750028bd6"
   },
   "outputs": [],
   "source": [
    "# uncomment to load the previously trained model\n",
    "model.load_state_dict(torch.load('model1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "def start_logger(fname = 'coconet.log'):\n",
    "      logger = logging.getLogger()\n",
    "      fhandler = logging.FileHandler(filename=fname, mode='w')\n",
    "      formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "      fhandler.setFormatter(formatter)\n",
    "      logger.addHandler(fhandler)\n",
    "      logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "B73-YuqDZniN",
    "outputId": "766698bd-e74a-4dd6-84bc-3cf60de9ef1a"
   },
   "source": [
    "# try out the Chorale class functionality with training samples\n",
    "<code>\n",
    "start_logger()\n",
    "high_c = -np.inf\n",
    "low_c = np.inf\n",
    "s_high_c = 0\n",
    "s_low_c = 0\n",
    "high_n = -np.inf\n",
    "low_n = np.inf\n",
    "s_high_n = 0\n",
    "s_low_n = 0\n",
    "logging.info('training sample: samp consonance score: cons note_score: note')\n",
    "logging.info('samp cons note')\n",
    "for training_samples in range(len(all_tracks)):\n",
    "    track = train_tracks[training_samples]\n",
    "    chorale = Chorale(track)\n",
    "    scores = chorale.score()\n",
    "    if scores[0] > high_c: \n",
    "        high_c = scores[0]\n",
    "        s_high_c = training_samples\n",
    "    if scores[0] < low_c: \n",
    "        low_c = scores[0]\n",
    "        s_low_c = training_samples\n",
    "    if scores[1] > high_n: \n",
    "        high_n = scores[1]\n",
    "        s_high_n = training_samples\n",
    "    if scores[1] < low_n: \n",
    "        low_n = scores[1]\n",
    "        s_low_n = training_samples\n",
    "    logging.info(f'{training_samples}    {scores[0]}   {scores[1]}')\n",
    "    # if training_samples > 5: break\n",
    "print(f'high_c: {high_c} at sample {s_high_c}, low_c: {low_c} at sample {s_low_c}')    \n",
    "print(f'high_n: {high_n} at sample {s_high_n}, low_n: {low_n} at sample {s_low_n}')    \n",
    "<code>\n",
    "## I really only had to do this once, and I saved the results    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "B73-YuqDZniN",
    "outputId": "766698bd-e74a-4dd6-84bc-3cf60de9ef1a"
   },
   "outputs": [],
   "source": [
    "score_sample = 3345 # lowest consonance at score of 142\n",
    "track = train_tracks[score_sample]\n",
    "chorale = Chorale(track)\n",
    "scores = chorale.score()\n",
    "print(f'{score_sample}    {scores[0]}   {scores[1]}')\n",
    "chorale.to_image()\n",
    "chorale.play()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "B73-YuqDZniN",
    "outputId": "766698bd-e74a-4dd6-84bc-3cf60de9ef1a"
   },
   "outputs": [],
   "source": [
    "score_sample = 2367 # most notes at 49\n",
    "track = train_tracks[score_sample]\n",
    "chorale = Chorale(track)\n",
    "scores = chorale.score()\n",
    "print(f'{score_sample}    {scores[0]}   {scores[1]}')\n",
    "chorale.to_image()\n",
    "chorale.play()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "B73-YuqDZniN",
    "outputId": "766698bd-e74a-4dd6-84bc-3cf60de9ef1a"
   },
   "outputs": [],
   "source": [
    "score_sample = 3 # this one was most consanant and fewest notes\n",
    "track = train_tracks[score_sample]\n",
    "chorale = Chorale(track)\n",
    "scores = chorale.score()\n",
    "print(f'{score_sample}    {scores[0]}   {scores[1]}')\n",
    "chorale.to_image()\n",
    "chorale.play()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "B73-YuqDZniN",
    "outputId": "766698bd-e74a-4dd6-84bc-3cf60de9ef1a"
   },
   "outputs": [],
   "source": [
    "# # let's try out a chorale generated by the model, \n",
    "# elaborating on the bass track of the last example\n",
    "# print('-------------')\n",
    "new_chorale = Chorale(chorale.elaborate_on_voices([3], model)) # chorale was ??\n",
    "print(new_chorale)\n",
    "new_chorale.to_image()\n",
    "new_chorale.play()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "7D85oUla-MBO",
    "outputId": "61b45332-f15b-44a6-963e-8f8758c5ce9a"
   },
   "outputs": [],
   "source": [
    "# let's try harmonizing a simple melody\n",
    "\n",
    "melody = [66, 66, 66, 66, 71, 71, 71, 71, 73, 73, 73, 73, 75, 75, 75, 75,\n",
    "         76, 76, 75, 73, 71, 71, 75, 75, 73, 73, 70, 70, 71, 71, 71, 71]\n",
    "\n",
    "y = np.random.randint(P, size=(I, T)) # assign random numbers 0 to 56 to an array of shape(4,32)\n",
    "print(f'y.shape: {y.shape}')\n",
    "print(f'P: {P}, I: {I}, T: {T}')\n",
    "y[0] = np.array(melody)-30\n",
    "D0 = np.ones((1, T)).astype(int) # mask the soprano part off from alteration.\n",
    "D1 = np.zeros((3, T)).astype(int) # move the other notes around as much as you need to.\n",
    "D = np.concatenate([D0, D1], axis=0) # build the mask\n",
    "\n",
    "for _ in range(1):\n",
    "    chorale = Chorale(harmonize(y, D, model)) # pass the y (4x32), the mask, and the model\n",
    "    scores = chorale.score()\n",
    "    print(f'consonance & note scores: {scores}')\n",
    "    chorale.to_image()\n",
    "    plt.show()\n",
    "    chorale.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "T5a1oVjIAfQR",
    "outputId": "a492e787-037e-40f7-a53e-e88f6c174110"
   },
   "outputs": [],
   "source": [
    "# let's do some more overfitting investigation\n",
    "# this sample has a suspiciously compelling bass line\n",
    "\n",
    "sample = [[74, 70, 65, 58], [74, 70, 65, 58], [74, 70, 65, 57], [74, 70, 65, 57],\n",
    "          [74, 70, 67, 55], [74, 70, 67, 55], [72, 69, 65, 53], [72, 69, 65, 53],\n",
    "          [70, 70, 67, 55], [70, 70, 67, 55], [70, 69, 67, 51], [70, 67, 67, 51],\n",
    "          [69, 69, 60, 53], [69, 69, 60, 53], [70, 65, 62, 50], [70, 65, 62, 50], \n",
    "          [72, 67, 63, 53], [72, 67, 63, 53], [72, 67, 57, 51], [72, 67, 57, 51], \n",
    "          [70, 65, 65, 46], [70, 65, 65, 46], [70, 65, 65, 46], [70, 65, 65, 46], \n",
    "          [70, 65, 62, 46], [70, 65, 62, 46], [70, 65, 62, 46], [70, 65, 62, 46], \n",
    "          [70, 65, 62, 46], [70, 65, 62, 46], [70, 65, 62, 46], [70, 65, 62, 46]]\n",
    "chorale = Chorale(np.array(sample).transpose(), subtract_30=True)\n",
    "scores = chorale.score()\n",
    "print(f'consonance & note scores: {scores}')\n",
    "chorale.play()\n",
    "chorale.to_image()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "T5a1oVjIAfQR",
    "outputId": "a492e787-037e-40f7-a53e-e88f6c174110"
   },
   "outputs": [],
   "source": [
    "sample = (np.array(sample)-30).transpose()\n",
    "\n",
    "bass_first_measure = sample[3, :16]\n",
    "\n",
    "training_bass_first_measures = train_tracks[:, 3, :16]\n",
    "\n",
    "sq_diff = np.power(bass_first_measure - training_bass_first_measures, 2)\n",
    "distances = np.sum(sq_diff, axis=1)\n",
    "\n",
    "distances_as_series = pd.Series(distances).sort_values()\n",
    "candidates = list(distances_as_series.index[:5])\n",
    "print(candidates)\n",
    "\n",
    "for c in candidates:\n",
    "    candidate_chorale = Chorale(train_tracks[c])\n",
    "    scores = candidate_chorale.score()\n",
    "    print(f'consonance & note scores: {scores}')\n",
    "    candidate_chorale.play()\n",
    "    candidate_chorale.to_image()\n",
    "    plt.show()\n",
    "#     track = train_tracks[c]\n",
    "#     print((track + 30).transpose().tolist())\n",
    "    \n",
    "# verdict: the sample simply noticed something which recurs in the chorales, \n",
    "# without copying it directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_number(n):\n",
    "    \"\"\"\n",
    "    prepare numbers for better file storage\n",
    "    \"\"\"\n",
    "    if n == 0:\n",
    "        return '00000'\n",
    "    else:\n",
    "        digits = int(np.ceil(np.log10(n)))\n",
    "        pad_zeros = 5 - digits\n",
    "        return '0'* pad_zeros + str(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I have hopelessly screwed up this save to midi file routine. Recover, recover, recover\n",
    "# create a random chorale using the harmonization method\n",
    "def save_midi_chorale(chorale, id_number):\n",
    "    \"\"\"\n",
    "    Save an existing chorale in a midi file named {id_number}midi.mid\n",
    "    \"\"\"  \n",
    "    prediction = chorale\n",
    "    print(f'chorale type: {type(chorale)}')\n",
    "    prediction = prediction.transpose().tolist()\n",
    "    prediction = np.array(prediction)\n",
    "    midi_output = piano_roll_to_midi(prediction)\n",
    "    save_name = str(pad_number(id_number)) + 'midi.mid'\n",
    "    midi_output.save(save_name)    \n",
    "\n",
    "def save_midi_random(id_number):\n",
    "    \"\"\"\n",
    "    Generate an artificial chorale from a random seed \n",
    "    \"\"\"\n",
    "    prediction = generate_random_chorale(model) + 30 # 30 back on before passing to piano_roll_to_midi\n",
    "    save_midi_chorale(prediction, id_number)\n",
    "\n",
    "def save_midi_harm(base_chorale, keep, id_number):\n",
    "    \"\"\"\n",
    "    Keep one voice and harmonize around it with the other three.\n",
    "    \"\"\"\n",
    "    chorale_type = Chorale(base_chorale, subtract_30=True)\n",
    "    chorale = chorale_type.elaborate_on_voices([keep], model)\n",
    "    save_midi_chorale(chorale + 30, id_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "B73-YuqDZniN",
    "outputId": "766698bd-e74a-4dd6-84bc-3cf60de9ef1a"
   },
   "outputs": [],
   "source": [
    "start_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "B73-YuqDZniN",
    "outputId": "766698bd-e74a-4dd6-84bc-3cf60de9ef1a"
   },
   "outputs": [],
   "source": [
    "sample = [[74, 70, 65, 58], [74, 70, 65, 58], [74, 70, 65, 57], [74, 70, 65, 57],\n",
    "          [74, 70, 67, 55], [74, 70, 67, 55], [72, 69, 65, 53], [72, 69, 65, 53],\n",
    "          [70, 70, 67, 55], [70, 70, 67, 55], [70, 69, 67, 51], [70, 67, 67, 51],\n",
    "          [69, 69, 60, 53], [69, 69, 60, 53], [70, 65, 62, 50], [70, 65, 62, 50], \n",
    "          [72, 67, 63, 53], [72, 67, 63, 53], [72, 67, 57, 51], [72, 67, 57, 51], \n",
    "          [70, 65, 65, 46], [70, 65, 65, 46], [70, 65, 65, 46], [70, 65, 65, 46], \n",
    "          [70, 65, 62, 46], [70, 65, 62, 46], [70, 65, 62, 46], [70, 65, 62, 46], \n",
    "          [70, 65, 62, 46], [70, 65, 62, 46], [70, 65, 62, 46], [70, 65, 62, 46]]\n",
    "chorale = np.array(sample).transpose()\n",
    "save_midi_chorale(chorale,30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "B73-YuqDZniN",
    "outputId": "766698bd-e74a-4dd6-84bc-3cf60de9ef1a"
   },
   "outputs": [],
   "source": [
    "metrics = []\n",
    "logging.info('start logging')\n",
    "logging.info(f'      file       pit pit pit  pitc poly   poly conso class')\n",
    "logging.info(f'      name       ran use clas entr phony  rate nant  entr')\n",
    "for examples in range(1):\n",
    "    keep = 3 # keep one of the melody 0, alto 1, tenor 2, or bass 3\n",
    "    save_midi_harm(chorale, keep, 30000 + examples) \n",
    "    music = muspy.read(filename)\n",
    "    metric = [os.path.basename(filename),\n",
    "        muspy.pitch_range(music),\n",
    "        muspy.n_pitches_used(music),\n",
    "        muspy.n_pitch_classes_used(music), \n",
    "        round(muspy.pitch_entropy(music),2),\n",
    "        round(muspy.polyphony(music),2), \n",
    "        round(muspy.polyphony_rate(music),2),\n",
    "        round(muspy.scale_consistency(music),2), \n",
    "        round(muspy.pitch_class_entropy(music),2)]\n",
    "    logging.info(metric)\n",
    "    metrics.append(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=4, figsize=(20,10)) # nrows must equal number of tracks\n",
    "muspy.show_pianoroll(music, grid_linewidth = (0.24), axs=axs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try out a chorale generated by the model, elaborating on the bass track of the last example\n",
    "print('-------------')\n",
    "new_chorale = Chorale(chorale.elaborate_on_voices([3], model))\n",
    "new_chorale.to_image()\n",
    "new_chorale.play()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [[74, 70, 65, 58], [74, 70, 65, 58], [74, 70, 65, 57], [74, 70, 65, 57], \n",
    "          [74, 70, 67, 55], [74, 70, 67, 55], [72, 69, 65, 53], [72, 69, 65, 53], \n",
    "          [70, 70, 67, 55], [70, 70, 67, 55], [70, 69, 67, 51], [70, 67, 67, 51], \n",
    "          [69, 69, 60, 53], [69, 69, 60, 53], [70, 65, 62, 50], [70, 65, 62, 50], \n",
    "          [72, 67, 63, 53], [72, 67, 63, 53], [72, 67, 57, 51], [72, 67, 57, 51], \n",
    "          [70, 65, 65, 46], [70, 65, 65, 46], [70, 65, 65, 46], [70, 65, 65, 46], \n",
    "          [70, 65, 62, 46], [70, 65, 62, 46], [70, 65, 62, 46], [70, 65, 62, 46], \n",
    "          [70, 65, 62, 46], [70, 65, 62, 46], [70, 65, 62, 46], [70, 65, 62, 46]]\n",
    "chorale = Chorale(np.array(sample).transpose(), subtract_30=True)\n",
    "new_chorale.to_image()\n",
    "chorale.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in a midi file, check the key, load into piano roll, set up np.array containing Nx4 sample.\n",
    "# calling program should slice the returned array as needed\n",
    "def midi_to_input(midi_file):\n",
    "    music = muspy.read(midi_file)\n",
    "    if music.key_signatures != []: # check if the midi file includes a key signature - some don't\n",
    "        root = music.key_signatures[0].root \n",
    "        mode = music.key_signatures[0].mode # major or minor\n",
    "    else: \n",
    "        print('Warning: no key signature found. Assuming C major')\n",
    "        mode = \"major\"\n",
    "        root = 0    \n",
    "    if music.time_signatures != []: # check if the midi file includes a time signature - some don't\n",
    "        numerator = music.time_signatures[0].numerator\n",
    "        denominator = music.time_signatures[0].denominator \n",
    "    else: \n",
    "        print('Warning: no time signature found. Assuming 4/4')\n",
    "    # turn it into a piano roll\n",
    "    piano_roll = muspy.to_pianoroll_representation(music,encode_velocity=False) # boolean piano roll if False, default True\n",
    "    # print(piano_roll.shape) # should be one time step for every click in the midi file\n",
    "    q = music.resolution # quarter note value in this midi file. \n",
    "    q16 = q // 4 # my desired resolution is by 1/16th notes\n",
    "    print(f'time signatures: {numerator}/{denominator}')\n",
    "    time_steps = piano_roll.shape[0] // q16\n",
    "    print(f'music.resolution is q: {q}. q16: {q16} time_steps: {time_steps} 1/16th notes')\n",
    "    sample= np.zeros(shape=(time_steps,4)).astype(int) # default is float unless .astype(int)\n",
    "    # This loop is able to load an array of shape N,4 with the notes that are being played in each time step\n",
    "    for click in range(0,piano_roll.shape[0],q16): # q16 is skip 240 steps for 1/16th note resolution\n",
    "        voice = 3 # start with the low voices and decrement for the higher voices as notes get higher\n",
    "        for i in range(piano_roll.shape[1]): # check if any notes are non-zero\n",
    "            time_interval = (click) // q16 \n",
    "            if (piano_roll[click][i]): # if velocity anything but zero - unless you set encode_velocity = False\n",
    "                # if time_interval % 16 == 0:\n",
    "                #     print(f'time step: {click} at index {i}, time_interval: {time_interval}, voice: {voice}')\n",
    "                sample[time_interval][voice] = i - root # index to the piano roll with a note - transpose\n",
    "                voice -= 1 # next instrument will get the higher note\n",
    "    return (sample,root,mode)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ('C ','C#','D ','D#','E ','F ','F#','G ','G#','A ','A#','B ')\n",
    "midi_file = 'ein_feste_burg.mid'\n",
    "sample,root,mode = midi_to_input(midi_file)\n",
    "print(f'file in key of {keys[root]} {mode}')\n",
    "print(f'sample.shape:{sample.shape}. contains {sample.shape[0] // 4} quarter notes')\n",
    "# To quote documentation, the basic slice syntax is i:j:k \n",
    "# where i is the starting index, j is the stopping index, and k is the step (when k > 0).\n",
    "start_16th_note = 3*4 # skip the first three quarter notes for this one.\n",
    "two_measures = sample[start_16th_note:32+start_16th_note]\n",
    "input = np.array(two_measures.transpose())\n",
    "chorale = Chorale(input, subtract_30=True)\n",
    "scores = chorale.score()\n",
    "print(f'consonance and note count {scores[0]}   {scores[1]}')\n",
    "chorale.to_image()\n",
    "chorale.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = 0 # keep one of the melody 0, alto 1, tenor 2, or bass 3\n",
    "new_chorale = Chorale(chorale.elaborate_on_voices([keep], model)) \n",
    "new_chorale.to_image()\n",
    "scores = new_chorale.score()\n",
    "print(f'consonance and note count {scores[0]}   {scores[1]}')\n",
    "new_chorale.play()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "coconet_reorganized.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
