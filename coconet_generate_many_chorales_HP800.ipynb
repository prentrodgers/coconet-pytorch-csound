{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a Lots of Chorales from \"Schmucke dich, o liebe Seele\" by JS Bach BWV 180 \n",
    "<p>The goals of this round:\n",
    "    \n",
    "- Create many 16 voice chorales and save them to files\n",
    "    \n",
    " </p>\n",
    "<p>Please note that the Csound instance in this notebook require sample files of a Bosendorfer piano which are licensed and cannot be included in the repository. The calls to Csound won't work without significant installation and configuration work.</P>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gTZ5xE7jaVy0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mido\n",
    "import time\n",
    "from midi2audio import FluidSynth\n",
    "from IPython.display import Audio, display\n",
    "import os\n",
    "import muspy\n",
    "import piano \n",
    "import subprocess\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng(42) # random seed in parens.\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "softmax = torch.nn.functional.softmax\n",
    "\n",
    "base_dir = ''\n",
    "CSD_FILE = 'goldberg_aria1.csd'\n",
    "NOTES_FILE = \"goldberg_aria1.mac.csv\"\n",
    "LOGNAME = 'goldberg5.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30471707975443135\n"
     ]
    }
   ],
   "source": [
    "print(rng.standard_normal())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the csd file in prep for sending notes to Csound\n",
    "This function creates a csound instance by loading a csd file, \n",
    "returning the ctcsound.CsoundPerformanceThread object and the ctcsound.Csound object\n",
    "the caller can then send score events to the pt (CsoundPerformanceThread) object, and \n",
    "when finished, close the ctcsout.Csound object.\n",
    "Caller needs to assemble a python list that looks like this:\n",
    "\n",
    "<code>\n",
    "#                  Inst Start  Dur   Vel   Ton  Oct  Voic Ste Envl Glis Upsm Renv 2-gl 3r-gl  Mult chan\n",
    "# pfields contains [1, 0.3125, 1.0, 68.0, 90.0, 5.0, 1.0, 7.0, 0.0, 0.0, 0.0, 1.0, 0.0, 4.0, 10.0, 2.0]\n",
    "\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csound_piano():\n",
    "    piano.start_logger(fname=LOGNAME)\n",
    "    piano.logging.info(f'Logging messages to: {LOGNAME}')\n",
    "    csd_content, lines = piano.load_csd(CSD_FILE)\n",
    "    piano.logging.info(f'Loaded the csd file {CSD_FILE}. There are {lines} lines read containg {len(csd_content)} bytes')\n",
    "    cs, pt = piano.load_csound(csd_content)\n",
    "    return (pt,cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "colab_type": "code",
    "id": "oDwOLhiCOw4C",
    "outputId": "e3d3bb86-94d7-4c0c-ad7d-a8f478a1ef9e"
   },
   "outputs": [],
   "source": [
    "def play_midi(path,volume): # 5 is half volume, 10 is max\n",
    "    \"\"\"\n",
    "    A script for playing the midi files using FluidSynth in the notebook. \n",
    "    path is the path to the midi file to be played\n",
    "    \"\"\"\n",
    "    if os.path.exists('test.wav'):\n",
    "        os.remove('test.wav')\n",
    "        print(f'path: {path}')\n",
    "    subprocess.run(['fluidsynth', '-ni', '-g', str(volume), 'font.sf2', path, '-F', 'test.wav'])\n",
    "    audio = Audio('test.wav')\n",
    "    display(audio)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ogxsHxCgak0U"
   },
   "outputs": [],
   "source": [
    "def piano_roll_to_midi(piece):\n",
    "    \"\"\"\n",
    "    piece is a an array of shape (T, 4) for some T.\n",
    "    The (i,j)th entry of the array is the midi pitch of the jth voice at time i. It's an integer in range(128).\n",
    "    outputs a mido object mid that you can convert to a midi file by called its .save() method\n",
    "    \"\"\"\n",
    "    piece = np.concatenate([piece, [[np.nan, np.nan, np.nan, np.nan]]], axis=0)\n",
    "\n",
    "    bpm = 50\n",
    "    microseconds_per_beat = 60 * 1000000 / bpm\n",
    "\n",
    "    mid = mido.MidiFile()\n",
    "    \n",
    "    tracks = {'soprano': mido.MidiTrack(), 'alto': mido.MidiTrack(),\n",
    "              'tenor': mido.MidiTrack(), 'bass': mido.MidiTrack()}\n",
    "    past_pitches = {'soprano': np.nan, 'alto': np.nan,\n",
    "                    'tenor': np.nan, 'bass': np.nan}\n",
    "    delta_time = {'soprano': 0, 'alto': 0, 'tenor': 0, 'bass': 0}\n",
    "\n",
    "\n",
    "    # create a track containing tempo data\n",
    "    metatrack = mido.MidiTrack()\n",
    "    metatrack.append(mido.MetaMessage('set_tempo',\n",
    "                                      tempo=int(microseconds_per_beat), time=0))\n",
    "    mid.tracks.append(metatrack)\n",
    "\n",
    "    # create the four voice tracks\n",
    "    for voice in tracks:\n",
    "        mid.tracks.append(tracks[voice])\n",
    "        tracks[voice].append(mido.Message(\n",
    "            'program_change', program=0, time=0)) # choir aahs=52, piano = 0\n",
    "\n",
    "    # add notes to the four voice tracks\n",
    "    for i in range(len(piece)):\n",
    "        pitches = {'soprano': piece[i, 0], 'alto': piece[i, 1],\n",
    "                   'tenor': piece[i, 2], 'bass': piece[i, 3]}\n",
    "        for voice in tracks:\n",
    "            if np.isnan(past_pitches[voice]):\n",
    "                past_pitches[voice] = None\n",
    "            if np.isnan(pitches[voice]):\n",
    "                pitches[voice] = None\n",
    "            if pitches[voice] != past_pitches[voice]:\n",
    "                if past_pitches[voice]:\n",
    "                    tracks[voice].append(mido.Message('note_off', note=int(past_pitches[voice]),\n",
    "                                                      velocity=64, time=delta_time[voice]))\n",
    "                    delta_time[voice] = 0\n",
    "                if pitches[voice]:\n",
    "                    tracks[voice].append(mido.Message('note_on', note=int(pitches[voice]),\n",
    "                                                      velocity=64, time=delta_time[voice]))\n",
    "                    delta_time[voice] = 0\n",
    "            past_pitches[voice] = pitches[voice]\n",
    "            # 480 ticks per beat and each line of the array is a 16th note\n",
    "            delta_time[voice] += 120\n",
    "\n",
    "    return mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sGQCWjlCbGAu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I voices: 4, T sample length: 32, P number of distinct pitches in the input chorales: 57\n"
     ]
    }
   ],
   "source": [
    "# set global variables\n",
    "\n",
    "I = 4 # number of voices\n",
    "T = 32 # length of samples (32 = two 4/4 measures in 1/16th note increments)\n",
    "P = (86-30) +1 # number of different pitches\n",
    "print(f'I voices: {I}, T sample length: {T}, P number of distinct pitches in the input chorales: {P}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ogxsHxCgak0U"
   },
   "outputs": [],
   "source": [
    "class Chorale:\n",
    "    \"\"\"\n",
    "    A class to store and manipulate an array self.arr that stores a chorale.\n",
    "    \"\"\"\n",
    "    def __init__(self, arr, subtract_30=False):\n",
    "        # arr is an array of shape (4, 32) with values in range(0, 57)\n",
    "        self.arr = arr.copy()\n",
    "        if subtract_30:\n",
    "            self.arr -= 30\n",
    "            \n",
    "        # the one_hot representation of the array\n",
    "        reshaped = self.arr.reshape(-1)\n",
    "        self.one_hot = np.zeros((I*T, P))\n",
    "        r = np.arange(I*T)\n",
    "        self.one_hot[r, reshaped] = 1\n",
    "        self.one_hot = self.one_hot.reshape(I, T, P)\n",
    "        \n",
    "\n",
    "    def to_image(self):\n",
    "        # visualize the four tracks as a images\n",
    "        soprano = self.one_hot[0].transpose()\n",
    "        alto = self.one_hot[1].transpose()\n",
    "        tenor = self.one_hot[2].transpose()\n",
    "        bass = self.one_hot[3].transpose()\n",
    "        \n",
    "        fig, axs = plt.subplots(1, 4)\n",
    "        axs[0].imshow(np.flip(soprano, axis=0), cmap='hot', interpolation='nearest')\n",
    "        axs[0].set_title('soprano')\n",
    "        axs[1].imshow(np.flip(alto, axis=0), cmap='hot', interpolation='nearest')\n",
    "        axs[1].set_title('alto')\n",
    "        axs[2].imshow(np.flip(tenor, axis=0), cmap='hot', interpolation='nearest')\n",
    "        axs[2].set_title('tenor')\n",
    "        axs[3].imshow(np.flip(bass, axis=0), cmap='hot', interpolation='nearest')\n",
    "        axs[3].set_title('bass')\n",
    "        fig.set_figheight(5)\n",
    "        fig.set_figwidth(15)\n",
    "        return fig, axs\n",
    "    \n",
    "    def play(self, filename='midi_track.mid'):\n",
    "        # display an in-notebook widget for playing audio\n",
    "        # saves the midi file as a file named name in base_dir/midi_files\n",
    "        \n",
    "        midi_arr = self.arr.transpose().copy()\n",
    "        midi_arr += 30\n",
    "        midi = piano_roll_to_midi(midi_arr)\n",
    "        midi.save(base_dir + 'midi_files/' + filename)\n",
    "        play_midi('midi_files/' + filename,10)\n",
    "        \n",
    "    def elaborate_on_voices(self, voices, model):\n",
    "        # voice is a set consisting of 0, 1, 2, or 3\n",
    "        # create a mask consisting of the given voices\n",
    "        # generate a chorale with the same voices as in voices\n",
    "        mask = np.zeros((I, T))\n",
    "        y = np.random.randint(P, size=(I, T))\n",
    "        for i in voices:\n",
    "            mask[i] = 1\n",
    "            y[i] = self.arr[i].copy()\n",
    "        return harmonize(y, mask, model)\n",
    "    \n",
    "    # I think we could improve this scoring method. It's pretty lame.\n",
    "    def score(self):\n",
    "        consonance_dict = {0: 1, 1: 0, 2: 0, 3: 1, 4: 1, 5: 1, 6: 0, \n",
    "                           7: 1, 8: 1, 9: 1, 10: 0, 11: 0}\n",
    "        consonance_score = 0\n",
    "        for k in range(32):\n",
    "            for i in range(4):\n",
    "                for j in range(i):\n",
    "                    consonance_score += consonance_dict[((self.arr[i, k] - self.arr[j, k]) % 12)]\n",
    "        \n",
    "        note_score = 0\n",
    "        for i in range(4):\n",
    "            for j in range(1, 32):\n",
    "                if self.arr[i, j] != self.arr[i, j-1]:\n",
    "                    note_score += 1\n",
    "        return consonance_score, note_score\n",
    "        \n",
    "# harmonize a melody\n",
    "def harmonize(y, C, model):\n",
    "    \"\"\"\n",
    "    Generate an artificial Bach Chorale starting with y, and keeping the pitches\n",
    "    where C==1.\n",
    "    Here C is an array of shape (4, 32) whose entries are 0 and 1.\n",
    "    The pitches outside of C are repeatedly resampled to generate new values.\n",
    "    For example, to harmonize the soprano line, let y be random except y[0] \n",
    "    contains the soprano line, let C[1:] be 0 and C[0] be 1.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = y\n",
    "        C2 = C.copy()\n",
    "        num_steps = int(2*I*T)\n",
    "        alpha_max = .999\n",
    "        alpha_min = .001\n",
    "        eta = 3/4\n",
    "        for i in range(num_steps):\n",
    "            p = np.maximum(alpha_min, alpha_max - i*(alpha_max-alpha_min)/(eta*num_steps))\n",
    "            sampled_binaries = np.random.choice(2, size = C.shape, p=[p, 1-p])\n",
    "            C2 += sampled_binaries\n",
    "            C2[C==1] = 1\n",
    "            x_cache = x\n",
    "            x = model.pred(x, C2)\n",
    "            x[C2==1] = x_cache[C2==1]\n",
    "            C2 = C.copy()\n",
    "        return x\n",
    "    \n",
    "def generate_random_chorale(model): # \n",
    "    \"\"\"\n",
    "    Calls harmonize with random initialization and C=0, masking none \n",
    "    and so generates a new sample that sounds like Bach.\n",
    "    \"\"\"\n",
    "    y = np.random.randint(P, size=(I, T)).astype(int)\n",
    "    C = np.zeros((I, T)).astype(int)\n",
    "    x = harmonize(y, C, model)\n",
    "    return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "24hPqElFbKhk"
   },
   "outputs": [],
   "source": [
    "hidden_size = 32\n",
    "\n",
    "class Unit(nn.Module):\n",
    "    \"\"\"\n",
    "    Two convolution layers each followed by batchnorm and relu, \n",
    "    plus a residual connection.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Unit, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(hidden_size, hidden_size, 3, padding=1)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(hidden_size, hidden_size, 3, padding=1)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        y = self.conv1(y)\n",
    "        y = self.batchnorm1(y)\n",
    "        y = self.relu1(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.batchnorm2(y)\n",
    "        y = y + x\n",
    "        y = self.relu2(y)\n",
    "        return y\n",
    "    \n",
    "    \n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    A CNN that where you input a starter chorale and a mask and it outputs a prediction for the values\n",
    "    in the starter chorale away from the mask that are most like the training data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.initial_conv = nn.Conv2d(2*I, hidden_size, 3, padding=1)\n",
    "        self.initial_batchnorm = nn.BatchNorm2d(hidden_size)\n",
    "        self.initial_relu = nn.ReLU()\n",
    "        self.unit1 = Unit()\n",
    "        self.unit2 = Unit()\n",
    "        self.unit3 = Unit()\n",
    "        self.unit4 = Unit()\n",
    "        self.unit5 = Unit()\n",
    "        self.unit6 = Unit()\n",
    "        self.unit7 = Unit()\n",
    "        self.unit8 = Unit()\n",
    "        self.unit9 = Unit()\n",
    "        self.unit10 = Unit()\n",
    "        self.unit11 = Unit()\n",
    "        self.unit12 = Unit()\n",
    "        self.unit13 = Unit()\n",
    "        self.unit14 = Unit()\n",
    "        self.unit15 = Unit()\n",
    "        self.unit16 = Unit()\n",
    "        self.affine = nn.Linear(hidden_size*T*P, I*T*P)\n",
    "        \n",
    "    def forward(self, x, C):\n",
    "        # x is a tensor of shape (N, I, T, P)\n",
    "        # C is a tensor of 0s and 1s of shape (N, I, T)\n",
    "        # returns a tensor of shape (N, I, T, P)\n",
    "        \n",
    "        # get the number of batches\n",
    "        N = x.shape[0]\n",
    "        \n",
    "        # tile the array C out of a tensor of shape (N, I, T, P)\n",
    "        tiled_C = C.view(N, I, T, 1)\n",
    "        tiled_C = tiled_C.repeat(1, 1, 1, P)\n",
    "        \n",
    "        # mask x and combine it with the mask to produce a tensor of shape (N, 2*I, T, P)\n",
    "        y = torch.cat((tiled_C*x, tiled_C), dim=1)\n",
    "        \n",
    "        # apply the convolution and relu layers\n",
    "        y = self.initial_conv(y)\n",
    "        y = self.initial_batchnorm(y)\n",
    "        y = self.initial_relu(y)\n",
    "        y = self.unit1(y)\n",
    "        y = self.unit2(y)\n",
    "        y = self.unit3(y)\n",
    "        y = self.unit4(y)\n",
    "        y = self.unit5(y)\n",
    "        y = self.unit6(y)\n",
    "        y = self.unit7(y)\n",
    "        y = self.unit8(y)\n",
    "        y = self.unit9(y)\n",
    "        y = self.unit10(y)\n",
    "        y = self.unit11(y)\n",
    "        y = self.unit12(y)\n",
    "        y = self.unit13(y)\n",
    "        y = self.unit14(y)\n",
    "        y = self.unit15(y)\n",
    "        y = self.unit16(y)\n",
    "            \n",
    "        # reshape before applying the fully connected layer\n",
    "        y = y.view(N, hidden_size*T*P)\n",
    "        y = self.affine(y)\n",
    "        \n",
    "        # reshape to (N, I, T, P)\n",
    "        y = y.view(N, I, T, P)\n",
    "                \n",
    "        return y\n",
    "    \n",
    "    def pred(self, y, C):\n",
    "        # y is an array of shape (I, T) with integer entries in [0, P)\n",
    "        # C is an array of shape (I, T) consisting of 0s and 1s\n",
    "        # the entries of y away from the support of C should be considered 'unknown'\n",
    "        \n",
    "        # x is shape (I, T, P) one-hot representation of y\n",
    "        compressed = y.reshape(-1)\n",
    "        x = np.zeros((I*T, P))\n",
    "        r = np.arange(I*T)\n",
    "        x[r, compressed] = 1\n",
    "        x = x.reshape(I, T, P)\n",
    "        \n",
    "        # prep x and C for the plugging into the model\n",
    "        x = torch.tensor(x).type(torch.FloatTensor).to(device)\n",
    "        x = x.view(1, I, T, P)\n",
    "        C2 = torch.tensor(C).type(torch.FloatTensor).view(1, I, T).to(device)\n",
    "        \n",
    "        # plug x and C2 into the model\n",
    "        with torch.no_grad():\n",
    "            out = self.forward(x, C2).view(I, T, P).cpu().numpy()\n",
    "            out = out.transpose(2, 0, 1) # shape (P, I, T)\n",
    "            probs = np.exp(out) / np.exp(out).sum(axis=0) # shape (P, I, T)\n",
    "            cum_probs = np.cumsum(probs, axis=0) # shape (P, I, T)\n",
    "            u = np.random.rand(I, T) # shape (I, T)\n",
    "            return np.argmax(cum_probs > u, axis=0)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xm0YK6yGbZg1"
   },
   "outputs": [],
   "source": [
    "model = Net().to(device) # need this in order to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bucYvJ5u7fyl",
    "outputId": "3544bbfd-62af-42b2-cf60-5a0750028bd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncomment to load the previously trained model\n",
    "model.load_state_dict(torch.load('model1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_number(n):\n",
    "    \"\"\"\n",
    "    prepare numbers for better file storage\n",
    "    \"\"\"\n",
    "    if n == 0:\n",
    "        return '00000'\n",
    "    else:\n",
    "        digits = int(np.ceil(np.log10(n)))\n",
    "        pad_zeros = 5 - digits\n",
    "        return '0'* pad_zeros + str(n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompression of model output back to the 2 1/2 measure segment.\n",
    "<p>This section turns the output of the model into a 40 slot segment from the output of the model. We compress the segment going into the model, so we decompress it coming out of the model. Decompress does several things. \n",
    "    \n",
    "- expands the end of segment for 0,1,2,3 and convert it to 4x40 array\n",
    "- fixes the end of the 4,5,6th by adding padding to convert to a 4x40 array\n",
    "    \n",
    "This will be called just after emerging from the model and before the midi file is written    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompress_end_of_segment(this_segment):\n",
    "    # this function expects a single segment of shape (4,32)\n",
    "    # this function will de-compress the last 8 1/16th notes into the space of 16 16/th notes\n",
    "    # it moves elements in the four voices, one at a time the new, larger array\n",
    "\n",
    "    expanded = np.zeros((4,40),dtype=int) # this is the original shape of the array before compression\n",
    "    # print(f'this_segment.shape: {this_segment.shape}')\n",
    "    \n",
    "    for voice in range(4): # for all voices in the segment, copy the first 32 slots into a new larger numpy array\n",
    "        # print(f'voice: {voice}')\n",
    "        for source_index in range(24): # copy the first 24 slots with no change\n",
    "            # print(f'source_index: {source_index}  ')\n",
    "            # print(f'this_segment[{voice}][{source_index}]:  {this_segment[voice][source_index]}')                  \n",
    "            expanded[voice][source_index] = this_segment[voice][source_index]\n",
    "    \n",
    "    \n",
    "    for voice in range(4): # then for each voice in the segment, spread the last 8 slots over 16 slots in the expanded array.\n",
    "        target_index = 24\n",
    "        for source_index in range(24,32):\n",
    "            # print(f'voice: {voice}, source_index: {source_index}')\n",
    "            expanded[voice,target_index] = this_segment[voice,source_index]\n",
    "            target_index += 1\n",
    "            # print(f'source_index: {source_index}, target_index starts at 24 and is now: {target_index}')\n",
    "            expanded[voice,target_index] = this_segment[voice,source_index]\n",
    "            target_index += 1\n",
    "\n",
    "    return(expanded) # return all four voices all notes in each voice. Return all 40 slots\n",
    "\n",
    "def decompress(arr):\n",
    "    s = 0\n",
    "    my_expanded_segment = np.zeros((7,4,40),dtype=int)\n",
    "    # for segments 0,1,2,3 passed into this function (that is the first ten measures of the chorale, which comprise four phrases, each 2 1/2 measures long)\n",
    "    for seg in arr: \n",
    "        if s > 3: break # process the decompression on segments 0,1,2,3. If you reach 4, stop processing\n",
    "        # print(f'arr.shape: {arr.shape}')\n",
    "        # print(f'arr[s].shape: {arr.shape[s]}')\n",
    "        my_expanded_segment[s] = decompress_end_of_segment(arr[s]) \n",
    "        s += 1\n",
    "\n",
    "    pad8 = np.zeros((4,8))  # pad the end of the segment with zeros\n",
    "    for i in range(4,7): # segments 4,5,6\n",
    "        my_expanded_segment[i] = np.concatenate((arr[i],pad8),axis=1)\n",
    "    return(my_expanded_segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpose from the key of C to the original key\n",
    "This is done to restore what the input midi file key was. I found that model inputs in the key of C are harmonized much better than those that are in other keys. I thought they took care of this in the model by transposing to different keys, but my experience suggests otherwize.\n",
    "Add the value of root (F is 5) to each note in the array, with the exception of the 0's, which have to remain the same 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_up_segment(my_segment,root):\n",
    "    new_segment = np.copy(my_segment) # just make a copy, you will change the non zero elements \n",
    "    v = 0\n",
    "    for voice in new_segment:\n",
    "        n = 0\n",
    "        for note in voice:\n",
    "            if note > 0:\n",
    "                new_segment[v,n] = note + root\n",
    "            n += 1\n",
    "        v += 1\n",
    "        \n",
    "    return(new_segment)\n",
    "\n",
    "def transpose_up(segments,root): # read in \n",
    "    s = 0\n",
    "    new_segment = np.copy(segments)\n",
    "    for seg in segments:\n",
    "        new_segment[s] = transpose_up_segment(seg,root)\n",
    "    return(new_segment)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIDI helper functions \n",
    "1. Save an array to a midi file\n",
    "2. Generate a random chorale\n",
    "3. Harmonize one particular line in a chorale and let the model figure out the other notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save a array as a midi file. It can be any length\n",
    "def save_midi_chorale(prediction, id_number):\n",
    "    \"\"\"\n",
    "    Save an existing chorale in a midi file named {id_number}midi.mid\n",
    "    \"\"\"  \n",
    "    prediction = prediction.transpose().tolist() \n",
    "    # prediction = np.array(prediction)\n",
    "    midi_output = piano_roll_to_midi(prediction)\n",
    "    save_name = str(pad_number(id_number)) + 'midi.mid'\n",
    "    # print(f'in save_midi_chorale. About to save file to: {save_name}')\n",
    "    midi_output.save(save_name)    \n",
    "    \n",
    "\n",
    "# generate a totally random chorale out of whole cloth\n",
    "def save_midi_random(id_number):\n",
    "    \"\"\"\n",
    "    Generate an artificial chorale from a random seed \n",
    "    \"\"\"\n",
    "    prediction = generate_random_chorale(model) + 30 # 30 back on before passing to piano_roll_to_midi\n",
    "    save_midi_chorale(prediction, id_number)\n",
    "    \n",
    "# keep one voice, and choose the other voices as you might if you were Bach on deadline.\n",
    "def save_midi_harm(base_chorale, keep, id_number):\n",
    "    \"\"\"\n",
    "    Keep one voice and harmonize around it with the other three. \n",
    "    Before passing to the model, the assumption is that you need to subtract 30 from the midi note numbers \n",
    "    This is so that the model never sees a number greater than 56\n",
    "    \"\"\"\n",
    "    chorale_type = Chorale(base_chorale) \n",
    "    chorale = chorale_type.elaborate_on_voices([keep], model)\n",
    "    expanded_chorale = decompress(chorale)\n",
    "    transposed_chorale = transpose_up(expanded_chorale)\n",
    "    save_midi_chorale(transposed_chorale + 30, id_number)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "def start_logger(fname = 'coconet.log'):\n",
    "      logger = logging.getLogger()\n",
    "      fhandler = logging.FileHandler(filename=fname, mode='w')\n",
    "      formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "      fhandler.setFormatter(formatter)\n",
    "      logger.addHandler(fhandler)\n",
    "      logger.setLevel(logging.DEBUG)\n",
    "start_logger()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messing with BWV 180 Schmucke dich, o liebe Seele\n",
    "This next section is dividing the above named chorale by Bach into segments that are two measures long to match the model input dimensions. Some have to be compressed, some lengthened, some are already two measures long. The requirement is for 32 1/6th note segments.\n",
    "<img src='BWV 180 Schmücke dich, o liebe Seele.png' width=\"800\" height=\"400\">\n",
    "<p>Here is a table of segments that I will create</p>\n",
    "\n",
    "| segment | note# | measures | start & | end keys |\n",
    "| :-: | :-: | :-: | :-: | :-: |\n",
    "| 0 | 1-9 | 2 1/2 | F | F |\n",
    "| 1 | 10-18 | 2 1/2 | C | F | \n",
    "| 2 | 19-27 | 2 1/2 | F | C |\n",
    "| 3 | 28-36 | 2 1/2 | F | C |\n",
    "| 4 | 37-44 | 2 | C | C |\n",
    "| 5 | 44-52 | 2 | A min | C |\n",
    "| 6 | 53 | 1 | C | C |\n",
    "\n",
    "<p>Segments 0 through 3 need to have the final 16 1/16th notes reduced to 8 1/16th notes, and then get the output of the model stretched back out again. The segments 4 and 5 don't need adjusting. Segment 6 needs to be doubled in length, by repeating the last measure a second time. When played, the chord will just be held a bit longer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--. 1 prent prent 3.7K Mar 16 11:24 '/home/prent/Downloads/chorales_018007b_(c)greentree.mid'\n"
     ]
    }
   ],
   "source": [
    "!ls -lth '/home/prent/Downloads/chorales_018007b_(c)greentree.mid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the BWV 180 Schmucke dich, o liebe Seele Chorale - nice variety of phrase lengths.\n",
    "# load a midi file into a list called sample - load the entire file, all tracks, all notes in all tracks\n",
    "# if the midi file has a key signature, it will print what it is. \n",
    "# the notes will be transposed by the loader to the key of C, by subtracting the root from each note. F = 5\n",
    "file_name = '/home/prent/Downloads/chorales_018007b_(c)greentree.mid'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a midi file into a numpy array\n",
    "Set certain values:\n",
    "\n",
    "- the numpy array of the whole piece is stored in variable \"sample'\n",
    "- store the root key and mode (F major, for example)\n",
    "- print the values of the time signature (must be 4/4 of you will need to do some extra work), quarter note clicks, clicks per 1/16th notes\n",
    "- any transpositions that must be performed to restore the original key\n",
    "- print the first 5 notes in each voice\n",
    "- print the shape of the variable \"sample\" containing the whole midi file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in a midi file, check the key, load into piano roll, set up np.array containing Nx4 sample.\n",
    "# calling program should slice the returned array as needed to create two measure segments for sending into the prediction model.\n",
    "\n",
    "def midi_to_input(midi_file):\n",
    "    music = muspy.read(midi_file)\n",
    "    if music.key_signatures != []: # check if the midi file includes a key signature - some don't\n",
    "        root = music.key_signatures[0].root \n",
    "        mode = music.key_signatures[0].mode # major or minor\n",
    "    else: \n",
    "        print('Warning: no key signature found. Assuming C major')\n",
    "        mode = \"major\"\n",
    "        root = 0    \n",
    "    if music.time_signatures != []: # check if the midi file includes a time signature - some don't\n",
    "        numerator = music.time_signatures[0].numerator\n",
    "        denominator = music.time_signatures[0].denominator \n",
    "    else: \n",
    "        print('Warning: no time signature found. Assuming 4/4')\n",
    "        numerator = 4\n",
    "        denominator = 4\n",
    "    # turn it into a piano roll\n",
    "    piano_roll = muspy.to_pianoroll_representation(music,encode_velocity=False) # boolean piano roll if False, default True\n",
    "    # print(piano_roll.shape) # should be one time step for every click in the midi file\n",
    "    q = music.resolution # quarter note value in this midi file. \n",
    "    q16 = q // 4 # my desired resolution is by 1/16th notes\n",
    "    print(f'time signatures: {numerator}/{denominator}')\n",
    "    time_steps = piano_roll.shape[0] // q16\n",
    "    print(f'music.resolution is q: {q}. q16: {q16} time_steps: {time_steps} 1/16th notes')\n",
    "    sample= np.zeros(shape=(time_steps,4)).astype(int) # default is float unless .astype(int)\n",
    "    # This loop is able to load an array of shape N,4 with the notes that are being played in each time step\n",
    "    for click in range(0,piano_roll.shape[0],q16): # q16 is skip 240 steps for 1/16th note resolution\n",
    "        voice = 3 # start with the low voices and decrement for the higher voices as notes get higher\n",
    "        for i in range(piano_roll.shape[1]): # check if any notes are non-zero\n",
    "            time_interval = (click) // q16 \n",
    "            if (piano_roll[click][i]): # if velocity anything but zero - unless you set encode_velocity = False\n",
    "                # if time_interval % 16 == 0:\n",
    "                #     print(f'time step: {click} at index {i}, time_interval: {time_interval}, voice: {voice}')\n",
    "                # i is the midi note number. I want to transpose it into C\n",
    "                sample[time_interval][voice] = i - root # index to the piano roll with a note - transposed by the key if not C which is 0\n",
    "                voice -= 1 # next instrument will get the higher note\n",
    "    return (sample,root,mode)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time signatures: 4/4\n",
      "music.resolution is q: 1024. q16: 256 time_steps: 320 1/16th notes\n",
      "/home/prent/Downloads/chorales_018007b_(c)greentree.mid, \n",
      "F  major transposed into C and then used to create the segments\n",
      "64  60  55  48  \n",
      "64  60  55  48  \n",
      "64  60  55  48  \n",
      "64  60  55  48  \n",
      "62  59  55  43  \n",
      "sample.shape: (320, 4). dtype(sample): <class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "# load the midi file into a numpy array, \n",
    "sample, root, mode = midi_to_input(file_name) # sample is time interval, voice\n",
    "keys = ['C ','C#','D ','D#','E ','F ','F#','G ','G#','A ','A#','B ']\n",
    "print(f'{file_name}, \\n{keys[root]} {mode} transposed into C and then used to create the segments')\n",
    "i = 0\n",
    "for t in sample: # for each time interval\n",
    "    i += 1\n",
    "    for v in t: # for each voice\n",
    "        print(v,' ' , end='')\n",
    "    print('')\n",
    "    if i > 4: break\n",
    "\n",
    "print(f'sample.shape: {sample.shape}. dtype(sample): {type(sample[0,0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the sample into segments based on phrase length\n",
    "In this case, the 1st four segments are 2 1/2 measures long. That Bach guy was full of surprises. The next two are repeats and can be discarded for now. The 4th and 5th are 2 measures long, which is what the model expects. The final one is the closing chord. At the end of this cell, you have a variable called \"segment\" which contains an array of 0 through 6 segments of the piece, each with 40 time slots for each of 4 voices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seg_num\tlength\tstart\tend\n",
      "0\t40\t0\t39\n",
      "1\t40\t40\t79\n",
      "2\t40\t160\t199\n",
      "3\t40\t200\t239\n",
      "4\t32\t240\t271\n",
      "5\t32\t272\t303\n",
      "6\t8\t304\t311\n"
     ]
    }
   ],
   "source": [
    "# sample is a piano roll of pitches in 1/16th note intervals of dimension (320 time intervals, 4 voices, 1 pitch per time interval and voice)\n",
    "\n",
    "seg_num = 0 # index into the segment array\n",
    "segment = np.zeros((7,4,40),dtype=int)  # seg_num, voices, 1/16th note values\n",
    "print(f'seg_num\\tlength\\tstart\\tend')\n",
    "pad8 = np.zeros((8,4)) # 8 zeros in each of four voices for segments 4 & 5\n",
    "\n",
    "phrase_len = int(4 * 4 * 2.5) # the first segmenst have phrases of 2 1/2 measures in length 4*4*2.5 = 40 12/16th notes\n",
    "for i in range(6): # sample 0 though 5, seg_num 0,1,2,3\n",
    "    start = i * phrase_len \n",
    "    end = (i + 1) * phrase_len\n",
    "    if i in (2,3): # note that the first two segments are repeated, so we can discard segments 2 & 3    \n",
    "        pass\n",
    "        # print(f'Ignore segments 2 & 3 they are repeats. seg_num: {seg_num}')\n",
    "    else:\n",
    "        print(f'{seg_num}\\t{phrase_len}\\t{start}\\t{end-1}')\n",
    "        transfer = sample[start:end]\n",
    "        segment[seg_num] = transfer.transpose()\n",
    "        seg_num += 1\n",
    "    \n",
    "phrase_len = int(4 * 4 * 2) # 32 1/16th notes   \n",
    "for i in range(6, 8): # seg_num: 4 & 5\n",
    "    start = end \n",
    "    end = (start + phrase_len)\n",
    "    print(f'{seg_num}\\t{phrase_len}\\t{start}\\t{end-1}')\n",
    "    transfer = np.concatenate((sample[start:end],pad8),axis=0) # load the segment with the first 8 1/16th notes from the next segment. We will ignore these later.\n",
    "    segment[seg_num] = transfer.transpose()\n",
    "    seg_num += 1\n",
    "\n",
    "phrase_len = int(4 * 2) # 8 1/16th notes in a whole note\n",
    "for i in range(8,9): # seg_num 6\n",
    "    start = end \n",
    "    end = (start + phrase_len)\n",
    "    print(f'{seg_num}\\t{phrase_len}\\t{start}\\t{end-1}')\n",
    "    transfer = sample[start:end], # load the segment with the first 8 1/16th notes from the next segment. We will ignore these later.\n",
    "    transfer = np.concatenate(transfer*5) # put 5 copies of the 8 1/16th notes one after the other fill out to 40 slots. Ignore the later slots.\n",
    "    segment[seg_num] = transfer.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress the 40 slot segments down to 32 slots\n",
    "This is done to match the model requirements. We create a helper function that compresses the last 16 slots down to 8 by skipping every other note in the 16. Not as crude at the clipping that was done in the mode, but it looses some information that cannot be retrieved upon decompressions. At the end of this process, we have a 7,4,32 array with 7 segments that are all 32 1/16th notes in length in a variable called \"sub_segment\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will take a 4,40 array and return a 4,32 array. It compresses the last 16 slots into 8 slots by skipping every other slot in the array.\n",
    "def compress_end_of_segment(input_array):\n",
    "    # let numpy do the slicing. It's better than a python list\n",
    "    # np_input = np.array(input_array) # don't need it because it's already a np.array\n",
    "    # this function will compress the last 16 1/16th notes into the space of 8 16/th notes\n",
    "    # it looks at the four voices, one at a time and moves the \n",
    "    for v in range(4):\n",
    "        n = 24 # start at this slot for each voice\n",
    "        for i in range(n,40,2): # start at 24, increment until just before 40 by 2 each time\n",
    "            input_array[v][n] = input_array[v][i]\n",
    "            n += 1\n",
    "    return(input_array[:,:-8]) # return all four voices all notes in each voice. Return only the first 32 slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 4, 40)\n"
     ]
    }
   ],
   "source": [
    "# compress segments 0,1,2,3 from 40 slots to 32 slots for all four voices\n",
    "# It leaves segments 4 & 5 alone, and expands the held note on segment 6 to 32 time slices.\n",
    "# print(segment)\n",
    "print(segment.shape)\n",
    "pad8 = np.reshape(pad8,(4,8))\n",
    "for seg_num in range(4): # we need to take the 40 slot arrays and reduce them to 32 slots.\n",
    "    # print(f'seg_num: {seg_num} before compression') \n",
    "    # print(f'segment[{seg_num}]: {segment[seg_num][0]}')\n",
    "    my_segment = compress_end_of_segment(segment[seg_num])\n",
    "    # print('after compression')\n",
    "    # print(f'my_segment: {my_segment[0]}')\n",
    "    segment[seg_num] = np.concatenate((my_segment,pad8),axis=1)\n",
    "sub_segment = segment[:,:,:32] # chop off the 33-40'th 1/16th note in the piano roll leaving 32 slots    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 4, 32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_segment.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harmonize to the bass part\n",
    "This cell uses the random harmonization function: <code>generate_random_chorale(model)</code>\n",
    "- It ignores the original midi file, taking no input at all.\n",
    "- passes that through the generate_random_chorale function seeding with a random set of digits, andletting the model figure out new soprano, alto, tenor, and bass lines.\n",
    "- decompresses the prediction output by taking the last 8 slots and turning them into 16 slots for a total of 40 in the segment\n",
    "- skip the transposisition back to the original key, since the sources are random\n",
    "- concatenates all the segments into one long array. Make sure to only include 32 slots for segments 4 and 5. \n",
    "<code>trans_segment[4,:,:32],trans_segment[5,:,:32]</code>\n",
    "- passes that to generate a midi file\n",
    "- plays the midi file\n",
    "\n",
    "This cell takes about 10 minutes to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60\n",
      "  60 60 60 60 60 60 60 60]\n",
      " [55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55\n",
      "  55 55 55 55 55 55 55 55]\n",
      " [52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52\n",
      "  52 52 52 52 52 52 52 52]\n",
      " [36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n",
      "  36 36 36 36 36 36 36 36]]\n"
     ]
    }
   ],
   "source": [
    "print(sub_segment[6]) # nice C major chord to end the chorale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate a random source chorale six new segments. Keep segment[6] which is a C major chord.\n",
    "# for i in range(6): # synthesize segmenst 0 through 5 of a chorale.\n",
    "#     # sub_segment[i] = generate_random_chorale(model)  \n",
    "#     sub_segment[i] = sub_segment[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st eight notes in 0th segment[0,:8]: [64 64 64 64 62 62 62 62]\n",
      "1st eight notes in 1th segment[0,:8]: [67 67 67 67 64 64 64 64]\n",
      "1st eight notes in 2th segment[0,:8]: [67 67 67 67 69 69 71 71]\n",
      "1st eight notes in 3th segment[0,:8]: [67 67 67 67 69 69 71 71]\n",
      "1st eight notes in 4th segment[0,:8]: [ 0  0  0  0 64 64 64 64]\n",
      "1st eight notes in 5th segment[0,:8]: [67 67 67 67 64 64 64 64]\n",
      "1st eight notes in 6th segment[0,:8]: [60 60 60 60 60 60 60 60]\n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "    print(f'1st eight notes in {i}th segment[0,:8]: {sub_segment[i,0,:8]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_em(segment,keep):\n",
    "    trans_segment = np.zeros((4,4,40),dtype=int) # save all four predictions after being de-compressed and transposed, then combined into fourple_segment\n",
    "    fourple_segment = np.zeros((7,16,40),dtype=int) # combine four trans_segments into one - don't need to predefine these\n",
    "    pad8 = np.zeros((4,8))  # pad the end of the segment with zeros\n",
    "    for examples in range(1):\n",
    "        s = 0\n",
    "        for seg in sub_segment: # for each segment of the chorale\n",
    "            # print(f'\\nbeginning prediction for segment {s}')\n",
    "            for pred in range(4): # make four predictions of each sub_segment and save all of them into expanded_segment\n",
    "                # the next line takes about 31 seconds of wall clock time per prediction\n",
    "                keep = np.random.choice([3])\n",
    "                prediction = Chorale(seg - 30).elaborate_on_voices([keep], model) + 30 \n",
    "                # print(f'prediction {pred} complete. last 8 notes of the soprano part {pred}: {prediction[0,24:]}')\n",
    "                # print(f'prediction.shape: {prediction.shape}')\n",
    "                if s in (0,1,2,3): # need to expand these\n",
    "                    trans_segment[pred] = decompress_end_of_segment(prediction) #  just decompress one segment\n",
    "                else: # don't need to expand 4,5,6 sub_segments\n",
    "                    trans_segment[pred] = np.concatenate((prediction,pad8),axis=1)\n",
    "            fourple_segment[s] = np.reshape(trans_segment,(16,40))\n",
    "            # print(f'fourple_segment[{s}].shape: {fourple_segment[s].shape}')\n",
    "            s += 1\n",
    "            # if s > 1: break\n",
    "        # print(f'shape of fourple_segment: {fourple_segment.shape}')\n",
    "        # you have to concatenate the fourple_segments, not reshape, because some are 40 and some are 32 slots long.\n",
    "        concat_chorale = np.concatenate((fourple_segment[0],\n",
    "                        fourple_segment[1],\n",
    "                        fourple_segment[2],\n",
    "                        fourple_segment[3],\n",
    "                        fourple_segment[4,:,:32],\n",
    "                        fourple_segment[5,:,:32],\n",
    "                        fourple_segment[6]),axis=1)\n",
    "        return(concat_chorale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 90\n",
      "saved_chorale390.npy\n",
      "3 91\n",
      "saved_chorale391.npy\n",
      "3 92\n",
      "saved_chorale392.npy\n",
      "3 93\n",
      "saved_chorale393.npy\n",
      "3 94\n",
      "saved_chorale394.npy\n",
      "3 95\n",
      "saved_chorale395.npy\n",
      "3 96\n",
      "saved_chorale396.npy\n",
      "3 97\n",
      "saved_chorale397.npy\n",
      "3 98\n",
      "saved_chorale398.npy\n",
      "3 99\n",
      "saved_chorale399.npy\n",
      "CPU times: user 7h 19min 25s, sys: 1min 12s, total: 7h 20min 37s\n",
      "Wall time: 1h 51min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "keep = 3\n",
    "for i in range(90,100):\n",
    "    print(keep,i)\n",
    "    concat_chorale = stack_em(segment,keep)\n",
    "    concat_chorale = transpose_up_segment(concat_chorale,root)\n",
    "    outfile = 'saved_chorale' + str(keep) + str(i) + '.npy' #<-- if you don't end it with .npy then it appends .npy to the name automatically\n",
    "    np.save(outfile, concat_chorale)\n",
    "    print (outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play a piano roll of any dimension\n",
    "This function takes in an array of (voices,time_steps) of indeterminant shape. It sends notes to csound for playing to an audio wave file.\n",
    "<p>There are several adjustments that can be made. Velocity is the proxy for how hard the keys are pressed. If it's less than 62, then the csound instance chooses extremely soft samples, as if the player is barely touching the keys. Here is a complete breakdown of velocity and sample sets:</p>\n",
    "    \n",
    "| velocity | sample set | max volume |\n",
    "| :-: | :-: | :-: |\n",
    "| 60 | 25 | 24 | \n",
    "| 62 | 31 | 16 |    \n",
    "| 64 | 39 | 15 |\n",
    "| 66 | 47 | 13 |    \n",
    "| 68 | 63 | 10 |\n",
    "| 70 | 78 | 10 |\n",
    "| 72 | 85 | 10 |\n",
    " \n",
    "<p>Notice that the range of the velocity is very small. Anything 60 or less uses the 25 sample set, which is very soft and quiet, while anything 72 or more uses the 85 sample set. There are three additional sample sets at 99, 113, and 127, which I left out of the csound csd file to reduce storage. I think it might be better to eliminate some of the higher notes, every other sample, and include the louder ones, but that's a job for another day.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ogxsHxCgak0U"
   },
   "outputs": [],
   "source": [
    "def piano_roll_to_csound(piece,velocity,volume):\n",
    "    if os.path.exists('goldberg5.log'):\n",
    "        os.remove(\"goldberg5.log\") # make sure the log starts over with a fresh log file. Next line starts the logger.\n",
    "    pt,cs = load_csound_piano() # load the csd file and return a performance thread and a Csound instance, start the logger.\n",
    "    piano.logging.info('ins star dur vel ton oc voi stero env glis upsa rEnv 2nd 3rd vol chan')\n",
    "    tpq = 2.0 # time per quarter note\n",
    "    tp16th = tpq / 4 # time per 1/16th note\n",
    "    hold = 0.2 # how long to hold to make more of a legato\n",
    "    pfields = []\n",
    "       \n",
    "    v = 0\n",
    "    for voice in piece: # once for each voice\n",
    "        prev_note = 0\n",
    "        duration = 0\n",
    "        start_time = 0\n",
    "        first = True\n",
    "        # print(f'\\tvoice\\tstart\\tnote\\tduration')\n",
    "        for note in voice: # one note for each time step in this voice [69 69 69 69 67 67 67 67 65 65 65 65 67 67 67 67]\n",
    "            if first:\n",
    "                prev_note = note\n",
    "                first = False\n",
    "            if note == prev_note:\n",
    "                duration += tp16th # add another 1/16th note duration to the current duration\n",
    "            else: # send the note to csound\n",
    "                # print(f'\\t{v} \\t{start_time}  \\t{prev_note} \\t{duration}')\n",
    "                octave = prev_note // 12\n",
    "                tone = prev_note - 12 * octave\n",
    "                octave -= 1\n",
    "                #        inst start       duration  velocity  tone  octave voice ster env glis upsa rEnv 2nd 3rd gl mult chan\n",
    "                random_velocity = velocity + np.random.randint(-3,2) # chose different sample sets based on greater or lesser velocity\n",
    "                random_start = start_time + round(rng.standard_normal()/75,5)\n",
    "                if random_start < 0: random_start = 0\n",
    "                # make the modifications to the bass lines to be an octave lower, and the soprano an octave higher.\n",
    "                if v == 3: # bass line \n",
    "                   octave += np.random.randint(-1,1)\n",
    "                elif v == 0: # soprano\n",
    "                    octave += np.random.randint(0,2)\n",
    "                pfields.append([1, random_start, duration + hold, random_velocity, tone, octave, 1.0, 8.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, volume, 1])\n",
    "                start_time += duration\n",
    "                duration = tp16th\n",
    "                prev_note = note \n",
    "        v += 1\n",
    "    \n",
    "    pfields.sort()\n",
    "    print(f'list of notes is {len(pfields)} long')\n",
    "    for i in range(len(pfields)):\n",
    "        # print(f'index: {i}\\t{pfields[i]}')\n",
    "        pt.scoreEvent(0, 'i', pfields[i]) \n",
    "        piano.logging.info(pfields[i]) \n",
    "\n",
    "    piano.printMessages(cs)\n",
    "    delay_time =  max(10,len(pfields) // 20) # need enough time to prevent csound being told to stop processing prematurely\n",
    "    print(f'about to delay to allow ctcsound to process the notes. delay_time: {delay_time}')\n",
    "    print(f'last start time was at {start_time}. Set f0 to {start_time+1}')\n",
    "    time.sleep(delay_time) # once you hit the next line csound stops\n",
    "    pt.stop() # this is important I think. It closes the output file.\n",
    "    pt.join()   \n",
    "    piano.printMessages(cs)    \n",
    "    cs.reset()\n",
    "    subprocess.run(['grep', 'invalid\\|range\\|error\\|replacing\\|overall', 'goldberg5.log']) # look in the log for important messages\n",
    "    audio = Audio('/home/prent/Music/sflib/goldberg_aria1.wav')\n",
    "    display(audio)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ogxsHxCgak0U"
   },
   "outputs": [],
   "source": [
    "play_chorale = np.load('segment600.npy')\n",
    "print(f'shape sent to piano_roll_to_csound: {play_chorale.shape}')        \n",
    "piano_roll_to_csound(play_chorale,69,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolve with the impulse response from Teatro Alcorcon in Madrid from Angelo Farina Collection\n",
    "<p>The next few cells require a great deal of installation work to accomplish. You need to install the following:\n",
    "    \n",
    "- Csound - available in most Linux repos with the operating system's standard program installer\n",
    "- sox \n",
    "- ffmpeg\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!csound goldberg_aria1c.csd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lth /home/prent/Music/sflib/goldberg_aria1a-c.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sox /home/prent/Music/sflib/goldberg_aria1a-c.wav save1.wav reverse\n",
    "!sox save1.wav save2.wav silence 1 0.01 0.01\n",
    "!sox save2.wav save1.wav reverse\n",
    "!sox save1.wav /home/prent/Music/sflib/goldberg_aria1-t4.wav silence 1 0.01 0.01\n",
    "!rm save1.wav\n",
    "!rm save2.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lth /home/prent/Music/sflib/goldberg_aria1-t4.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -y -i /home/prent/Music/sflib/goldberg_aria1-t4.wav -b:a 320k /home/prent/Music/sflib/goldberg_aria1-t4.mp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = Audio('/home/prent/Music/sflib/goldberg_aria1-t4.mp3')\n",
    "display(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = 'chorale.npy' #<-- if you don't end it with .npy then it appends .npy to the name automatically\n",
    "np.save(outfile, concat_chorale)\n",
    "saved_chorale = np.load(outfile)\n",
    "print(saved_chorale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "coconet_reorganized.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
