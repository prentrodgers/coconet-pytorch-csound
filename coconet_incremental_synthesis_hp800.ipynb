{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b117a290-8a63-499a-beda-d0efaee10852",
   "metadata": {},
   "source": [
    "## Incrementally add new voices to a chorale.\n",
    "<p>The goals of this round:\n",
    "    \n",
    "- Take a chorale, keep three voices, synthesize the fourth. save the new one in an array.\n",
    "- Repeat: keep three voices, including the new one, and discard one of the original ones, synthesize the missing one, save it.\n",
    "- Continue dropping voices, create a new one, and save it. Do this for a while, always discarding the oldest one until you have a multi-voice chorale that sounds interesting and without too many wrong notes. \n",
    "    \n",
    " </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "953c8070-c8af-45c7-8422-383ccbb0d205",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gTZ5xE7jaVy0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mido\n",
    "import time\n",
    "from midi2audio import FluidSynth\n",
    "from IPython.display import Audio, display\n",
    "import os\n",
    "import muspy\n",
    "import piano \n",
    "import selective_stretching_codes\n",
    "import samples_used\n",
    "import subprocess\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng(42) # random seed in parens.\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "softmax = torch.nn.functional.softmax\n",
    "\n",
    "base_dir = ''\n",
    "CSD_FILE = 'goldberg_aria1.csd'\n",
    "NOTES_FILE = \"goldberg_aria1.mac.csv\"\n",
    "LOGNAME = 'goldberg5.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e363a75-2514-4917-b8d7-b200d91ed27f",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sGQCWjlCbGAu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I voices: 4, T sample length: 32, P number of distinct pitches in the input chorales: 57\n"
     ]
    }
   ],
   "source": [
    "# set global variables\n",
    "\n",
    "I = 4 # number of voices\n",
    "T = 32 # length of samples (32 = two 4/4 measures in 1/16th note increments)\n",
    "P = (86-30) +1 # number of different pitches\n",
    "print(f'I voices: {I}, T sample length: {T}, P number of distinct pitches in the input chorales: {P}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ab18dd7-fbcd-4ca6-8409-3abbba1602c2",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ogxsHxCgak0U"
   },
   "outputs": [],
   "source": [
    "class Chorale:\n",
    "    \"\"\"\n",
    "    A class to store and manipulate an array self.arr that stores a chorale.\n",
    "    \"\"\"\n",
    "    def __init__(self, arr, subtract_30=False):\n",
    "        # arr is an array of shape (4, 32) with values in range(0, 57)\n",
    "        self.arr = arr.copy()\n",
    "        if subtract_30:\n",
    "            self.arr -= 30\n",
    "            \n",
    "        # the one_hot representation of the array\n",
    "        reshaped = self.arr.reshape(-1)\n",
    "        self.one_hot = np.zeros((I*T, P))\n",
    "        r = np.arange(I*T)\n",
    "        self.one_hot[r, reshaped] = 1\n",
    "        self.one_hot = self.one_hot.reshape(I, T, P)\n",
    "        \n",
    "\n",
    "    def to_image(self):\n",
    "        # visualize the four tracks as a images\n",
    "        soprano = self.one_hot[0].transpose()\n",
    "        alto = self.one_hot[1].transpose()\n",
    "        tenor = self.one_hot[2].transpose()\n",
    "        bass = self.one_hot[3].transpose()\n",
    "        \n",
    "        fig, axs = plt.subplots(1, 4)\n",
    "        axs[0].imshow(np.flip(soprano, axis=0), cmap='hot', interpolation='nearest')\n",
    "        axs[0].set_title('soprano')\n",
    "        axs[1].imshow(np.flip(alto, axis=0), cmap='hot', interpolation='nearest')\n",
    "        axs[1].set_title('alto')\n",
    "        axs[2].imshow(np.flip(tenor, axis=0), cmap='hot', interpolation='nearest')\n",
    "        axs[2].set_title('tenor')\n",
    "        axs[3].imshow(np.flip(bass, axis=0), cmap='hot', interpolation='nearest')\n",
    "        axs[3].set_title('bass')\n",
    "        fig.set_figheight(5)\n",
    "        fig.set_figwidth(15)\n",
    "        return fig, axs\n",
    "    \n",
    "    def play(self, filename='midi_track.mid'):\n",
    "        # display an in-notebook widget for playing audio\n",
    "        # saves the midi file as a file named name in base_dir/midi_files\n",
    "        \n",
    "        midi_arr = self.arr.transpose().copy()\n",
    "        midi_arr += 30\n",
    "        midi = selective_stretching_codes.piano_roll_to_midi(midi_arr)\n",
    "        midi.save(base_dir + 'midi_files/' + filename)\n",
    "        play_midi('midi_files/' + filename,10)\n",
    "        \n",
    "    def elaborate_on_voices(self, voices, model):\n",
    "        # voice is a set consisting of 0, 1, 2, or 3\n",
    "        # create a mask consisting of the given voices\n",
    "        # generate a chorale with the same voices as in voices\n",
    "        mask = np.zeros((I, T))\n",
    "        y = np.random.randint(P, size=(I, T))\n",
    "        for i in voices:\n",
    "            mask[i] = 1\n",
    "            y[i] = self.arr[i].copy()\n",
    "        return harmonize(y, mask, model)\n",
    "    \n",
    "    # I think we could improve this scoring method. It's pretty lame.\n",
    "    def score(self):\n",
    "        consonance_dict = {0: 1, 1: 0, 2: 0, 3: 1, 4: 1, 5: 1, 6: 0, \n",
    "                           7: 1, 8: 1, 9: 1, 10: 0, 11: 0}\n",
    "        consonance_score = 0\n",
    "        for k in range(32):\n",
    "            for i in range(4):\n",
    "                for j in range(i):\n",
    "                    consonance_score += consonance_dict[((self.arr[i, k] - self.arr[j, k]) % 12)]\n",
    "        \n",
    "        note_score = 0\n",
    "        for i in range(4):\n",
    "            for j in range(1, 32):\n",
    "                if self.arr[i, j] != self.arr[i, j-1]:\n",
    "                    note_score += 1\n",
    "        return consonance_score, note_score\n",
    "        \n",
    "# harmonize a melody\n",
    "def harmonize(y, C, model):\n",
    "    \"\"\"\n",
    "    Generate an artificial Bach Chorale starting with y, and keeping the pitches\n",
    "    where C==1.\n",
    "    Here C is an array of shape (4, 32) whose entries are 0 and 1.\n",
    "    The pitches outside of C are repeatedly resampled to generate new values.\n",
    "    For example, to harmonize the soprano line, let y be random except y[0] \n",
    "    contains the soprano line, let C[1:] be 0 and C[0] be 1.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = y\n",
    "        C2 = C.copy()\n",
    "        num_steps = int(2*I*T)\n",
    "        alpha_max = .999\n",
    "        alpha_min = .001\n",
    "        eta = 3/4\n",
    "        for i in range(num_steps):\n",
    "            p = np.maximum(alpha_min, alpha_max - i*(alpha_max-alpha_min)/(eta*num_steps))\n",
    "            sampled_binaries = np.random.choice(2, size = C.shape, p=[p, 1-p])\n",
    "            C2 += sampled_binaries\n",
    "            C2[C==1] = 1\n",
    "            x_cache = x\n",
    "            x = model.pred(x, C2)\n",
    "            x[C2==1] = x_cache[C2==1]\n",
    "            C2 = C.copy()\n",
    "        return x\n",
    "    \n",
    "def generate_random_chorale(model): # \n",
    "    \"\"\"\n",
    "    Calls harmonize with random initialization and C=0, masking none \n",
    "    and so generates a new sample that sounds like Bach.\n",
    "    \"\"\"\n",
    "    y = np.random.randint(P, size=(I, T)).astype(int)\n",
    "    C = np.zeros((I, T)).astype(int)\n",
    "    x = harmonize(y, C, model)\n",
    "    return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dccb0b6-7482-4baf-8bb6-afd681a5425a",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "24hPqElFbKhk"
   },
   "outputs": [],
   "source": [
    "hidden_size = 32\n",
    "\n",
    "class Unit(nn.Module):\n",
    "    \"\"\"\n",
    "    Two convolution layers each followed by batchnorm and relu, \n",
    "    plus a residual connection.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Unit, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(hidden_size, hidden_size, 3, padding=1)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(hidden_size, hidden_size, 3, padding=1)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        y = self.conv1(y)\n",
    "        y = self.batchnorm1(y)\n",
    "        y = self.relu1(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.batchnorm2(y)\n",
    "        y = y + x\n",
    "        y = self.relu2(y)\n",
    "        return y\n",
    "    \n",
    "    \n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    A CNN that where you input a starter chorale and a mask and it outputs a prediction for the values\n",
    "    in the starter chorale away from the mask that are most like the training data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.initial_conv = nn.Conv2d(2*I, hidden_size, 3, padding=1)\n",
    "        self.initial_batchnorm = nn.BatchNorm2d(hidden_size)\n",
    "        self.initial_relu = nn.ReLU()\n",
    "        self.unit1 = Unit()\n",
    "        self.unit2 = Unit()\n",
    "        self.unit3 = Unit()\n",
    "        self.unit4 = Unit()\n",
    "        self.unit5 = Unit()\n",
    "        self.unit6 = Unit()\n",
    "        self.unit7 = Unit()\n",
    "        self.unit8 = Unit()\n",
    "        self.unit9 = Unit()\n",
    "        self.unit10 = Unit()\n",
    "        self.unit11 = Unit()\n",
    "        self.unit12 = Unit()\n",
    "        self.unit13 = Unit()\n",
    "        self.unit14 = Unit()\n",
    "        self.unit15 = Unit()\n",
    "        self.unit16 = Unit()\n",
    "        self.affine = nn.Linear(hidden_size*T*P, I*T*P)\n",
    "        \n",
    "    def forward(self, x, C):\n",
    "        # x is a tensor of shape (N, I, T, P)\n",
    "        # C is a tensor of 0s and 1s of shape (N, I, T)\n",
    "        # returns a tensor of shape (N, I, T, P)\n",
    "        \n",
    "        # get the number of batches\n",
    "        N = x.shape[0]\n",
    "        \n",
    "        # tile the array C out of a tensor of shape (N, I, T, P)\n",
    "        tiled_C = C.view(N, I, T, 1)\n",
    "        tiled_C = tiled_C.repeat(1, 1, 1, P)\n",
    "        \n",
    "        # mask x and combine it with the mask to produce a tensor of shape (N, 2*I, T, P)\n",
    "        y = torch.cat((tiled_C*x, tiled_C), dim=1)\n",
    "        \n",
    "        # apply the convolution and relu layers\n",
    "        y = self.initial_conv(y)\n",
    "        y = self.initial_batchnorm(y)\n",
    "        y = self.initial_relu(y)\n",
    "        y = self.unit1(y)\n",
    "        y = self.unit2(y)\n",
    "        y = self.unit3(y)\n",
    "        y = self.unit4(y)\n",
    "        y = self.unit5(y)\n",
    "        y = self.unit6(y)\n",
    "        y = self.unit7(y)\n",
    "        y = self.unit8(y)\n",
    "        y = self.unit9(y)\n",
    "        y = self.unit10(y)\n",
    "        y = self.unit11(y)\n",
    "        y = self.unit12(y)\n",
    "        y = self.unit13(y)\n",
    "        y = self.unit14(y)\n",
    "        y = self.unit15(y)\n",
    "        y = self.unit16(y)\n",
    "            \n",
    "        # reshape before applying the fully connected layer\n",
    "        y = y.view(N, hidden_size*T*P)\n",
    "        y = self.affine(y)\n",
    "        \n",
    "        # reshape to (N, I, T, P)\n",
    "        y = y.view(N, I, T, P)\n",
    "                \n",
    "        return y\n",
    "    \n",
    "    def pred(self, y, C):\n",
    "        # y is an array of shape (I, T) with integer entries in [0, P)\n",
    "        # C is an array of shape (I, T) consisting of 0s and 1s\n",
    "        # the entries of y away from the support of C should be considered 'unknown'\n",
    "        \n",
    "        # x is shape (I, T, P) one-hot representation of y\n",
    "        compressed = y.reshape(-1)\n",
    "        x = np.zeros((I*T, P))\n",
    "        r = np.arange(I*T)\n",
    "        x[r, compressed] = 1\n",
    "        x = x.reshape(I, T, P)\n",
    "        \n",
    "        # prep x and C for the plugging into the model\n",
    "        x = torch.tensor(x).type(torch.FloatTensor).to(device)\n",
    "        x = x.view(1, I, T, P)\n",
    "        C2 = torch.tensor(C).type(torch.FloatTensor).view(1, I, T).to(device)\n",
    "        \n",
    "        # plug x and C2 into the model\n",
    "        with torch.no_grad():\n",
    "            out = self.forward(x, C2).view(I, T, P).cpu().numpy()\n",
    "            out = out.transpose(2, 0, 1) # shape (P, I, T)\n",
    "            probs = np.exp(out) / np.exp(out).sum(axis=0) # shape (P, I, T)\n",
    "            cum_probs = np.cumsum(probs, axis=0) # shape (P, I, T)\n",
    "            u = np.random.rand(I, T) # shape (I, T)\n",
    "            return np.argmax(cum_probs > u, axis=0)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e59f1340-bcc3-4d3a-9784-7ce430a5b072",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xm0YK6yGbZg1"
   },
   "outputs": [],
   "source": [
    "model = Net().to(device) # need this in order to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed5c4611-2ba4-49b1-b96b-14edcab208d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bucYvJ5u7fyl",
    "outputId": "3544bbfd-62af-42b2-cf60-5a0750028bd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncomment to load the previously trained model\n",
    "model.load_state_dict(torch.load('../model1.pt')) # you will need to download this from the authors web site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c631ae54-6c52-43d4-8342-e4905e9e23b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_number(n):\n",
    "    \"\"\"\n",
    "    prepare numbers for better file storage\n",
    "    \"\"\"\n",
    "    if n == 0:\n",
    "        return '00000'\n",
    "    else:\n",
    "        digits = int(np.ceil(np.log10(n)))\n",
    "        pad_zeros = 5 - digits\n",
    "        return '0'* pad_zeros + str(n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0624aea1-e113-4bd0-9f44-562b2d3948fc",
   "metadata": {},
   "source": [
    "## Load a midi file into a numpy array\n",
    "Set certain values:\n",
    "\n",
    "- the numpy array of the whole piece is stored in variable \"sample'\n",
    "- store the root key and mode (F major, for example)\n",
    "- print the values of the time signature (must be 4/4 of you will need to do some extra work), quarter note clicks, clicks per 1/16th notes\n",
    "- any transpositions that must be performed to restore the original key\n",
    "- print the first 5 notes in each voice\n",
    "- print the shape of the variable \"sample\" containing the whole midi file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b76dd6dc-74be-4bb9-aab3-c7cb8f223eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in a midi file, check the key, load into piano roll, set up np.array containing Nx4 sample.\n",
    "# calling program should slice the returned array as needed to create two measure segments for sending into the prediction model.\n",
    "\n",
    "def midi_to_input(midi_file):\n",
    "    music = muspy.read(midi_file)\n",
    "    if music.key_signatures != []: # check if the midi file includes a key signature - some don't\n",
    "        root = music.key_signatures[0].root \n",
    "        mode = music.key_signatures[0].mode # major or minor\n",
    "    else: \n",
    "        print('Warning: no key signature found. Assuming C major')\n",
    "        mode = \"major\"\n",
    "        root = 0    \n",
    "    if music.time_signatures != []: # check if the midi file includes a time signature - some don't\n",
    "        numerator = music.time_signatures[0].numerator\n",
    "        denominator = music.time_signatures[0].denominator \n",
    "    else: \n",
    "        print('Warning: no time signature found. Assuming 4/4')\n",
    "        numerator = 4\n",
    "        denominator = 4\n",
    "    # turn it into a piano roll\n",
    "    piano_roll = muspy.to_pianoroll_representation(music,encode_velocity=False) # boolean piano roll if False, default True\n",
    "    # print(piano_roll.shape) # should be one time step for every click in the midi file\n",
    "    q = music.resolution # quarter note value in this midi file. \n",
    "    q16 = q // 4 # my desired resolution is by 1/16th notes\n",
    "    print(f'time signatures: {numerator}/{denominator}')\n",
    "    time_steps = piano_roll.shape[0] // q16\n",
    "    print(f'music.resolution is q: {q}. q16: {q16} time_steps: {time_steps} 1/16th notes')\n",
    "    sample= np.zeros(shape=(time_steps,4)).astype(int) # default is float unless .astype(int)\n",
    "    # This loop is able to load an array of shape N,4 with the notes that are being played in each time step\n",
    "    for click in range(0,piano_roll.shape[0],q16): # q16 is skip 240 steps for 1/16th note resolution\n",
    "        voice = 3 # start with the low voices and decrement for the higher voices as notes get higher\n",
    "        for i in range(piano_roll.shape[1]): # check if any notes are non-zero\n",
    "            time_interval = (click) // q16 \n",
    "            if (piano_roll[click][i]): # if velocity anything but zero - unless you set encode_velocity = False\n",
    "                # if time_interval % 16 == 0:\n",
    "                #     print(f'time step: {click} at index {i}, time_interval: {time_interval}, voice: {voice}')\n",
    "                # i is the midi note number. I want to transpose it into C\n",
    "                sample[time_interval][voice] = i - root # index to the piano roll with a note - transposed by the key if not C which is 0\n",
    "                voice -= 1 # next instrument will get the higher note\n",
    "    return (sample,root,mode)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7966d06-32cc-428d-8009-b942bf83ea90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time signatures: 4/4\n",
      "music.resolution is q: 1024. q16: 256 time_steps: 320 1/16th notes\n",
      "schmucke.mid, \n",
      "F  major transposed into C and then used to create the segments\n",
      "64  60  55  48  \n",
      "64  60  55  48  \n",
      "64  60  55  48  \n",
      "64  60  55  48  \n",
      "62  59  55  43  \n",
      "sample.shape: (320, 4). dtype(sample): <class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "# load the BWV 180 Schmucke dich, o liebe Seele Chorale - nice variety of phrase lengths.\n",
    "# load a midi file into a list called sample - load the entire file, all tracks, all notes in all tracks\n",
    "# if the midi file has a key signature, it will print what it is. \n",
    "# the notes will be transposed by the loader to the key of C, by subtracting the root from each note. F = 5\n",
    "file_name = 'schmucke.mid'\n",
    "# load the midi file into an instance of the music class from muspy.\n",
    "sample, root, mode = midi_to_input(file_name) # sample is time interval, voice\n",
    "keys = ['C ','C#','D ','D#','E ','F ','F#','G ','G#','A ','A#','B ']\n",
    "print(f'{file_name}, \\n{keys[root]} {mode} transposed into C and then used to create the segments')\n",
    "i = 0\n",
    "for t in sample: # for each time interval\n",
    "    i += 1\n",
    "    for v in t: # for each voice\n",
    "        print(v,' ' , end='')\n",
    "    print('')\n",
    "    if i > 4: break\n",
    "\n",
    "print(f'sample.shape: {sample.shape}. dtype(sample): {type(sample[0,0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f441d7-9313-4547-aed8-1dfcd14f9687",
   "metadata": {},
   "source": [
    "## Divide the sample into segments based on phrase length\n",
    "In this case, the 1st four segments are 2 1/2 measures long. That Bach guy was full of surprises. The next two are repeats and can be discarded for now. The 4th and 5th are 2 measures long, which is what the model expects. The final one is the closing chord. At the end of this cell, you have a variable called \"segment\" which contains an array of 0 through 6 segments of the piece, each with 40 time slots for each of 4 voices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1a2470d-a708-4980-bf61-0498a39a63ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seg_num\tlength\tstart\tend\n",
      "0\t40\t0\t39\n",
      "1\t40\t40\t79\n",
      "2\t40\t160\t199\n",
      "3\t40\t200\t239\n",
      "4\t32\t240\t271\n",
      "5\t32\t272\t303\n",
      "6\t8\t304\t311\n"
     ]
    }
   ],
   "source": [
    "# sample is a piano roll of pitches in 1/16th note intervals of dimension (320 time intervals, 4 voices, 1 pitch per time interval and voice)\n",
    "\n",
    "seg_num = 0 # index into the segment array\n",
    "segment = np.zeros((7,4,40),dtype=int)  # seg_num, voices, 1/16th note values\n",
    "print(f'seg_num\\tlength\\tstart\\tend')\n",
    "pad8 = np.zeros((8,4)) # 8 zeros in each of four voices for segments 4 & 5\n",
    "\n",
    "phrase_len = int(4 * 4 * 2.5) # the first segmenst have phrases of 2 1/2 measures in length 4*4*2.5 = 40 12/16th notes\n",
    "for i in range(6): # sample 0 though 5, seg_num 0,1,2,3\n",
    "    start = i * phrase_len \n",
    "    end = (i + 1) * phrase_len\n",
    "    if i in (2,3): # note that the first two segments are repeated, so we can discard segments 2 & 3    \n",
    "        pass\n",
    "        # print(f'Ignore segments 2 & 3 they are repeats. seg_num: {seg_num}')\n",
    "    else:\n",
    "        print(f'{seg_num}\\t{phrase_len}\\t{start}\\t{end-1}')\n",
    "        transfer = sample[start:end]\n",
    "        segment[seg_num] = transfer.transpose()\n",
    "        seg_num += 1\n",
    "    \n",
    "phrase_len = int(4 * 4 * 2) # 32 1/16th notes   \n",
    "for i in range(6, 8): # seg_num: 4 & 5\n",
    "    start = end \n",
    "    end = (start + phrase_len)\n",
    "    print(f'{seg_num}\\t{phrase_len}\\t{start}\\t{end-1}')\n",
    "    transfer = np.concatenate((sample[start:end],pad8),axis=0) # load the segment with the first 8 1/16th notes from the next segment. We will ignore these later.\n",
    "    segment[seg_num] = transfer.transpose()\n",
    "    seg_num += 1\n",
    "\n",
    "phrase_len = int(4 * 2) # 8 1/16th notes in a whole note\n",
    "for i in range(8,9): # seg_num 6\n",
    "    start = end \n",
    "    end = (start + phrase_len)\n",
    "    print(f'{seg_num}\\t{phrase_len}\\t{start}\\t{end-1}')\n",
    "    transfer = sample[start:end], # load the segment with the first 8 1/16th notes from the next segment. We will ignore these later.\n",
    "    transfer = np.concatenate(transfer*5) # put 5 copies of the 8 1/16th notes one after the other fill out to 40 slots. Ignore the later slots.\n",
    "    segment[seg_num] = transfer.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a5367-bd94-4050-bc1d-bbb5d0e4e69a",
   "metadata": {},
   "source": [
    "## Compress the 40 slot segments down to 32 slots\n",
    "This is done to match the model requirements. We create a helper function that compresses the last 16 slots down to 8 by skipping every other note in the 16. Not as crude at the clipping that was done in the mode, but it looses some information that cannot be retrieved upon decompressions. At the end of this process, we have a 7,4,32 array with 7 segments that are all 32 1/16th notes in length in a variable called \"sub_segment\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "031fc2bf-7017-4bd3-84b5-ad0de8336459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 4, 40)\n",
      "seg_num: 0 before compression\n",
      "segment[0]: [64 64 62 62 60 60 62 62  0 65 67 67 65 65 65 65 64 64 64 64 67 67 67 67\n",
      " 65 65 65 65 65 65 65 65 64 64 64 64 64 64 64 64]\n",
      "after compression\n",
      "my_segment: [64 64 62 62 60 60 62 62  0 65 67 67 65 65 65 65 64 64 64 64 67 67 67 67\n",
      " 65 65 65 65 64 64 64 64]\n",
      "seg_num: 1 before compression\n",
      "segment[1]: [67 67 67 67 64 64 64 64 65 65 65 65 64 64 62 62  0  0 62 62 64 64 64 64\n",
      " 62 62 62 62 62 62 62 62 60 60 60 60 60 60 60 60]\n",
      "after compression\n",
      "my_segment: [67 67 67 67 64 64 64 64 65 65 65 65 64 64 62 62  0  0 62 62 64 64 64 64\n",
      " 62 62 62 62 60 60 60 60]\n",
      "seg_num: 2 before compression\n",
      "segment[2]: [67 67 67 67 69 69 71 71 72 72 72 72 72 72 72 72 71 71 69 69 67 67 69 69\n",
      " 69 69 69 69 69 69 69 69 67 67 67 67 67 67 67 67]\n",
      "after compression\n",
      "my_segment: [67 67 67 67 69 69 71 71 72 72 72 72 72 72 72 72 71 71 69 69 67 67 69 69\n",
      " 69 69 69 69 67 67 67 67]\n",
      "seg_num: 3 before compression\n",
      "segment[3]: [67 67 67 67 69 69 71 71 72 72 72 72 72 72 72 72 71 71 69 69  0  0 69 69\n",
      " 69 69 69 69 69 69 69 69 67 67 67 67 67 67 67 67]\n",
      "after compression\n",
      "my_segment: [67 67 67 67 69 69 71 71 72 72 72 72 72 72 72 72 71 71 69 69  0  0 69 69\n",
      " 69 69 69 69 67 67 67 67]\n"
     ]
    }
   ],
   "source": [
    "# compress segments 0,1,2,3 from 40 slots to 32 slots for all four voices\n",
    "# It leaves segments 4 & 5 alone, and expands the held note on segment 6 to 32 time slices.\n",
    "# print(segment)\n",
    "print(segment.shape)\n",
    "pad8 = np.reshape(pad8,(4,8))\n",
    "for seg_num in range(4): # we need to take the 40 slot arrays and reduce them to 32 slots.\n",
    "    print(f'seg_num: {seg_num} before compression') \n",
    "    print(f'segment[{seg_num}]: {segment[seg_num][0]}')\n",
    "    my_segment = selective_stretching_codes.compress_segment(segment[seg_num],24,40) # compress the last 16 to 8\n",
    "    print('after compression')\n",
    "    print(f'my_segment: {my_segment[0]}')\n",
    "    segment[seg_num] = np.concatenate((my_segment,pad8),axis=1)\n",
    "sub_segment = segment[:,:,:32] # chop off the 33-40'th 1/16th note in the piano roll leaving 32 slots    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9165e0e-9970-48c9-9594-c3f61adffaa8",
   "metadata": {},
   "source": [
    "## What we have at this point\n",
    "\n",
    "- We have an array of seven segments in the variable sub_segment with a shape (7,4,32).\n",
    "- These are compressed versions of the original chorale, with the 40 slot segments compressed down to 32 slots\n",
    "- These segments can each be individually sent to the model for potential replacements, since the model expects 4 voices and 32 time steps.\n",
    "\n",
    "## What I need to do next:\n",
    "\n",
    "- Create a function that takes in a segment and masks one of the four voices and asks the model to synthesize the missing voice and return the replacement voice as an array. \n",
    "- store that array for later use in subsequent synthesis activities.\n",
    "- repeat by masking a different voice, ideally the oldest one, and generating a replacement for that voice.\n",
    "- keep at it as many times as you can.\n",
    "\n",
    "## How will we do this:\n",
    "\n",
    "- use the function: def harmonize(y, C, model):\n",
    "- per the docs:\n",
    "--  Generate an artificial Bach Chorale starting with y, and keeping the pitches where C==1.\n",
    "--  Here C is an array of shape (4, 32) whose entries are 0 and 1.\n",
    "--  The pitches outside of C are repeatedly resampled to generate new values.\n",
    "--  For example, to harmonize the soprano line, let y be random except y[0] contains the soprano line, let C[1:] be 0 and C[0] be 1.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48fb7573-563a-445a-89f6-28dd95bb52d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 4, 32)\n"
     ]
    }
   ],
   "source": [
    "print(sub_segment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b88dfd3-2e93-47ec-97e5-cf188c5acdf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_to_numpy(segments):\n",
    "    # each of these predictions takes about 19 seconds of wall clock time 19 * 4 = 5 minutes * seven segments = 9 minutes\n",
    "    #                     +--- which segment\n",
    "    #                     | +--- which of 4 copies stacked vertically\n",
    "    #                     | | +--- which voicesegment\n",
    "    #                     | | | +--- notes in the segment\n",
    "    new_voice = np.zeros((7,4,4,32),dtype=int)\n",
    "    s = 0\n",
    "    for segment in segments: # for each of 7 segments in the input chorale\n",
    "        print(f'process segment {s}')\n",
    "        old_chorale = segment - 30 # start with the segment, but reduce it to fit in the model MIDI number limits\n",
    "        for chorale in range(4): # make a total of four chorales to stack on top of each other\n",
    "            print(f'synthesize 4 voice chorale {chorale}')\n",
    "            v = 0\n",
    "            for voice in segment: # for each voice in the segment, mask it, then predict a new harmonization\n",
    "                # print(f'process voice {v}')\n",
    "                mask = selective_stretching_codes.mask_voice(v) # set this voice to zero to drop it from the voices\n",
    "                new_chorale = harmonize(old_chorale,mask,model) # spend about 19 seconds doing the inference\n",
    "                old_chorale = new_chorale # make sure the next round starts with the new harmonization\n",
    "                v += 1\n",
    "            new_voice[s,chorale] = new_chorale # save the current chorale in an array\n",
    "        s += 1\n",
    "    return(np.reshape(new_voice,(7,16,32))) # make it a array of segments times a 16,32 array for the segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40d99549-d0a7-4ce1-b329-24942ed8bd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict chorale 0\n",
      "process segment 0\n",
      "synthesize 4 voice chorale 0\n",
      "synthesize 4 voice chorale 1\n",
      "synthesize 4 voice chorale 2\n",
      "synthesize 4 voice chorale 3\n",
      "process segment 1\n",
      "synthesize 4 voice chorale 0\n",
      "synthesize 4 voice chorale 1\n",
      "synthesize 4 voice chorale 2\n",
      "synthesize 4 voice chorale 3\n",
      "process segment 2\n",
      "synthesize 4 voice chorale 0\n",
      "synthesize 4 voice chorale 1\n",
      "synthesize 4 voice chorale 2\n",
      "synthesize 4 voice chorale 3\n",
      "process segment 3\n",
      "synthesize 4 voice chorale 0\n",
      "synthesize 4 voice chorale 1\n",
      "synthesize 4 voice chorale 2\n",
      "synthesize 4 voice chorale 3\n",
      "process segment 4\n",
      "synthesize 4 voice chorale 0\n",
      "synthesize 4 voice chorale 1\n",
      "synthesize 4 voice chorale 2\n",
      "synthesize 4 voice chorale 3\n",
      "process segment 5\n",
      "synthesize 4 voice chorale 0\n",
      "synthesize 4 voice chorale 1\n",
      "synthesize 4 voice chorale 2\n",
      "synthesize 4 voice chorale 3\n",
      "process segment 6\n",
      "synthesize 4 voice chorale 0\n",
      "synthesize 4 voice chorale 1\n",
      "synthesize 4 voice chorale 2\n",
      "synthesize 4 voice chorale 3\n",
      "saving new chorale to segmented_chorales/chorale_0.npy\n",
      "CPU times: user 2h 51min 43s, sys: 1min 3s, total: 2h 52min 46s\n",
      "Wall time: 44min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Start out with 0,1 - I ran it 100 times using 0,100 - took around 45 minutes per numpy array\n",
    "for chorales in range(0,1): \n",
    "    print(f'predict chorale {chorales}')\n",
    "    new_voices = predict_to_numpy(sub_segment)\n",
    "    filename = os.path.join('segmented_chorales','chorale_' + str(chorales) + '.npy')\n",
    "    print(f'saving new chorale to {filename}')\n",
    "    np.save(filename,new_voices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fc060f-a673-478e-a806-3c573817ab47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
