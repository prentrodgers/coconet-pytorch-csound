{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b117a290-8a63-499a-beda-d0efaee10852",
   "metadata": {},
   "source": [
    "## Incrementally add new voices to a chorale.\n",
    "<p>The goals of this round:\n",
    "    \n",
    "- Take a chorale, keep three voices, synthesize the fourth. save the new one in an array.\n",
    "- Repeat: keep three voices, including the new one, and discard one of the original ones, synthesize the missing one, save it.\n",
    "- Continue dropping voices, create a new one, and save it. Do this for a while, always discarding the oldest one until you have a multi-voice chorale that sounds interesting and without too many wrong notes. \n",
    "    \n",
    " </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "953c8070-c8af-45c7-8422-383ccbb0d205",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gTZ5xE7jaVy0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mido\n",
    "import time\n",
    "from midi2audio import FluidSynth\n",
    "from IPython.display import Audio, display\n",
    "import os\n",
    "import muspy\n",
    "import piano \n",
    "import subprocess\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng(42) # random seed in parens.\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "softmax = torch.nn.functional.softmax\n",
    "\n",
    "base_dir = ''\n",
    "CSD_FILE = 'goldberg_aria1.csd'\n",
    "NOTES_FILE = \"goldberg_aria1.mac.csv\"\n",
    "LOGNAME = 'goldberg5.log'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f65c45a9-bb20-41c3-9ee4-61a65becd6da",
   "metadata": {},
   "source": [
    "def load_csound_piano():\n",
    "    piano.start_logger(fname=LOGNAME)\n",
    "    piano.logging.info(f'Logging messages to: {LOGNAME}')\n",
    "    csd_content, lines = piano.load_csd(CSD_FILE)\n",
    "    piano.logging.info(f'Loaded the csd file {CSD_FILE}. There are {lines} lines read containg {len(csd_content)} bytes')\n",
    "    cs, pt = piano.load_csound(csd_content)\n",
    "    return (pt,cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e363a75-2514-4917-b8d7-b200d91ed27f",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sGQCWjlCbGAu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I voices: 4, T sample length: 32, P number of distinct pitches in the input chorales: 57\n"
     ]
    }
   ],
   "source": [
    "# set global variables\n",
    "\n",
    "I = 4 # number of voices\n",
    "T = 32 # length of samples (32 = two 4/4 measures in 1/16th note increments)\n",
    "P = (86-30) +1 # number of different pitches\n",
    "print(f'I voices: {I}, T sample length: {T}, P number of distinct pitches in the input chorales: {P}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c395ae6-6a82-4457-b41f-f8a1e2b187ba",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ogxsHxCgak0U"
   },
   "outputs": [],
   "source": [
    "# function for converting arrays of shape (T, 4) into midi files\n",
    "# the input array has entries that are np.nan (representing a rest)\n",
    "# or an integer between 0 and 127 inclusive\n",
    "#\n",
    "# Altered to accept pieces of arbitrary number of voices. \n",
    "# Mine are all 264 notes by 16 voices per chorale\n",
    "# what comes into this is (16,264)\n",
    "def piano_roll_to_midi(piece):\n",
    "    \"\"\"\n",
    "    piece is a an array of shape (T, 4) for some T.\n",
    "    The (i,j)th entry of the array is the midi pitch of the jth voice at time i. It's an integer in range(128).\n",
    "    outputs a mido object mid that you can convert to a midi file by called its .save() method\n",
    "    \"\"\"\n",
    "    # piece = np.concatenate([piece, [[np.nan, np.nan, np.nan, np.nan]]], axis=0)\n",
    "\n",
    "    bpm = 50\n",
    "    microseconds_per_beat = 60 * 1000000 / bpm\n",
    "\n",
    "    mid = mido.MidiFile()\n",
    "    \n",
    "    # modified to make the number of voices dependent on what is passed into the function\n",
    "    v = 0\n",
    "    tracks = {}\n",
    "    past_pitches = {}\n",
    "    delta_time = {}\n",
    "    for voice in piece:\n",
    "        tracks['piano' + str(v)] = mido.MidiTrack()\n",
    "        past_pitches['piano' + str(v)] = np.nan\n",
    "        delta_time['piano' + str(v)] = 0\n",
    "        v += 1\n",
    "    \n",
    "    # create a track containing tempo data\n",
    "    metatrack = mido.MidiTrack()\n",
    "    metatrack.append(mido.MetaMessage('set_tempo',\n",
    "                                      tempo=int(microseconds_per_beat), time=0))\n",
    "    mid.tracks.append(metatrack)\n",
    "\n",
    "    # create the N voice tracks (was 4)\n",
    "    for voice in tracks:\n",
    "        mid.tracks.append(tracks[voice])\n",
    "        tracks[voice].append(mido.Message(\n",
    "            'program_change', program=0, time=0)) # choir aahs=52, piano = 0\n",
    "\n",
    "    # add notes to the N voice tracks\n",
    "    # this function expects an array in this form: chorale type: <class 'numpy.ndarray'>\n",
    "    # piece.shape: (33, 4) \n",
    "    # mine are (16,264)\n",
    "    \n",
    "    pitches = {}\n",
    "    for i in range(piece[1].shape[0]): # 0 - 263 in my case\n",
    "        v = 0\n",
    "        for voice in piece: # 0-15 in my case\n",
    "            pitches['piano'+str(v)] = piece[v,i] # i is from 0 to 263, v is 0 to 15\n",
    "            v += 1\n",
    "        for voice in tracks:\n",
    "            if np.isnan(past_pitches[voice]):\n",
    "                past_pitches[voice] = None\n",
    "            if np.isnan(pitches[voice]):\n",
    "                pitches[voice] = None\n",
    "            if pitches[voice] != past_pitches[voice]:\n",
    "                if past_pitches[voice]:\n",
    "                    tracks[voice].append(mido.Message('note_off', note=int(past_pitches[voice]),\n",
    "                                                      velocity=64, time=delta_time[voice]))\n",
    "                    delta_time[voice] = 0\n",
    "                if pitches[voice]:\n",
    "                    tracks[voice].append(mido.Message('note_on', note=int(pitches[voice]),\n",
    "                                                      velocity=64, time=delta_time[voice]))\n",
    "                    delta_time[voice] = 0\n",
    "            past_pitches[voice] = pitches[voice]\n",
    "            # 480 ticks per beat and each line of the array is a 16th note\n",
    "            delta_time[voice] += 120\n",
    "\n",
    "    return mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ab18dd7-fbcd-4ca6-8409-3abbba1602c2",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ogxsHxCgak0U"
   },
   "outputs": [],
   "source": [
    "class Chorale:\n",
    "    \"\"\"\n",
    "    A class to store and manipulate an array self.arr that stores a chorale.\n",
    "    \"\"\"\n",
    "    def __init__(self, arr, subtract_30=False):\n",
    "        # arr is an array of shape (4, 32) with values in range(0, 57)\n",
    "        self.arr = arr.copy()\n",
    "        if subtract_30:\n",
    "            self.arr -= 30\n",
    "            \n",
    "        # the one_hot representation of the array\n",
    "        reshaped = self.arr.reshape(-1)\n",
    "        self.one_hot = np.zeros((I*T, P))\n",
    "        r = np.arange(I*T)\n",
    "        self.one_hot[r, reshaped] = 1\n",
    "        self.one_hot = self.one_hot.reshape(I, T, P)\n",
    "        \n",
    "\n",
    "    def to_image(self):\n",
    "        # visualize the four tracks as a images\n",
    "        soprano = self.one_hot[0].transpose()\n",
    "        alto = self.one_hot[1].transpose()\n",
    "        tenor = self.one_hot[2].transpose()\n",
    "        bass = self.one_hot[3].transpose()\n",
    "        \n",
    "        fig, axs = plt.subplots(1, 4)\n",
    "        axs[0].imshow(np.flip(soprano, axis=0), cmap='hot', interpolation='nearest')\n",
    "        axs[0].set_title('soprano')\n",
    "        axs[1].imshow(np.flip(alto, axis=0), cmap='hot', interpolation='nearest')\n",
    "        axs[1].set_title('alto')\n",
    "        axs[2].imshow(np.flip(tenor, axis=0), cmap='hot', interpolation='nearest')\n",
    "        axs[2].set_title('tenor')\n",
    "        axs[3].imshow(np.flip(bass, axis=0), cmap='hot', interpolation='nearest')\n",
    "        axs[3].set_title('bass')\n",
    "        fig.set_figheight(5)\n",
    "        fig.set_figwidth(15)\n",
    "        return fig, axs\n",
    "    \n",
    "    def play(self, filename='midi_track.mid'):\n",
    "        # display an in-notebook widget for playing audio\n",
    "        # saves the midi file as a file named name in base_dir/midi_files\n",
    "        \n",
    "        midi_arr = self.arr.transpose().copy()\n",
    "        midi_arr += 30\n",
    "        midi = piano_roll_to_midi(midi_arr)\n",
    "        midi.save(base_dir + 'midi_files/' + filename)\n",
    "        play_midi('midi_files/' + filename,10)\n",
    "        \n",
    "    def elaborate_on_voices(self, voices, model):\n",
    "        # voice is a set consisting of 0, 1, 2, or 3\n",
    "        # create a mask consisting of the given voices\n",
    "        # generate a chorale with the same voices as in voices\n",
    "        mask = np.zeros((I, T))\n",
    "        y = np.random.randint(P, size=(I, T))\n",
    "        for i in voices:\n",
    "            mask[i] = 1\n",
    "            y[i] = self.arr[i].copy()\n",
    "        return harmonize(y, mask, model)\n",
    "    \n",
    "    # I think we could improve this scoring method. It's pretty lame.\n",
    "    def score(self):\n",
    "        consonance_dict = {0: 1, 1: 0, 2: 0, 3: 1, 4: 1, 5: 1, 6: 0, \n",
    "                           7: 1, 8: 1, 9: 1, 10: 0, 11: 0}\n",
    "        consonance_score = 0\n",
    "        for k in range(32):\n",
    "            for i in range(4):\n",
    "                for j in range(i):\n",
    "                    consonance_score += consonance_dict[((self.arr[i, k] - self.arr[j, k]) % 12)]\n",
    "        \n",
    "        note_score = 0\n",
    "        for i in range(4):\n",
    "            for j in range(1, 32):\n",
    "                if self.arr[i, j] != self.arr[i, j-1]:\n",
    "                    note_score += 1\n",
    "        return consonance_score, note_score\n",
    "        \n",
    "# harmonize a melody\n",
    "def harmonize(y, C, model):\n",
    "    \"\"\"\n",
    "    Generate an artificial Bach Chorale starting with y, and keeping the pitches\n",
    "    where C==1.\n",
    "    Here C is an array of shape (4, 32) whose entries are 0 and 1.\n",
    "    The pitches outside of C are repeatedly resampled to generate new values.\n",
    "    For example, to harmonize the soprano line, let y be random except y[0] \n",
    "    contains the soprano line, let C[1:] be 0 and C[0] be 1.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = y\n",
    "        C2 = C.copy()\n",
    "        num_steps = int(2*I*T)\n",
    "        alpha_max = .999\n",
    "        alpha_min = .001\n",
    "        eta = 3/4\n",
    "        for i in range(num_steps):\n",
    "            p = np.maximum(alpha_min, alpha_max - i*(alpha_max-alpha_min)/(eta*num_steps))\n",
    "            sampled_binaries = np.random.choice(2, size = C.shape, p=[p, 1-p])\n",
    "            C2 += sampled_binaries\n",
    "            C2[C==1] = 1\n",
    "            x_cache = x\n",
    "            x = model.pred(x, C2)\n",
    "            x[C2==1] = x_cache[C2==1]\n",
    "            C2 = C.copy()\n",
    "        return x\n",
    "    \n",
    "def generate_random_chorale(model): # \n",
    "    \"\"\"\n",
    "    Calls harmonize with random initialization and C=0, masking none \n",
    "    and so generates a new sample that sounds like Bach.\n",
    "    \"\"\"\n",
    "    y = np.random.randint(P, size=(I, T)).astype(int)\n",
    "    C = np.zeros((I, T)).astype(int)\n",
    "    x = harmonize(y, C, model)\n",
    "    return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9dccb0b6-7482-4baf-8bb6-afd681a5425a",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "24hPqElFbKhk"
   },
   "outputs": [],
   "source": [
    "hidden_size = 32\n",
    "\n",
    "class Unit(nn.Module):\n",
    "    \"\"\"\n",
    "    Two convolution layers each followed by batchnorm and relu, \n",
    "    plus a residual connection.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Unit, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(hidden_size, hidden_size, 3, padding=1)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(hidden_size, hidden_size, 3, padding=1)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        y = self.conv1(y)\n",
    "        y = self.batchnorm1(y)\n",
    "        y = self.relu1(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.batchnorm2(y)\n",
    "        y = y + x\n",
    "        y = self.relu2(y)\n",
    "        return y\n",
    "    \n",
    "    \n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    A CNN that where you input a starter chorale and a mask and it outputs a prediction for the values\n",
    "    in the starter chorale away from the mask that are most like the training data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.initial_conv = nn.Conv2d(2*I, hidden_size, 3, padding=1)\n",
    "        self.initial_batchnorm = nn.BatchNorm2d(hidden_size)\n",
    "        self.initial_relu = nn.ReLU()\n",
    "        self.unit1 = Unit()\n",
    "        self.unit2 = Unit()\n",
    "        self.unit3 = Unit()\n",
    "        self.unit4 = Unit()\n",
    "        self.unit5 = Unit()\n",
    "        self.unit6 = Unit()\n",
    "        self.unit7 = Unit()\n",
    "        self.unit8 = Unit()\n",
    "        self.unit9 = Unit()\n",
    "        self.unit10 = Unit()\n",
    "        self.unit11 = Unit()\n",
    "        self.unit12 = Unit()\n",
    "        self.unit13 = Unit()\n",
    "        self.unit14 = Unit()\n",
    "        self.unit15 = Unit()\n",
    "        self.unit16 = Unit()\n",
    "        self.affine = nn.Linear(hidden_size*T*P, I*T*P)\n",
    "        \n",
    "    def forward(self, x, C):\n",
    "        # x is a tensor of shape (N, I, T, P)\n",
    "        # C is a tensor of 0s and 1s of shape (N, I, T)\n",
    "        # returns a tensor of shape (N, I, T, P)\n",
    "        \n",
    "        # get the number of batches\n",
    "        N = x.shape[0]\n",
    "        \n",
    "        # tile the array C out of a tensor of shape (N, I, T, P)\n",
    "        tiled_C = C.view(N, I, T, 1)\n",
    "        tiled_C = tiled_C.repeat(1, 1, 1, P)\n",
    "        \n",
    "        # mask x and combine it with the mask to produce a tensor of shape (N, 2*I, T, P)\n",
    "        y = torch.cat((tiled_C*x, tiled_C), dim=1)\n",
    "        \n",
    "        # apply the convolution and relu layers\n",
    "        y = self.initial_conv(y)\n",
    "        y = self.initial_batchnorm(y)\n",
    "        y = self.initial_relu(y)\n",
    "        y = self.unit1(y)\n",
    "        y = self.unit2(y)\n",
    "        y = self.unit3(y)\n",
    "        y = self.unit4(y)\n",
    "        y = self.unit5(y)\n",
    "        y = self.unit6(y)\n",
    "        y = self.unit7(y)\n",
    "        y = self.unit8(y)\n",
    "        y = self.unit9(y)\n",
    "        y = self.unit10(y)\n",
    "        y = self.unit11(y)\n",
    "        y = self.unit12(y)\n",
    "        y = self.unit13(y)\n",
    "        y = self.unit14(y)\n",
    "        y = self.unit15(y)\n",
    "        y = self.unit16(y)\n",
    "            \n",
    "        # reshape before applying the fully connected layer\n",
    "        y = y.view(N, hidden_size*T*P)\n",
    "        y = self.affine(y)\n",
    "        \n",
    "        # reshape to (N, I, T, P)\n",
    "        y = y.view(N, I, T, P)\n",
    "                \n",
    "        return y\n",
    "    \n",
    "    def pred(self, y, C):\n",
    "        # y is an array of shape (I, T) with integer entries in [0, P)\n",
    "        # C is an array of shape (I, T) consisting of 0s and 1s\n",
    "        # the entries of y away from the support of C should be considered 'unknown'\n",
    "        \n",
    "        # x is shape (I, T, P) one-hot representation of y\n",
    "        compressed = y.reshape(-1)\n",
    "        x = np.zeros((I*T, P))\n",
    "        r = np.arange(I*T)\n",
    "        x[r, compressed] = 1\n",
    "        x = x.reshape(I, T, P)\n",
    "        \n",
    "        # prep x and C for the plugging into the model\n",
    "        x = torch.tensor(x).type(torch.FloatTensor).to(device)\n",
    "        x = x.view(1, I, T, P)\n",
    "        C2 = torch.tensor(C).type(torch.FloatTensor).view(1, I, T).to(device)\n",
    "        \n",
    "        # plug x and C2 into the model\n",
    "        with torch.no_grad():\n",
    "            out = self.forward(x, C2).view(I, T, P).cpu().numpy()\n",
    "            out = out.transpose(2, 0, 1) # shape (P, I, T)\n",
    "            probs = np.exp(out) / np.exp(out).sum(axis=0) # shape (P, I, T)\n",
    "            cum_probs = np.cumsum(probs, axis=0) # shape (P, I, T)\n",
    "            u = np.random.rand(I, T) # shape (I, T)\n",
    "            return np.argmax(cum_probs > u, axis=0)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e59f1340-bcc3-4d3a-9784-7ce430a5b072",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xm0YK6yGbZg1"
   },
   "outputs": [],
   "source": [
    "model = Net().to(device) # need this in order to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed5c4611-2ba4-49b1-b96b-14edcab208d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bucYvJ5u7fyl",
    "outputId": "3544bbfd-62af-42b2-cf60-5a0750028bd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncomment to load the previously trained model\n",
    "model.load_state_dict(torch.load('model1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c631ae54-6c52-43d4-8342-e4905e9e23b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_number(n):\n",
    "    \"\"\"\n",
    "    prepare numbers for better file storage\n",
    "    \"\"\"\n",
    "    if n == 0:\n",
    "        return '00000'\n",
    "    else:\n",
    "        digits = int(np.ceil(np.log10(n)))\n",
    "        pad_zeros = 5 - digits\n",
    "        return '0'* pad_zeros + str(n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adab19e-1b6a-4be9-98da-ab94bdd49d7c",
   "metadata": {},
   "source": [
    "## Decompression of model output back to the 2 1/2 measure segment.\n",
    "<p>This section turns the output of the model into a 40 slot segment from the output of the model. We compress the segment going into the model, so we decompress it coming out of the model. Decompress does several things. \n",
    "    \n",
    "- expands the end of segment for 0,1,2,3 and convert it to 4x40 array\n",
    "- fixes the end of the 4,5,6th by adding padding to convert to a 4x40 array\n",
    "    \n",
    "This will be called just after emerging from the model and before the midi file is written    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "608b6fc2-5250-4acb-bce6-1499e1619288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompress_end_of_segment(this_segment):\n",
    "    # this function expects a single segment of shape (4,32)\n",
    "    # this function will de-compress the last 8 1/16th notes into the space of 16 16/th notes\n",
    "    # it moves elements in the four voices, one at a time the new, larger array\n",
    "\n",
    "    expanded = np.zeros((4,40),dtype=int) # this is the original shape of the array before compression\n",
    "    # print(f'this_segment.shape: {this_segment.shape}')\n",
    "    \n",
    "    for voice in range(4): # for all voices in the segment, copy the first 32 slots into a new larger numpy array\n",
    "        # print(f'voice: {voice}')\n",
    "        for source_index in range(24): # copy the first 24 slots with no change\n",
    "            # print(f'source_index: {source_index}  ')\n",
    "            # print(f'this_segment[{voice}][{source_index}]:  {this_segment[voice][source_index]}')                  \n",
    "            expanded[voice][source_index] = this_segment[voice][source_index]\n",
    "    \n",
    "    \n",
    "    for voice in range(4): # then for each voice in the segment, spread the last 8 slots over 16 slots in the expanded array.\n",
    "        target_index = 24\n",
    "        for source_index in range(24,32):\n",
    "            # print(f'voice: {voice}, source_index: {source_index}')\n",
    "            expanded[voice,target_index] = this_segment[voice,source_index]\n",
    "            target_index += 1\n",
    "            # print(f'source_index: {source_index}, target_index starts at 24 and is now: {target_index}')\n",
    "            expanded[voice,target_index] = this_segment[voice,source_index]\n",
    "            target_index += 1\n",
    "\n",
    "    return(expanded) # return all four voices all notes in each voice. Return all 40 slots\n",
    "\n",
    "def decompress(arr):\n",
    "    s = 0\n",
    "    my_expanded_segment = np.zeros((7,4,40),dtype=int)\n",
    "    # for segments 0,1,2,3 passed into this function (that is the first ten measures of the chorale, which comprise four phrases, each 2 1/2 measures long)\n",
    "    for seg in arr: \n",
    "        if s > 3: break # process the decompression on segments 0,1,2,3. If you reach 4, stop processing\n",
    "        # print(f'arr.shape: {arr.shape}')\n",
    "        # print(f'arr[s].shape: {arr.shape[s]}')\n",
    "        my_expanded_segment[s] = decompress_end_of_segment(arr[s]) \n",
    "        s += 1\n",
    "\n",
    "    pad8 = np.zeros((4,8))  # pad the end of the segment with zeros\n",
    "    for i in range(4,7): # segments 4,5,6\n",
    "        my_expanded_segment[i] = np.concatenate((arr[i],pad8),axis=1)\n",
    "    return(my_expanded_segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9617b3-edeb-45d0-b679-7d8e5211da7c",
   "metadata": {},
   "source": [
    "## Transpose from the key of C to the original key\n",
    "This is done to restore what the input midi file key was. I found that model inputs in the key of C are harmonized much better than those that are in other keys. I thought they took care of this in the model by transposing to different keys, but my experience suggests otherwize.\n",
    "Add the value of root (F is 5) to each note in the array, with the exception of the 0's, which have to remain the same 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9367b3fd-2e84-4901-aea2-5157cf26d21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_up_segment(my_segment,root):\n",
    "    new_segment = np.copy(my_segment) # just make a copy, you will change the non zero elements \n",
    "    v = 0\n",
    "    for voice in new_segment:\n",
    "        n = 0\n",
    "        for note in voice:\n",
    "            if note > 0:\n",
    "                new_segment[v,n] = note + root\n",
    "            n += 1\n",
    "        v += 1\n",
    "        \n",
    "    return(new_segment)\n",
    "\n",
    "def transpose_up(segments,root): # read in \n",
    "    s = 0\n",
    "    new_segment = np.copy(segments)\n",
    "    for seg in segments:\n",
    "        new_segment[s] = transpose_up_segment(seg,root)\n",
    "    return(new_segment)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0624aea1-e113-4bd0-9f44-562b2d3948fc",
   "metadata": {},
   "source": [
    "## Load a midi file into a numpy array\n",
    "Set certain values:\n",
    "\n",
    "- the numpy array of the whole piece is stored in variable \"sample'\n",
    "- store the root key and mode (F major, for example)\n",
    "- print the values of the time signature (must be 4/4 of you will need to do some extra work), quarter note clicks, clicks per 1/16th notes\n",
    "- any transpositions that must be performed to restore the original key\n",
    "- print the first 5 notes in each voice\n",
    "- print the shape of the variable \"sample\" containing the whole midi file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b76dd6dc-74be-4bb9-aab3-c7cb8f223eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in a midi file, check the key, load into piano roll, set up np.array containing Nx4 sample.\n",
    "# calling program should slice the returned array as needed to create two measure segments for sending into the prediction model.\n",
    "\n",
    "def midi_to_input(midi_file):\n",
    "    music = muspy.read(midi_file)\n",
    "    if music.key_signatures != []: # check if the midi file includes a key signature - some don't\n",
    "        root = music.key_signatures[0].root \n",
    "        mode = music.key_signatures[0].mode # major or minor\n",
    "    else: \n",
    "        print('Warning: no key signature found. Assuming C major')\n",
    "        mode = \"major\"\n",
    "        root = 0    \n",
    "    if music.time_signatures != []: # check if the midi file includes a time signature - some don't\n",
    "        numerator = music.time_signatures[0].numerator\n",
    "        denominator = music.time_signatures[0].denominator \n",
    "    else: \n",
    "        print('Warning: no time signature found. Assuming 4/4')\n",
    "        numerator = 4\n",
    "        denominator = 4\n",
    "    # turn it into a piano roll\n",
    "    piano_roll = muspy.to_pianoroll_representation(music,encode_velocity=False) # boolean piano roll if False, default True\n",
    "    # print(piano_roll.shape) # should be one time step for every click in the midi file\n",
    "    q = music.resolution # quarter note value in this midi file. \n",
    "    q16 = q // 4 # my desired resolution is by 1/16th notes\n",
    "    print(f'time signatures: {numerator}/{denominator}')\n",
    "    time_steps = piano_roll.shape[0] // q16\n",
    "    print(f'music.resolution is q: {q}. q16: {q16} time_steps: {time_steps} 1/16th notes')\n",
    "    sample= np.zeros(shape=(time_steps,4)).astype(int) # default is float unless .astype(int)\n",
    "    # This loop is able to load an array of shape N,4 with the notes that are being played in each time step\n",
    "    for click in range(0,piano_roll.shape[0],q16): # q16 is skip 240 steps for 1/16th note resolution\n",
    "        voice = 3 # start with the low voices and decrement for the higher voices as notes get higher\n",
    "        for i in range(piano_roll.shape[1]): # check if any notes are non-zero\n",
    "            time_interval = (click) // q16 \n",
    "            if (piano_roll[click][i]): # if velocity anything but zero - unless you set encode_velocity = False\n",
    "                # if time_interval % 16 == 0:\n",
    "                #     print(f'time step: {click} at index {i}, time_interval: {time_interval}, voice: {voice}')\n",
    "                # i is the midi note number. I want to transpose it into C\n",
    "                sample[time_interval][voice] = i - root # index to the piano roll with a note - transposed by the key if not C which is 0\n",
    "                voice -= 1 # next instrument will get the higher note\n",
    "    return (sample,root,mode)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c7966d06-32cc-428d-8009-b942bf83ea90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time signatures: 4/4\n",
      "music.resolution is q: 1024. q16: 256 time_steps: 320 1/16th notes\n",
      "/home/prent/Downloads/chorales_018007b_(c)greentree.mid, \n",
      "F  major transposed into C and then used to create the segments\n",
      "64  60  55  48  \n",
      "64  60  55  48  \n",
      "64  60  55  48  \n",
      "64  60  55  48  \n",
      "62  59  55  43  \n",
      "sample.shape: (320, 4). dtype(sample): <class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "# load the BWV 180 Schmucke dich, o liebe Seele Chorale - nice variety of phrase lengths.\n",
    "# load a midi file into a list called sample - load the entire file, all tracks, all notes in all tracks\n",
    "# if the midi file has a key signature, it will print what it is. \n",
    "# the notes will be transposed by the loader to the key of C, by subtracting the root from each note. F = 5\n",
    "file_name = '/home/prent/Downloads/chorales_018007b_(c)greentree.mid'\n",
    "\n",
    "# load the midi file into an instance of the music class from muspy.\n",
    "sample, root, mode = midi_to_input(file_name) # sample is time interval, voice\n",
    "keys = ['C ','C#','D ','D#','E ','F ','F#','G ','G#','A ','A#','B ']\n",
    "print(f'{file_name}, \\n{keys[root]} {mode} transposed into C and then used to create the segments')\n",
    "i = 0\n",
    "for t in sample: # for each time interval\n",
    "    i += 1\n",
    "    for v in t: # for each voice\n",
    "        print(v,' ' , end='')\n",
    "    print('')\n",
    "    if i > 4: break\n",
    "\n",
    "print(f'sample.shape: {sample.shape}. dtype(sample): {type(sample[0,0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f441d7-9313-4547-aed8-1dfcd14f9687",
   "metadata": {},
   "source": [
    "## Divide the sample into segments based on phrase length\n",
    "In this case, the 1st four segments are 2 1/2 measures long. That Bach guy was full of surprises. The next two are repeats and can be discarded for now. The 4th and 5th are 2 measures long, which is what the model expects. The final one is the closing chord. At the end of this cell, you have a variable called \"segment\" which contains an array of 0 through 6 segments of the piece, each with 40 time slots for each of 4 voices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1a2470d-a708-4980-bf61-0498a39a63ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seg_num\tlength\tstart\tend\n",
      "0\t40\t0\t39\n",
      "1\t40\t40\t79\n",
      "2\t40\t160\t199\n",
      "3\t40\t200\t239\n",
      "4\t32\t240\t271\n",
      "5\t32\t272\t303\n",
      "6\t8\t304\t311\n"
     ]
    }
   ],
   "source": [
    "# sample is a piano roll of pitches in 1/16th note intervals of dimension (320 time intervals, 4 voices, 1 pitch per time interval and voice)\n",
    "\n",
    "seg_num = 0 # index into the segment array\n",
    "segment = np.zeros((7,4,40),dtype=int)  # seg_num, voices, 1/16th note values\n",
    "print(f'seg_num\\tlength\\tstart\\tend')\n",
    "pad8 = np.zeros((8,4)) # 8 zeros in each of four voices for segments 4 & 5\n",
    "\n",
    "phrase_len = int(4 * 4 * 2.5) # the first segmenst have phrases of 2 1/2 measures in length 4*4*2.5 = 40 12/16th notes\n",
    "for i in range(6): # sample 0 though 5, seg_num 0,1,2,3\n",
    "    start = i * phrase_len \n",
    "    end = (i + 1) * phrase_len\n",
    "    if i in (2,3): # note that the first two segments are repeated, so we can discard segments 2 & 3    \n",
    "        pass\n",
    "        # print(f'Ignore segments 2 & 3 they are repeats. seg_num: {seg_num}')\n",
    "    else:\n",
    "        print(f'{seg_num}\\t{phrase_len}\\t{start}\\t{end-1}')\n",
    "        transfer = sample[start:end]\n",
    "        segment[seg_num] = transfer.transpose()\n",
    "        seg_num += 1\n",
    "    \n",
    "phrase_len = int(4 * 4 * 2) # 32 1/16th notes   \n",
    "for i in range(6, 8): # seg_num: 4 & 5\n",
    "    start = end \n",
    "    end = (start + phrase_len)\n",
    "    print(f'{seg_num}\\t{phrase_len}\\t{start}\\t{end-1}')\n",
    "    transfer = np.concatenate((sample[start:end],pad8),axis=0) # load the segment with the first 8 1/16th notes from the next segment. We will ignore these later.\n",
    "    segment[seg_num] = transfer.transpose()\n",
    "    seg_num += 1\n",
    "\n",
    "phrase_len = int(4 * 2) # 8 1/16th notes in a whole note\n",
    "for i in range(8,9): # seg_num 6\n",
    "    start = end \n",
    "    end = (start + phrase_len)\n",
    "    print(f'{seg_num}\\t{phrase_len}\\t{start}\\t{end-1}')\n",
    "    transfer = sample[start:end], # load the segment with the first 8 1/16th notes from the next segment. We will ignore these later.\n",
    "    transfer = np.concatenate(transfer*5) # put 5 copies of the 8 1/16th notes one after the other fill out to 40 slots. Ignore the later slots.\n",
    "    segment[seg_num] = transfer.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a5367-bd94-4050-bc1d-bbb5d0e4e69a",
   "metadata": {},
   "source": [
    "## Compress the 40 slot segments down to 32 slots\n",
    "This is done to match the model requirements. We create a helper function that compresses the last 16 slots down to 8 by skipping every other note in the 16. Not as crude at the clipping that was done in the mode, but it looses some information that cannot be retrieved upon decompressions. At the end of this process, we have a 7,4,32 array with 7 segments that are all 32 1/16th notes in length in a variable called \"sub_segment\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "adcd62a4-5835-4128-9c62-31c56d5b43fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will take a 4,40 array and return a 4,32 array. It compresses the last 16 slots into 8 slots by skipping every other slot in the array.\n",
    "def compress_end_of_segment(input_array):\n",
    "    # let numpy do the slicing. It's better than a python list\n",
    "    # np_input = np.array(input_array) # don't need it because it's already a np.array\n",
    "    # this function will compress the last 16 1/16th notes into the space of 8 16/th notes\n",
    "    # it looks at the four voices, one at a time and moves the \n",
    "    for v in range(4):\n",
    "        n = 24 # start at this slot for each voice\n",
    "        for i in range(n,40,2): # start at 24, increment until just before 40 by 2 each time\n",
    "            input_array[v][n] = input_array[v][i]\n",
    "            n += 1\n",
    "    return(input_array[:,:-8]) # return all four voices all notes in each voice. Return only the first 32 slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "031fc2bf-7017-4bd3-84b5-ad0de8336459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 4, 40)\n",
      "seg_num: 0 before compression\n",
      "segment[0]: [64 64 64 64 62 62 62 62 60 60 60 60 62 62 62 62  0  0 65 65 67 67 67 67\n",
      " 65 65 65 65 65 65 65 65 64 64 64 64 64 64 64 64]\n",
      "after compression\n",
      "my_segment: [64 64 64 64 62 62 62 62 60 60 60 60 62 62 62 62  0  0 65 65 67 67 67 67\n",
      " 65 65 65 65 64 64 64 64]\n",
      "seg_num: 1 before compression\n",
      "segment[1]: [67 67 67 67 64 64 64 64 65 65 65 65 64 64 62 62  0  0 62 62 64 64 64 64\n",
      " 62 62 62 62 62 62 62 62 60 60 60 60 60 60 60 60]\n",
      "after compression\n",
      "my_segment: [67 67 67 67 64 64 64 64 65 65 65 65 64 64 62 62  0  0 62 62 64 64 64 64\n",
      " 62 62 62 62 60 60 60 60]\n",
      "seg_num: 2 before compression\n",
      "segment[2]: [67 67 67 67 69 69 71 71 72 72 72 72 72 72 72 72 71 71 69 69 67 67 69 69\n",
      " 69 69 69 69 69 69 69 69 67 67 67 67 67 67 67 67]\n",
      "after compression\n",
      "my_segment: [67 67 67 67 69 69 71 71 72 72 72 72 72 72 72 72 71 71 69 69 67 67 69 69\n",
      " 69 69 69 69 67 67 67 67]\n",
      "seg_num: 3 before compression\n",
      "segment[3]: [67 67 67 67 69 69 71 71 72 72 72 72 72 72 72 72 71 71 69 69  0  0 69 69\n",
      " 69 69 69 69 69 69 69 69 67 67 67 67 67 67 67 67]\n",
      "after compression\n",
      "my_segment: [67 67 67 67 69 69 71 71 72 72 72 72 72 72 72 72 71 71 69 69  0  0 69 69\n",
      " 69 69 69 69 67 67 67 67]\n"
     ]
    }
   ],
   "source": [
    "# compress segments 0,1,2,3 from 40 slots to 32 slots for all four voices\n",
    "# It leaves segments 4 & 5 alone, and expands the held note on segment 6 to 32 time slices.\n",
    "# print(segment)\n",
    "print(segment.shape)\n",
    "pad8 = np.reshape(pad8,(4,8))\n",
    "for seg_num in range(4): # we need to take the 40 slot arrays and reduce them to 32 slots.\n",
    "    print(f'seg_num: {seg_num} before compression') \n",
    "    print(f'segment[{seg_num}]: {segment[seg_num][0]}')\n",
    "    my_segment = compress_end_of_segment(segment[seg_num])\n",
    "    print('after compression')\n",
    "    print(f'my_segment: {my_segment[0]}')\n",
    "    segment[seg_num] = np.concatenate((my_segment,pad8),axis=1)\n",
    "sub_segment = segment[:,:,:32] # chop off the 33-40'th 1/16th note in the piano roll leaving 32 slots    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9165e0e-9970-48c9-9594-c3f61adffaa8",
   "metadata": {},
   "source": [
    "## What we have at this point\n",
    "\n",
    "- We have an array of seven segments in the variable sub_segment with a shape (7,4,32).\n",
    "- These are compressed versions of the original chorale, with the 40 slot segments compressed down to 32 slots\n",
    "- These segments can each be individually sent to the model for potential replacements, since the model expects 4 voices and 32 time steps.\n",
    "\n",
    "## What I need to do next:\n",
    "\n",
    "- Create a function that takes in a segment and masks one of the four voices and asks the model to synthesize the missing voice and return the replacement voice as an array. \n",
    "- store that array for later use in subsequent synthesis activities.\n",
    "- repeat by masking a different voice, ideally the oldest one, and generating a replacement for that voice.\n",
    "- keep at it as many times as you can.\n",
    "\n",
    "## How will we do this:\n",
    "\n",
    "- use the function: def harmonize(y, C, model):\n",
    "- per the docs:\n",
    "--  Generate an artificial Bach Chorale starting with y, and keeping the pitches where C==1.\n",
    "--  Here C is an array of shape (4, 32) whose entries are 0 and 1.\n",
    "--  The pitches outside of C are repeatedly resampled to generate new values.\n",
    "--  For example, to harmonize the soprano line, let y be random except y[0] contains the soprano line, let C[1:] be 0 and C[0] be 1.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "48fb7573-563a-445a-89f6-28dd95bb52d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 4, 32)\n"
     ]
    }
   ],
   "source": [
    "print(sub_segment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "47d4dbbe-862e-4866-a538-abd48dec551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_play(chorale):\n",
    "    midi_output = piano_roll_to_midi(chorale) # convert to mido object\n",
    "    music = muspy.from_mido(midi_output) # convert mido to muspy music\n",
    "    muspy.write_midi('test.midi', music)\n",
    "    muspy.write_audio('test.wav', music,'wav','font.sf2',44100,)\n",
    "    audio = Audio('test.wav')\n",
    "    display(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b88dfd3-2e93-47ec-97e5-cf188c5acdf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_to_numpy(segments):\n",
    "    # each of these predictions takes about 19 seconds of wall clock time 19 * 4 = 5 minutes * seven segments = 9 minutes\n",
    "    #                     +--- which segment\n",
    "    #                     | +--- which of 4 copies stacked vertically\n",
    "    #                     | | +--- which voicesegment\n",
    "    #                     | | | +--- notes in the segment\n",
    "    new_voice = np.zeros((7,4,4,32),dtype=int)\n",
    "    s = 0\n",
    "    for segment in segments: # for each of 7 segments in the input chorale\n",
    "        print(f'process segment {s}')\n",
    "        old_chorale = segment - 30 # start with the segment, but reduce it to fit in the model MIDI number limits\n",
    "        for chorale in range(4): # make a total of four chorales to stack on top of each other\n",
    "            print(f'synthesize 4 voice chorale {chorale}')\n",
    "            v = 0\n",
    "            for voice in segment: # for each voice in the segment, mask it, then predict a new harmonization\n",
    "                # print(f'process voice {v}')\n",
    "                mask = mask_voice(v) # set this voice to zero to drop it from the voices\n",
    "                new_chorale = harmonize(old_chorale,mask,model) # spend about 19 seconds doing the inference\n",
    "                old_chorale = new_chorale # make sure the next round starts with the new harmonization\n",
    "                v += 1\n",
    "            new_voice[s,chorale] = new_chorale # save the current chorale in an array\n",
    "        s += 1\n",
    "    return(np.reshape(new_voice,(7,16,32))) # make it a array of segments times a 16,32 array for the segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "40d99549-d0a7-4ce1-b329-24942ed8bd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict chorale 0\n",
      "process segment 0\n",
      "synthesize 4 voice chorale 0\n",
      "synthesize 4 voice chorale 1\n",
      "synthesize 4 voice chorale 2\n",
      "synthesize 4 voice chorale 3\n",
      "process segment 1\n",
      "synthesize 4 voice chorale 0\n",
      "synthesize 4 voice chorale 1\n",
      "synthesize 4 voice chorale 2\n",
      "synthesize 4 voice chorale 3\n",
      "process segment 2\n",
      "synthesize 4 voice chorale 0\n",
      "synthesize 4 voice chorale 1\n",
      "synthesize 4 voice chorale 2\n",
      "synthesize 4 voice chorale 3\n",
      "process segment 3\n",
      "synthesize 4 voice chorale 0\n",
      "synthesize 4 voice chorale 1\n",
      "synthesize 4 voice chorale 2\n",
      "synthesize 4 voice chorale 3\n",
      "process segment 4\n",
      "synthesize 4 voice chorale 0\n",
      "synthesize 4 voice chorale 1\n",
      "synthesize 4 voice chorale 2\n",
      "synthesize 4 voice chorale 3\n",
      "process segment 5\n",
      "synthesize 4 voice chorale 0\n",
      "synthesize 4 voice chorale 1\n",
      "synthesize 4 voice chorale 2\n",
      "synthesize 4 voice chorale 3\n",
      "process segment 6\n",
      "synthesize 4 voice chorale 0\n",
      "synthesize 4 voice chorale 1\n",
      "synthesize 4 voice chorale 2\n",
      "synthesize 4 voice chorale 3\n",
      "saving new chorale to segmented_chorales/chorale_0.npy\n",
      "CPU times: user 2h 57min 34s, sys: 30.6 s, total: 2h 58min 5s\n",
      "Wall time: 44min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for chorales in range(0,1):\n",
    "    print(f'predict chorale {chorales}')\n",
    "    new_voices = predict_to_numpy(sub_segment)\n",
    "    filename = os.path.join('segmented_chorales','chorale_' + str(chorales) + '.npy')\n",
    "    print(f'saving new chorale to {filename}')\n",
    "    np.save(filename,new_voices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fc060f-a673-478e-a806-3c573817ab47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
