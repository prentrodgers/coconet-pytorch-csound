{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gvs_VMUkuWmB"
   },
   "source": [
    "This is a notebook that implements the paper https://arxiv.org/pdf/1903.07227.pdf in PyTorch. The goal is to generate samples of music, in the form of midi files, that sound like Bach chorales. Each Bach chorale is a piece of music for four voices. These chorales can be encoded in arrays of shape (4, N) where N is the number of 16th notes on the chorale and a value of 60 (say) at i, j indicates that voice is singing the pitch 60 at the jth 16th note. These encodings are in the file Jsb16thSeparated.npz.\n",
    "\n",
    "I split these encodings into two measure chunks, so arrays of shape (4, 32). After one-hot encoding the entries, they become arrays of shape (4, 32, P) where P is the number of possible pitches.\n",
    "\n",
    "To train a neural net to generate samples like the training samples, you generate samples which consist of random entries from a chorale plus the location of those entries. The neural net is then trained to predict the rest of the entries. For example, the net might be given the entries of one voice in the chorale and then its job is to predict the rest of the entries. In practice, this works by randomly generating arrays of shape (4, 32) whose entries are 0 and 1. A chorale is multiplied by this array to erase part of its data. Then the partially erased array and the masking array of 0s and 1s are fed through the neural net, which outputs a predicted array of shape (4, 32, P). This output array is compared with the full (4, 32, P) array of the inputted chorale via cross entropy loss, and gradient descent is applied with respect to this loss function. This encourages the network to learn the pitches in the Bach chorale that were erased in the input.\n",
    "\n",
    "To generate good samples for listening, it helps to repeatedly resample. You generate a completely unmasked chorale, then slowly freeze notes (as if the composer has decided finally that this note is good) and resample with the frozen notes masked. As you resample, you freeze more and more notes, until you're masking all the notes. At this point the sample has been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "nVc5sjL6PMPM",
    "outputId": "8567e9b4-6571-4912-9652-2852a0553efe"
   },
   "outputs": [],
   "source": [
    "# installations needed for in-colab midi playback\n",
    "# !apt install fluidsynth\n",
    "# !cp /usr/share/sounds/sf2/FluidR3_GM.sf2 ./font.sf2\n",
    "# !pip install midi2audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gTZ5xE7jaVy0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mido\n",
    "import time\n",
    "from midi2audio import FluidSynth\n",
    "from IPython.display import Audio, display\n",
    "import os\n",
    "\n",
    "# device = 'cuda:0'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "softmax = torch.nn.functional.softmax\n",
    "\n",
    "base_dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "colab_type": "code",
    "id": "oDwOLhiCOw4C",
    "outputId": "e3d3bb86-94d7-4c0c-ad7d-a8f478a1ef9e"
   },
   "outputs": [],
   "source": [
    "def play_midi(path):\n",
    "    \"\"\"\n",
    "    A script for playing the midi files in the notebook. path is the path to the midi file to be played, relative to base_dir.\n",
    "    \"\"\"\n",
    "    if os.path.exists('test.wav'):\n",
    "        os.remove('test.wav')\n",
    "    FluidSynth('/usr/share/soundfonts/default.sf2').midi_to_audio(base_dir + path, 'test.wav')\n",
    "    audio = Audio('test.wav')\n",
    "    display(audio)\n",
    "    \n",
    "path = '30000midi.mid'\n",
    "play_midi(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L_fxHt7VaaZU"
   },
   "outputs": [],
   "source": [
    "# load training data\n",
    "data = np.load('Jsb16thSeparated.npz', encoding='bytes', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "n-BehQbialC1",
    "outputId": "25b7cbf8-b122-492d-8c6e-19fe77fb2741"
   },
   "outputs": [],
   "source": [
    "# transpose chorales to different keys, so there's more variation in training data\n",
    "all_tracks = []\n",
    "for x in data.files:\n",
    "    for y in data[x]:\n",
    "        for i in range(-6, 6):\n",
    "            all_tracks.append(y + i)\n",
    "\n",
    "print(len(all_tracks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Axm9wOqobD9G",
    "outputId": "21a97d41-956b-48e4-9f12-d49145a18351"
   },
   "outputs": [],
   "source": [
    "# determine highest and lowest pitches\n",
    "\n",
    "max_midi_pitch = -np.inf\n",
    "min_midi_pitch = np.inf\n",
    "for x in all_tracks:\n",
    "    if x.max() > max_midi_pitch:\n",
    "        max_midi_pitch = int(x.max())\n",
    "    if x.min() < min_midi_pitch:\n",
    "        min_midi_pitch = int(x.min())\n",
    "        \n",
    "print(max_midi_pitch, min_midi_pitch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sGQCWjlCbGAu"
   },
   "outputs": [],
   "source": [
    "# set global variables\n",
    "\n",
    "I = 4 # number of voices\n",
    "T = 32 # length of samples (32 = two 4/4 measures)\n",
    "P = max_midi_pitch - min_midi_pitch +1 # number of different pitches\n",
    "batch_size=24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IRiSKKXhbHI6"
   },
   "outputs": [],
   "source": [
    "# prepare the training dataset by cutting chorales in 2 measure pieces\n",
    "\n",
    "train_tracks = []\n",
    "\n",
    "for track in all_tracks:\n",
    "    track = track.transpose()\n",
    "    cut = 0\n",
    "    while cut < track.shape[1]-T:\n",
    "        if (track[:, cut:cut+T] > 0).all():\n",
    "            train_tracks.append(track[:, cut:cut+T] - min_midi_pitch)\n",
    "        cut += T\n",
    "        \n",
    "\n",
    "train_tracks = np.array(train_tracks).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jGGVthWObJI7",
    "outputId": "7a4cac32-5247-407f-9436-565d45758ae3"
   },
   "outputs": [],
   "source": [
    "print(train_tracks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ogxsHxCgak0U"
   },
   "outputs": [],
   "source": [
    "# function for converting arrays of shape (T, 4) into midi files\n",
    "# the input array has entries that are np.nan (representing a rest)\n",
    "# of an integer between 0 and 127 inclusive\n",
    "\n",
    "def piano_roll_to_midi(piece):\n",
    "    \"\"\"\n",
    "    piece is a an array of shape (T, 4) for some T.\n",
    "    The (i,j)th entry of the array is the midi pitch of the jth voice at time i. It's an integer in range(128).\n",
    "    outputs a mido object mid that you can convert to a midi file by called its .save() method\n",
    "    \"\"\"\n",
    "    piece = np.concatenate([piece, [[np.nan, np.nan, np.nan, np.nan]]], axis=0)\n",
    "\n",
    "    bpm = 50\n",
    "    microseconds_per_beat = 60 * 1000000 / bpm\n",
    "\n",
    "    mid = mido.MidiFile()\n",
    "    tracks = {'soprano': mido.MidiTrack(), 'alto': mido.MidiTrack(),\n",
    "              'tenor': mido.MidiTrack(), 'bass': mido.MidiTrack()}\n",
    "    past_pitches = {'soprano': np.nan, 'alto': np.nan,\n",
    "                    'tenor': np.nan, 'bass': np.nan}\n",
    "    delta_time = {'soprano': 0, 'alto': 0, 'tenor': 0, 'bass': 0}\n",
    "\n",
    "    # create a track containing tempo data\n",
    "    metatrack = mido.MidiTrack()\n",
    "    metatrack.append(mido.MetaMessage('set_tempo',\n",
    "                                      tempo=int(microseconds_per_beat), time=0))\n",
    "    mid.tracks.append(metatrack)\n",
    "\n",
    "    # create the four voice tracks\n",
    "    for voice in tracks:\n",
    "        mid.tracks.append(tracks[voice])\n",
    "        tracks[voice].append(mido.Message(\n",
    "            'program_change', program=52, time=0))\n",
    "\n",
    "    # add notes to the four voice tracks\n",
    "    for i in range(len(piece)):\n",
    "        pitches = {'soprano': piece[i, 0], 'alto': piece[i, 1],\n",
    "                   'tenor': piece[i, 2], 'bass': piece[i, 3]}\n",
    "        for voice in tracks:\n",
    "            if np.isnan(past_pitches[voice]):\n",
    "                past_pitches[voice] = None\n",
    "            if np.isnan(pitches[voice]):\n",
    "                pitches[voice] = None\n",
    "            if pitches[voice] != past_pitches[voice]:\n",
    "                if past_pitches[voice]:\n",
    "                    tracks[voice].append(mido.Message('note_off', note=int(past_pitches[voice]),\n",
    "                                                      velocity=64, time=delta_time[voice]))\n",
    "                    delta_time[voice] = 0\n",
    "                if pitches[voice]:\n",
    "                    tracks[voice].append(mido.Message('note_on', note=int(pitches[voice]),\n",
    "                                                      velocity=64, time=delta_time[voice]))\n",
    "                    delta_time[voice] = 0\n",
    "            past_pitches[voice] = pitches[voice]\n",
    "            # 480 ticks per beat and each line of the array is a 16th note\n",
    "            delta_time[voice] += 120\n",
    "\n",
    "    return mid\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Chorale:\n",
    "    \"\"\"\n",
    "    A class to store and manipulate an array self.arr that stores a chorale.\n",
    "    \"\"\"\n",
    "    def __init__(self, arr, subtract_30=False):\n",
    "        # arr is an array of shape (4, 32) with values in range(0, 57)\n",
    "        self.arr = arr.copy()\n",
    "        if subtract_30:\n",
    "            self.arr -= 30\n",
    "            \n",
    "        # the one_hot representation of the array\n",
    "        reshaped = self.arr.reshape(-1)\n",
    "        self.one_hot = np.zeros((I*T, P))\n",
    "        r = np.arange(I*T)\n",
    "        self.one_hot[r, reshaped] = 1\n",
    "        self.one_hot = self.one_hot.reshape(I, T, P)\n",
    "        \n",
    "\n",
    "    def to_image(self):\n",
    "        # visualize the four tracks as a images\n",
    "        soprano = self.one_hot[0].transpose()\n",
    "        alto = self.one_hot[1].transpose()\n",
    "        tenor = self.one_hot[2].transpose()\n",
    "        bass = self.one_hot[3].transpose()\n",
    "        \n",
    "        fig, axs = plt.subplots(1, 4)\n",
    "        axs[0].imshow(np.flip(soprano, axis=0), cmap='hot', interpolation='nearest')\n",
    "        axs[0].set_title('soprano')\n",
    "        axs[1].imshow(np.flip(alto, axis=0), cmap='hot', interpolation='nearest')\n",
    "        axs[1].set_title('alto')\n",
    "        axs[2].imshow(np.flip(tenor, axis=0), cmap='hot', interpolation='nearest')\n",
    "        axs[2].set_title('tenor')\n",
    "        axs[3].imshow(np.flip(bass, axis=0), cmap='hot', interpolation='nearest')\n",
    "        axs[3].set_title('bass')\n",
    "        fig.set_figheight(5)\n",
    "        fig.set_figwidth(15)\n",
    "        return fig, axs\n",
    "    \n",
    "    def play(self, filename='midi_track.mid'):\n",
    "        # display an in-notebook widget for playing audio\n",
    "        # saves the midi file as a file named name in base_dir/midi_files\n",
    "        \n",
    "        midi_arr = self.arr.transpose().copy()\n",
    "        midi_arr += 30\n",
    "        midi = piano_roll_to_midi(midi_arr)\n",
    "        midi.save(base_dir + 'midi_files/' + filename)\n",
    "        play_midi('midi_files/' + filename)\n",
    "        \n",
    "    def elaborate_on_voices(self, voices, model):\n",
    "        # voice is a set consisting of 0, 1, 2, or 3\n",
    "        # create a mask consisting of the given voices\n",
    "        # generate a chorale with the same voices as in voices\n",
    "        mask = np.zeros((I, T))\n",
    "        y = np.random.randint(P, size=(I, T))\n",
    "        for i in voices:\n",
    "            mask[i] = 1\n",
    "            y[i] = self.arr[i].copy()\n",
    "        return harmonize(y, mask, model)\n",
    "\n",
    "    def score(self):\n",
    "        consonance_dict = {0: 1, 1: 0, 2: 0, 3: 1, 4: 1, 5: 1, 6: 0, 7: 1, 8: 1, 9: 1, 10: 0, 11: 0}\n",
    "        consonance_score = 0\n",
    "        for k in range(32):\n",
    "            for i in range(4):\n",
    "                for j in range(i):\n",
    "                    consonance_score += consonance_dict[((self.arr[i, k] - self.arr[j, k]) % 12)]\n",
    "        \n",
    "        note_score = 0\n",
    "        for i in range(4):\n",
    "            for j in range(1, 32):\n",
    "                if self.arr[i, j] != self.arr[i, j-1]:\n",
    "                    note_score += 1\n",
    "        return consonance_score, note_score\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# harmonize a melody\n",
    "def harmonize(y, C, model):\n",
    "    \"\"\"\n",
    "    Generate an artificial Bach Chorale starting with y, and keeping the pitches where C==1.\n",
    "    Here C is an array of shape (4, 32) whose entries are 0 and 1.\n",
    "    The pitches outside of C are repeatedly resampled to generate new values.\n",
    "    For example, to harmonize the soprano line, let y be random except y[0] contains the soprano line, let C[1:] be 0 and C[0] be 1.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = y\n",
    "        C2 = C.copy()\n",
    "        num_steps = int(2*I*T)\n",
    "        alpha_max = .999\n",
    "        alpha_min = .001\n",
    "        eta = 3/4\n",
    "        for i in range(num_steps):\n",
    "            p = np.maximum(alpha_min, alpha_max - i*(alpha_max-alpha_min)/(eta*num_steps))\n",
    "            sampled_binaries = np.random.choice(2, size = C.shape, p=[p, 1-p])\n",
    "            C2 += sampled_binaries\n",
    "            C2[C==1] = 1\n",
    "            x_cache = x\n",
    "            x = model.pred(x, C2)\n",
    "            x[C2==1] = x_cache[C2==1]\n",
    "            C2 = C.copy()\n",
    "        return x\n",
    "    \n",
    "def generate_random_chorale(model):\n",
    "    \"\"\"\n",
    "    Calls harmonize with random initialization and C=0, and so generates a new sample that sounds like Bach.\n",
    "    \"\"\"\n",
    "    y = np.random.randint(P, size=(I, T)).astype(int)\n",
    "    C = np.zeros((I, T)).astype(int)\n",
    "    return harmonize(y, C, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "24hPqElFbKhk"
   },
   "outputs": [],
   "source": [
    "hidden_size = 32\n",
    "\n",
    "class Unit(nn.Module):\n",
    "    \"\"\"\n",
    "    Two convolution layers each followed by batchnorm and relu, plus a residual connection.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Unit, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(hidden_size, hidden_size, 3, padding=1)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(hidden_size, hidden_size, 3, padding=1)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        y = self.conv1(y)\n",
    "        y = self.batchnorm1(y)\n",
    "        y = self.relu1(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.batchnorm2(y)\n",
    "        y = y + x\n",
    "        y = self.relu2(y)\n",
    "        return y\n",
    "    \n",
    "    \n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    A CNN that where you input a starter chorale and a mask \n",
    "    and it outputs a prediction for the values\n",
    "    in the starter chorale away from the mask \n",
    "    that are most like the training data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.initial_conv = nn.Conv2d(2*I, hidden_size, 3, padding=1)\n",
    "        self.initial_batchnorm = nn.BatchNorm2d(hidden_size)\n",
    "        self.initial_relu = nn.ReLU()\n",
    "        self.unit1 = Unit()\n",
    "        self.unit2 = Unit()\n",
    "        self.unit3 = Unit()\n",
    "        self.unit4 = Unit()\n",
    "        self.unit5 = Unit()\n",
    "        self.unit6 = Unit()\n",
    "        self.unit7 = Unit()\n",
    "        self.unit8 = Unit()\n",
    "        self.unit9 = Unit()\n",
    "        self.unit10 = Unit()\n",
    "        self.unit11 = Unit()\n",
    "        self.unit12 = Unit()\n",
    "        self.unit13 = Unit()\n",
    "        self.unit14 = Unit()\n",
    "        self.unit15 = Unit()\n",
    "        self.unit16 = Unit()\n",
    "        self.affine = nn.Linear(hidden_size*T*P, I*T*P)\n",
    "        \n",
    "    def forward(self, x, C):\n",
    "        # x is a tensor of shape (N, I, T, P)\n",
    "        # C is a tensor of 0s and 1s of shape (N, I, T)\n",
    "        # returns a tensor of shape (N, I, T, P)\n",
    "        \n",
    "        # get the number of batches\n",
    "        N = x.shape[0]\n",
    "        \n",
    "        # tile the array C out of a tensor of shape (N, I, T, P)\n",
    "        tiled_C = C.view(N, I, T, 1)\n",
    "        tiled_C = tiled_C.repeat(1, 1, 1, P)\n",
    "        \n",
    "        # mask x and combine it with the mask to produce a tensor of shape (N, 2*I, T, P)\n",
    "        y = torch.cat((tiled_C*x, tiled_C), dim=1)\n",
    "        \n",
    "        # apply the convolution and relu layers\n",
    "        y = self.initial_conv(y)\n",
    "        y = self.initial_batchnorm(y)\n",
    "        y = self.initial_relu(y)\n",
    "        y = self.unit1(y)\n",
    "        y = self.unit2(y)\n",
    "        y = self.unit3(y)\n",
    "        y = self.unit4(y)\n",
    "        y = self.unit5(y)\n",
    "        y = self.unit6(y)\n",
    "        y = self.unit7(y)\n",
    "        y = self.unit8(y)\n",
    "        y = self.unit9(y)\n",
    "        y = self.unit10(y)\n",
    "        y = self.unit11(y)\n",
    "        y = self.unit12(y)\n",
    "        y = self.unit13(y)\n",
    "        y = self.unit14(y)\n",
    "        y = self.unit15(y)\n",
    "        y = self.unit16(y)\n",
    "            \n",
    "        # reshape before applying the fully connected layer\n",
    "        y = y.view(N, hidden_size*T*P)\n",
    "        y = self.affine(y)\n",
    "        \n",
    "        # reshape to (N, I, T, P)\n",
    "        y = y.view(N, I, T, P)\n",
    "                \n",
    "        return y\n",
    "    \n",
    "    def pred(self, y, C):\n",
    "        # y is an array of shape (I, T) with integer entries in [0, P)\n",
    "        # C is an array of shape (I, T) consisting of 0s and 1s\n",
    "        # the entries of y away from the support of C should be considered 'unknown'\n",
    "        \n",
    "        # x is shape (I, T, P) one-hot representation of y\n",
    "        compressed = y.reshape(-1)\n",
    "        x = np.zeros((I*T, P))\n",
    "        r = np.arange(I*T)\n",
    "        x[r, compressed] = 1\n",
    "        x = x.reshape(I, T, P)\n",
    "        \n",
    "        # prep x and C for the plugging into the model\n",
    "        x = torch.tensor(x).type(torch.FloatTensor).to(device)\n",
    "        x = x.view(1, I, T, P)\n",
    "        C2 = torch.tensor(C).type(torch.FloatTensor).view(1, I, T).to(device)\n",
    "        \n",
    "        # plug x and C2 into the model\n",
    "        with torch.no_grad():\n",
    "            out = self.forward(x, C2).view(I, T, P).cpu().numpy()\n",
    "            out = out.transpose(2, 0, 1) # shape (P, I, T)\n",
    "            probs = np.exp(out) / np.exp(out).sum(axis=0) # shape (P, I, T)\n",
    "            cum_probs = np.cumsum(probs, axis=0) # shape (P, I, T)\n",
    "            u = np.random.rand(I, T) # shape (I, T)\n",
    "            return np.argmax(cum_probs > u, axis=0)         \n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xm0YK6yGbZg1"
   },
   "outputs": [],
   "source": [
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bucYvJ5u7fyl",
    "outputId": "3544bbfd-62af-42b2-cf60-5a0750028bd6"
   },
   "outputs": [],
   "source": [
    "# uncomment to load the previously trained model\n",
    "model.load_state_dict(torch.load('model1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "B73-YuqDZniN",
    "outputId": "766698bd-e74a-4dd6-84bc-3cf60de9ef1a"
   },
   "outputs": [],
   "source": [
    " # try out the Chorale class functionality with training samples\n",
    "\n",
    "track = train_tracks[18]\n",
    "chorale = Chorale(track)\n",
    "scores = chorale.score()\n",
    "chorale.to_image()\n",
    "chorale.play()\n",
    "\n",
    "\n",
    "\n",
    "# # let's try out a chorale generated by the model, elaborating on the bass track of the last example\n",
    "# print('-------------')\n",
    "# new_chorale = Chorale(chorale.elaborate_on_voices([3], model))\n",
    "# new_chorale.to_image()\n",
    "# new_chorale.play()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "7D85oUla-MBO",
    "outputId": "61b45332-f15b-44a6-963e-8f8758c5ce9a"
   },
   "outputs": [],
   "source": [
    "# let's try harmonizing a simple melody. It looks like it's random unless you load a previously trained model above\n",
    "\n",
    "melody = [66, 66, 66, 66, 71, 71, 71, 71, 73, 73, 73, 73, 75, 75, 75, 75,\n",
    "         76, 76, 75, 73, 71, 71, 75, 75, 73, 73, 70, 70, 71, 71, 71, 71]\n",
    "\n",
    "y = np.random.randint(P, size=(I, T))\n",
    "y[0] = np.array(melody)-30\n",
    "D0 = np.ones((1, T)).astype(int)\n",
    "D1 = np.zeros((3, T)).astype(int)\n",
    "D = np.concatenate([D0, D1], axis=0)\n",
    "\n",
    "for _ in range(3):\n",
    "    chorale = Chorale(harmonize(y, D, model))\n",
    "    chorale.to_image()\n",
    "    plt.show()\n",
    "    chorale.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'P: {P}, I: {I}, T: {T}')\n",
    "print(f'y: {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "T5a1oVjIAfQR",
    "outputId": "a492e787-037e-40f7-a53e-e88f6c174110"
   },
   "outputs": [],
   "source": [
    "# let's do some more overfitting investigation\n",
    "# this sample has a suspiciously compelling bass line\n",
    "\n",
    "sample = [[74, 70, 65, 58], [74, 70, 65, 58], [74, 70, 65, 57], [74, 70, 65, 57], \n",
    "          [74, 70, 67, 55], [74, 70, 67, 55], [72, 69, 65, 53], [72, 69, 65, 53], \n",
    "          [70, 70, 67, 55], [70, 70, 67, 55], [70, 69, 67, 51], [70, 67, 67, 51], \n",
    "          [69, 69, 60, 53], [69, 69, 60, 53], [70, 65, 62, 50], [70, 65, 62, 50], \n",
    "          [72, 67, 63, 53], [72, 67, 63, 53], [72, 67, 57, 51], [72, 67, 57, 51], \n",
    "          [70, 65, 65, 46], [70, 65, 65, 46], [70, 65, 65, 46], [70, 65, 65, 46], \n",
    "          [70, 65, 62, 46], [70, 65, 62, 46], [70, 65, 62, 46], [70, 65, 62, 46], \n",
    "          [70, 65, 62, 46], [70, 65, 62, 46], [70, 65, 62, 46], [70, 65, 62, 46]]\n",
    "chorale = Chorale(np.array(sample).transpose(), subtract_30=True)\n",
    "chorale.play()\n",
    "\n",
    "sample = (np.array(sample)-30).transpose()\n",
    "\n",
    "bass_first_measure = sample[3, :16]\n",
    "\n",
    "training_bass_first_measures = train_tracks[:, 3, :16]\n",
    "\n",
    "sq_diff = np.power(bass_first_measure - training_bass_first_measures, 2)\n",
    "distances = np.sum(sq_diff, axis=1)\n",
    "\n",
    "distances_as_series = pd.Series(distances).sort_values()\n",
    "candidates = list(distances_as_series.index[:5])\n",
    "print(candidates)\n",
    "\n",
    "for c in candidates:\n",
    "    candidate_chorale = Chorale(train_tracks[c])\n",
    "    candidate_chorale.play()\n",
    "#     track = train_tracks[c]\n",
    "#     print((track + 30).transpose().tolist())\n",
    "    \n",
    "# verdict: the sample simply noticed something which recurs in the chorales, without copying it directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oxljSzQS_HZM"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wYSNIdxj3DQz"
   },
   "outputs": [],
   "source": [
    "soprano_probs = []\n",
    "alto_probs = []\n",
    "tenor_probs = []\n",
    "bass_probs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "omXM3-ObbbTQ"
   },
   "outputs": [],
   "source": [
    "# some helper functions for getting feedback data while training\n",
    "\n",
    "def pad_number(n):\n",
    "    \"\"\"\n",
    "    prepare numbers for better file storage\n",
    "    \"\"\"\n",
    "    if n == 0:\n",
    "        return '00000'\n",
    "    else:\n",
    "        digits = int(np.ceil(np.log10(n)))\n",
    "        pad_zeros = 5 - digits\n",
    "        return '0'*pad_zeros + str(n)\n",
    "\n",
    "\n",
    "def return_probs(y, C):\n",
    "    \"\"\"\n",
    "    Plugs (y, C) into model and converts the (logprob) output to probabilities.\n",
    "    In other words, in the output, the (i,j,k)th entry is the probability of getting the kth pitch when you sample for the ith voice at time j.\n",
    "    \"\"\"\n",
    "    compressed = y.reshape(-1)\n",
    "    x = np.zeros((I*T, P))\n",
    "    r = np.arange(I*T)\n",
    "    x[r, compressed] = 1\n",
    "    x = x.reshape(I, T, P)\n",
    "    x = torch.tensor(x).type(torch.FloatTensor).to(device)\n",
    "    x = x.view(1, I, T, P)\n",
    "    C2 = torch.tensor(C).type(torch.FloatTensor).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model.forward(x, C2).view(I, T, P).cpu().numpy().transpose(2, 0, 1)\n",
    "        probs = np.exp(out)/np.sum(np.exp(out), axis=0)\n",
    "        return probs.transpose(1, 2, 0)\n",
    "    \n",
    "def store_heatmaps(x, C):\n",
    "    \"\"\"\n",
    "    The output of a inputting a single sample into the net is an array of shape (I, T, P) that is interpreted as log probabilities.\n",
    "    After normalizing to probabilities, it can be interpreted as four arrays (once for each voice soprano, alto, tenor, bass) of shape (T, P)\n",
    "    That consist of the probabilities of selecting given pitches for each voice at each time. These probabilities can be visualized in heatmaps,\n",
    "    and this function stores those four heatmaps in the arrays soprano_probs, alto_probs, tenor_probs, bass_probs.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        probs = return_probs(x, C)\n",
    "        soprano_probs.append(probs[0].transpose())\n",
    "        alto_probs.append(probs[1].transpose())\n",
    "        tenor_probs.append(probs[2].transpose())\n",
    "        bass_probs.append(probs[3].transpose())\n",
    "    \n",
    "def display_heatmaps():\n",
    "    \"\"\"\n",
    "    Displays the latest heatmaps produced by store_heatmaps.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 4)\n",
    "    axs[0].imshow(np.flip(soprano_probs[-1], axis=0), cmap='hot', interpolation='nearest')\n",
    "    axs[0].set_title('soprano')\n",
    "    axs[1].imshow(np.flip(alto_probs[-1], axis=0), cmap='hot', interpolation='nearest')\n",
    "    axs[1].set_title('alto')\n",
    "    axs[2].imshow(np.flip(tenor_probs[-1], axis=0), cmap='hot', interpolation='nearest')\n",
    "    axs[2].set_title('tenor')\n",
    "    axs[3].imshow(np.flip(bass_probs[-1], axis=0), cmap='hot', interpolation='nearest')\n",
    "    axs[3].set_title('bass')\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(15)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# melody = [66, 66, 66, 66, 71, 71, 71, 71, 73, 73, 73, 73, 75, 75, 75, 75,\n",
    "#         76, 76, 75, 73, 71, 71, 75, 75, 73, 73, 70, 70, 71, 71, 71, 71]\n",
    "\n",
    "def save_midi(melody, id_number):\n",
    "    \"\"\"\n",
    "    Generate an artificial chorale which has melody in the soprano line and a Bach-like harmonization in the other lines.\n",
    "    Save the result in a midi file named {id_number}midi.mid\n",
    "    \"\"\"\n",
    "    y = np.random.randint(P, size=(I, T))\n",
    "    y[0] = np.array(melody)-30 # subtract 30 because 30 is the minimum midi_value\n",
    "    D0 = np.ones((1, T)).astype(int)\n",
    "    D1 = np.zeros((3, T)).astype(int)\n",
    "    D = np.concatenate([D0, D1], axis=0)\n",
    "    prediction = harmonize(y, D, model) + 30 # 30 back on before passing to piano_roll_to_midi\n",
    "    prediction = prediction.transpose().tolist()\n",
    "    prediction = np.array(prediction)\n",
    "    midi_output = piano_roll_to_midi(prediction)\n",
    "    midi_output.save(str(pad_number(id_number)) + 'midi.mid')\n",
    "    \n",
    "goldberg_like_line = [67, 67, 67, 67, 67, 67, 67, 67, 71, 71, 71, 71, 71, 71, 71, 71,\n",
    "                      69, 69, 69, 69, 67, 67, 66, 66, 64, 64, 64, 64, 62, 62, 62, 62]    \n",
    "    \n",
    "    \n",
    "goldberg_like_line_down = [37, 37, 37, 37, 37, 37, 37, 37, 41, 41, 41, 41, 41, 41, 41, 41,\n",
    "                      39, 39, 39, 39, 37, 37, 36, 36, 34, 34, 34, 34, 32, 32, 32, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L2E-OdRrbdlv"
   },
   "outputs": [],
   "source": [
    "# this cell will take about 30 hours.\n",
    "model.train()\n",
    "N = batch_size\n",
    "\n",
    "# for i in range(30000):\n",
    "for i in range(3):\n",
    "    \n",
    "    # tensor of shape (N, I, T)\n",
    "    C = np.random.randint(2, size=(N, I, T))\n",
    "      \n",
    "    # batch is an np array of shape (N, I, T), entries are integers in [0, P)\n",
    "    indices = np.random.choice(train_tracks.shape[0], size=N)\n",
    "    batch = train_tracks[indices]    \n",
    "    \n",
    "    # targets is of shape (N*I*T)\n",
    "    targets = batch.reshape(-1)\n",
    "    targets = torch.tensor(targets).to(device)\n",
    "    \n",
    "    # x is of shape (N, I, T, P)\n",
    "    \n",
    "    batch = batch.reshape(N*I*T)\n",
    "    x = np.zeros((N*I*T, P))\n",
    "    r = np.arange(N*I*T)\n",
    "    x[r, batch] = 1\n",
    "    x = x.reshape(N, I, T, P)\n",
    "    x = torch.tensor(x).type(torch.FloatTensor).to(device)\n",
    "\n",
    "    C2 = torch.tensor(C).type(torch.FloatTensor).to(device)\n",
    "    out = model(x, C2)\n",
    "    out = out.view(N*I*T, P)\n",
    "    loss = loss_fn(out, targets)\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(i)\n",
    "        print('loss: ', loss.item())\n",
    "        D0 = np.ones((1, T))\n",
    "        D1 = np.zeros((3, T))\n",
    "        D = np.concatenate([D0, D1], axis=0).astype(int)\n",
    "        y = np.random.randint(P, size=(I, T))\n",
    "        y[0, :] = np.array(goldberg_like_line_down)\n",
    "        store_heatmaps(y, D)\n",
    "        display_heatmaps()\n",
    "        if i % 500 == 0:\n",
    "            save_midi(goldberg_like_line, i)\n",
    "        model.train()\n",
    "        \n",
    "    # adjust learning rate    \n",
    "    if i % 5000 == 0:\n",
    "        # for g in optimizer.uparam_groups:\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] *= .75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwhCmTINaI6Z"
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '????????.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VDfcw1egbgPj"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "47vlHkxwclfs"
   },
   "outputs": [],
   "source": [
    "# script to produce a gif of the probability heatmaps during training\n",
    "\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "fig, axs = plt.subplots(1, 4)\n",
    "axs[0].imshow(np.flip(soprano_probs[0], axis=0), cmap='hot', interpolation='nearest')\n",
    "axs[0].set_title('soprano')\n",
    "axs[1].imshow(np.dflip(alto_probs[0], axis=0), cmap='hot', interpolation='nearest')\n",
    "axs[1].set_title('alto')\n",
    "axs[2].imshow(np.flip(tenor_probs[0], axis=0), cmap='hot', interpolation='nearest')\n",
    "axs[2].set_title('tenor')\n",
    "axs[3].imshow(np.flip(bass_probs[0], axis=0), cmap='hot', interpolation='nearest')\n",
    "axs[3].set_title('bass')\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "\n",
    "def update(i):\n",
    "    axs[0].imshow(np.flip(soprano_probs[i], axis=0), cmap='hot', interpolation='nearest')\n",
    "    axs[1].imshow(np.flip(alto_probs[i], axis=0), cmap='hot', interpolation='nearest')\n",
    "    axs[2].imshow(np.flip(tenor_probs[i], axis=0), cmap='hot', interpolation='nearest')\n",
    "    axs[3].imshow(np.flip(bass_probs[i], axis=0), cmap='hot', interpolation='nearest')\n",
    "    \n",
    "anim = FuncAnimation(fig, update, interval=300, repeat=True, frames=4)\n",
    "anim.save('anim.gif', writer='pillow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TZtLtBxbLJ5s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gDHMGYWW2zF_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "coconet_reorganized.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
